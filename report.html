<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Preprints</title>
    <style>
        body {
            font-family: "Times New Roman", Times, serif;
            margin: 40px;
            background-color: #fdfdfd;
            color: #000;
            line-height: 1.6;
        }
        h1 {
            text-align: center;
            margin-bottom: 50px;
        }
        .preprint {
            margin-bottom: 40px;
            padding-bottom: 20px;
            border-bottom: 1px solid #ccc;
        }
        .preprint-title {
            font-size: 1.4em;
            font-weight: bold;
            margin-bottom: 5px;
        }
        .preprint-date {
            margin-bottom: 10px;
        }
        .preprint-authors {
            font-style: italic;
            margin-bottom: 10px;
        }
        .preprint-abstract {
            margin-top: 10px;
            font-size: 1em;
        }
    </style>
</head>
<body>
    <h1>Preprints: physics.bio-ph, physics.chem-ph, cs.AI, cs.LG, bioinformatics, biophysics</h1>
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.08245">ExMAG: Learning of Maximally Ancestral Graphs</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Petr Ry\v{s}av\'y, Pavel Ryt\'i\v{r}, Xiaoyu He, Georgios Korpas, Jakub Mare\v{c}ek</div>
        <div class="preprint-abstract">arXiv:2503.08245v2 Announce Type: replace 
Abstract: As one transitions from statistical to causal learning, one is seeking the most appropriate causal model. Dynamic Bayesian networks are a popular model, where a weighted directed acyclic graph represents the causal relationships. Stochastic processes are represented by its vertices, and weighted oriented edges suggest the strength of the causal relationships. When there are confounders, one would like to utilize both oriented edges (when the direction of causality is clear) and edges that are not oriented (when there is a confounder or not a relationship), yielding mixed graphs. A little-studied extension of acyclicity to this mixed-graph setting is known as maximally ancestral graphs with consideration of confounders.
  We propose a score-based learning algorithm for learning maximally ancestral graphs. A mixed-integer quadratic program is formulated, and an algorithmic approach is proposed, in which the pre-generation of exponentially many constraints is avoided by generating only violated constraints in the so-called branch-and-cut (``lazy constraint'') method. Comparing the novel approach to the state-of-the-art, we show that the proposed approach turns out to produce more accurate results when applied to small and medium-sized synthetic instances containing up to 25 variables.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00737">Application of a spherically averaged pair potential in \emph{ab initio} path integral Monte Carlo simulations of the warm dense electron gas</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Tobias Dornheim, Thomas M. Chuna, Hannah M. Bellenbaum, Zhandos Moldabekov, Panagiotis Tolias, Jan Vorberger</div>
        <div class="preprint-abstract">arXiv:2504.00737v1 Announce Type: cross 
Abstract: Spherically averaged periodic pair potentials offer the enticing promise to provide accurate results at a drastically reduced computational cost compared to the traditional Ewald sum. In this work, we employ the pair potential by Yakub and Ronchi [\textit{J.~Chem.~Phys.}~\textbf{119}, 11556 (2003)] in \emph{ab initio} path integral Monte Carlo (PIMC) simulations of the warm dense uniform electron gas. Overall, we find very accurate results with respect to Ewald reference data for integrated properties such as the kinetic and potential energy, whereas wavenumber resolved properties such as the static structure factor $S(\mathbf{q})$, the static linear density response $\chi(\mathbf{q})$ and the static quadratic density response $\chi^{(2)}(\mathbf{q},0)$ fluctuate for small $q$. In addition, we perform an analytic continuation to compute the dynamic structure factor $S(\mathbf{q},\omega)$ from PIMC results of the imaginary-time density--density correlation function $F(\mathbf{q},\tau)$ for both pair potentials. Our results have important implications for future PIMC calculations, which can be sped up significantly using the YR potential for the estimation of equation-of-state properties or $q$-resolved observables in the non-collective regime, whereas a full Ewald treatment is mandatory to accurately resolve physical effects manifesting for smaller $q$, including the evaluation of compressibility sum rules, the interpretation of x-ray scattering experiments at small scattering angles, and the estimation of optical and transport properties.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.22517">Exploiting Mixture-of-Experts Redundancy Unlocks Multimodal Generative Abilities</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Raman Dutt, Harleen Hanspal, Guoxuan Xia, Petru-Daniel Tudosiu, Alexander Black, Yongxin Yang, Steven McDonagh, Sarah Parisot</div>
        <div class="preprint-abstract">arXiv:2503.22517v2 Announce Type: replace-cross 
Abstract: In this work, we undertake the challenge of augmenting the existing generative capabilities of pre-trained text-only large language models (LLMs) with multi-modal generation capability while satisfying two core constraints: C1 preserving the preservation of original language generative capabilities with negligible performance degradation, and C2 adhering to a small parameter budget to learn the new modality, ensuring scalability and efficiency. In contrast to current approaches that add dedicated modules, thereby significantly increasing the parameter count, we propose a method that leverages the underutilized capacity inherent in deep models. Specifically, we exploit the parameter redundancy within Mixture-of-Experts (MoEs) as a source of additional capacity for learning a new modality, enabling better parameter efficiency (C1). Moreover, we preserve the original language generation capabilities by applying low-rank adaptation exclusively to the tokens of the new modality (C2). Furthermore, we introduce a novel parameter initialization scheme based on the Gromov-Wasserstein distance to improve convergence and training stability. Through an extensive analysis of the routing mechanism, we uncover the emergence of modality-specific pathways and decreased redundancy within the experts that can efficiently unlock multi-modal generative capabilities. Overall, our method can be seamlessly applied to a wide range of contemporary LLMs, providing a new pathway for transitioning from uni-modal to multi-modal architectures.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00618">Deep Learning Model Predictive Control for Deep Brain Stimulation in Parkinson's Disease</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Sebastian Steffen, Mark Cannon</div>
        <div class="preprint-abstract">arXiv:2504.00618v1 Announce Type: cross 
Abstract: We present a nonlinear data-driven Model Predictive Control (MPC) algorithm for deep brain stimulation (DBS) for the treatment of Parkinson's disease (PD). Although DBS is typically implemented in open-loop, closed-loop DBS (CLDBS) uses the amplitude of neural oscillations in specific frequency bands (e.g. beta 13-30 Hz) as a feedback signal, resulting in improved treatment outcomes with reduced side effects and slower rates of patient habituation to stimulation. To date, CLDBS has only been implemented in vivo with simple control algorithms, such as proportional or proportional-integral control. Our approach employs a multi-step predictor based on differences of input-convex neural networks to model the future evolution of beta oscillations. The use of a multi-step predictor enhances prediction accuracy over the optimization horizon and simplifies online computation. In tests using a simulated model of beta-band activity response and data from PD patients, we achieve reductions of more than 20% in both tracking error and control activity in comparison with existing CLDBS algorithms. The proposed control strategy provides a generalizable data-driven technique that can be applied to the treatment of PD and other diseases targeted by CLDBS, as well as to other neuromodulation techniques.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2405.13900">Rehearsal-free Federated Domain-incremental Learning</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Rui Sun, Haoran Duan, Jiahua Dong, Varun Ojha, Tejal Shah, Rajiv Ranjan</div>
        <div class="preprint-abstract">arXiv:2405.13900v2 Announce Type: replace 
Abstract: We introduce a rehearsal-free federated domain incremental learning framework, RefFiL, based on a global prompt-sharing paradigm to alleviate catastrophic forgetting challenges in federated domain-incremental learning, where unseen domains are continually learned. Typical methods for mitigating forgetting, such as the use of additional datasets and the retention of private data from earlier tasks, are not viable in federated learning (FL) due to devices' limited resources. Our method, RefFiL, addresses this by learning domain-invariant knowledge and incorporating various domain-specific prompts from the domains represented by different FL participants. A key feature of RefFiL is the generation of local fine-grained prompts by our domain adaptive prompt generator, which effectively learns from local domain knowledge while maintaining distinctive boundaries on a global scale. We also introduce a domain-specific prompt contrastive learning loss that differentiates between locally generated prompts and those from other domains, enhancing RefFiL's precision and effectiveness. Compared to existing methods, RefFiL significantly alleviates catastrophic forgetting without requiring extra memory space, making it ideal for privacy-sensitive and resource-constrained devices.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.10460">Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and Beyond</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, Xiangzheng Zhang</div>
        <div class="preprint-abstract">arXiv:2503.10460v3 Announce Type: replace-cross 
Abstract: This paper introduces Light-R1, an open-source suite for training long reasoning models using reproducible and cost-effective methodology. Given the proprietary nature of data used in the DeepSeek-R1 series, we develop an alternative approach leveraging exclusively public data and models. Our curriculum training progressively increases data difficulty, combined with multi-staged post-training. Our Light-R1-32B model, trained from Qwen2.5-32B-Instruct, outperforms DeepSeek-R1-Distill-Qwen-32B in math reasoning.
  Experimental results show that this curriculum approach becomes more effective when distinct, diverse datasets are available for different training stages: fine-tuning DeepSeek-R1-Distilled models (pre-tuned by DeepSeek team on proprietary data) with 3,000 challenging examples from our curriculum dataset yielded state-of-the-art 7B and 14B models, while the 32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.
  Furthermore, we extend our work by applying GRPO on long reasoning models. Our final Light-R1-14B-DS achieves SOTA performance among 14B models in math, with AIME24 \& 25 scores of 74.0 and 60.2 respectively, surpassing many 32B models and DeepSeek-R1-Distill-Llama-70B. Despite math-focused training, Light-R1-14B-DS demonstrates strong cross-domain generalization.
  Light-R1 represents a significant advancement in making sophisticated reasoning models more accessible and implementable in real-world applications. Our models, training data and code have been made available at https://github.com/Qihoo360/Light-R1.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2411.17387">Robust Bayesian Optimization via Localized Online Conformal Prediction</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Dongwon Kim, Matteo Zecchin, Sangwoo Park, Joonhyuk Kang, Osvaldo Simeone</div>
        <div class="preprint-abstract">arXiv:2411.17387v2 Announce Type: replace 
Abstract: Bayesian optimization (BO) is a sequential approach for optimizing black-box objective functions using zeroth-order noisy observations. In BO, Gaussian processes (GPs) are employed as probabilistic surrogate models to estimate the objective function based on past observations, guiding the selection of future queries to maximize utility. However, the performance of BO heavily relies on the quality of these probabilistic estimates, which can deteriorate significantly under model misspecification. To address this issue, we introduce localized online conformal prediction-based Bayesian optimization (LOCBO), a BO algorithm that calibrates the GP model through localized online conformal prediction (CP). LOCBO corrects the GP likelihood based on predictive sets produced by LOCBO, and the corrected GP likelihood is then denoised to obtain a calibrated posterior distribution on the objective function. The likelihood calibration step leverages an input-dependent calibration threshold to tailor coverage guarantees to different regions of the input space. Under minimal noise assumptions, we provide theoretical performance guarantees for LOCBO's iterates that hold for the unobserved objective function. These theoretical findings are validated through experiments on synthetic and real-world optimization tasks, demonstrating that LOCBO consistently outperforms state-of-the-art BO algorithms in the presence of model misspecification.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00187">Insight-RAG: Enhancing LLMs with Insight-Driven Augmentation</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Pouya Pezeshkpour, Estevam Hruschka</div>
        <div class="preprint-abstract">arXiv:2504.00187v1 Announce Type: cross 
Abstract: Retrieval Augmented Generation (RAG) frameworks have shown significant promise in leveraging external knowledge to enhance the performance of large language models (LLMs). However, conventional RAG methods often retrieve documents based solely on surface-level relevance, leading to many issues: they may overlook deeply buried information within individual documents, miss relevant insights spanning multiple sources, and are not well-suited for tasks beyond traditional question answering. In this paper, we propose Insight-RAG, a novel framework designed to address these issues. In the initial stage of Insight-RAG, instead of using traditional retrieval methods, we employ an LLM to analyze the input query and task, extracting the underlying informational requirements. In the subsequent stage, a specialized LLM -- trained on the document database -- is queried to mine content that directly addresses these identified insights. Finally, by integrating the original query with the retrieved insights, similar to conventional RAG approaches, we employ a final LLM to generate a contextually enriched and accurate response. Using two scientific paper datasets, we created evaluation benchmarks targeting each of the mentioned issues and assessed Insight-RAG against traditional RAG pipeline. Our results demonstrate that the Insight-RAG pipeline successfully addresses these challenges, outperforming existing methods by a significant margin in most cases. These findings suggest that integrating insight-driven retrieval within the RAG framework not only enhances performance but also broadens the applicability of RAG to tasks beyond conventional question answering.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00954">IDMR: Towards Instance-Driven Precise Visual Correspondence in Multimodal Retrieval</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Bangwei Liu, Yicheng Bao, Shaohui Lin, Xuhong Wang, Xin Tan, Yingchun Wang, Yuan Xie, Chaochao Lu</div>
        <div class="preprint-abstract">arXiv:2504.00954v1 Announce Type: cross 
Abstract: Multimodal retrieval systems are becoming increasingly vital for cutting-edge AI technologies, such as embodied AI and AI-driven digital content industries. However, current multimodal retrieval tasks lack sufficient complexity and demonstrate limited practical application value. It spires us to design Instance-Driven Multimodal Image Retrieval (IDMR), a novel task that requires models to retrieve images containing the same instance as a query image while matching a text-described scenario. Unlike existing retrieval tasks focused on global image similarity or category-level matching, IDMR demands fine-grained instance-level consistency across diverse contexts. To benchmark this capability, we develop IDMR-bench using real-world object tracking and first-person video data. Addressing the scarcity of training data, we propose a cross-domain synthesis method that creates 557K training samples by cropping objects from standard detection datasets. Our Multimodal Large Language Model (MLLM) based retrieval model, trained on 1.2M samples, outperforms state-of-the-art approaches on both traditional benchmarks and our zero-shot IDMR-bench. Experimental results demonstrate previous models' limitations in instance-aware retrieval and highlight the potential of MLLM for advanced retrieval applications. The whole training dataset, codes and models, with wide ranges of sizes, are available at https://github.com/BwLiu01/IDMR.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2408.13805">Prior Learning in Introspective VAEs</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Ioannis Athanasiadis, Fredrik Lindsten, Michael Felsberg</div>
        <div class="preprint-abstract">arXiv:2408.13805v2 Announce Type: replace 
Abstract: Variational Autoencoders (VAEs) are a popular framework for unsupervised learning and data generation. A plethora of methods have been proposed focusing on improving VAEs, with the incorporation of adversarial objectives and the integration of prior learning mechanisms being prominent directions. When it comes to the former, an indicative instance is the recently introduced family of Introspective VAEs aiming at ensuring that a low likelihood is assigned to unrealistic samples. In this study, we focus on the Soft-IntroVAE (S-IntroVAE) and investigate the implication of incorporating a multimodal and learnable prior into this framework. Namely, we formulate the prior as a third player and show that when trained in cooperation with the decoder constitutes an effective way for prior learning, which shares the Nash Equilibrium with the vanilla S-IntroVAE. Furthermore, based on a modified formulation of the optimal ELBO in S-IntroVAE, we develop theoretically motivated regularizations, that is (i) adaptive variance clipping to stabilize training when learning the prior and (ii) responsibility regularization to discourage the formation of inactive prior mode. Finally, we perform a series of targeted experiments on a 2D density estimation benchmark and in an image generation setting comprised of the (F)-MNIST and CIFAR-10 datasets demonstrating the benefit of prior learning in S-IntroVAE in generation and representation learning.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00526">High-Quality Pseudo-Label Generation Based on Visual Prompt Assisted Cloud Model Update</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Xinrun Xu, Qiuhong Zhang, Jianwen Yang, Zhanbiao Lian, Jin Yan, Zhiming Ding, Shan Jiang</div>
        <div class="preprint-abstract">arXiv:2504.00526v1 Announce Type: cross 
Abstract: Generating high-quality pseudo-labels on the cloud is crucial for cloud-edge object detection, especially in dynamic traffic monitoring where data distributions evolve. Existing methods often assume reliable cloud models, neglecting potential errors or struggling with complex distribution shifts. This paper proposes Cloud-Adaptive High-Quality Pseudo-label generation (CA-HQP), addressing these limitations by incorporating a learnable Visual Prompt Generator (VPG) and dual feature alignment into cloud model updates. The VPG enables parameter-efficient adaptation by injecting visual prompts, enhancing flexibility without extensive fine-tuning. CA-HQP mitigates domain discrepancies via two feature alignment techniques: global Domain Query Feature Alignment (DQFA) capturing scene-level shifts, and fine-grained Temporal Instance-Aware Feature Embedding Alignment (TIAFA) addressing instance variations. Experiments on the Bellevue traffic dataset demonstrate that CA-HQP significantly improves pseudo-label quality compared to existing methods, leading to notable performance gains for the edge model and showcasing CA-HQP's adaptation effectiveness. Ablation studies validate each component (DQFA, TIAFA, VPG) and the synergistic effect of combined alignment strategies, highlighting the importance of adaptive cloud updates and domain adaptation for robust object detection in evolving scenarios. CA-HQP provides a promising solution for enhancing cloud-edge object detection systems in real-world applications.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2502.12191">AnyTouch: Learning Unified Static-Dynamic Representation across Multiple Visuo-tactile Sensors</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Ruoxuan Feng, Jiangyu Hu, Wenke Xia, Tianci Gao, Ao Shen, Yuhao Sun, Bin Fang, Di Hu</div>
        <div class="preprint-abstract">arXiv:2502.12191v3 Announce Type: replace 
Abstract: Visuo-tactile sensors aim to emulate human tactile perception, enabling robots to precisely understand and manipulate objects. Over time, numerous meticulously designed visuo-tactile sensors have been integrated into robotic systems, aiding in completing various tasks. However, the distinct data characteristics of these low-standardized visuo-tactile sensors hinder the establishment of a powerful tactile perception system. We consider that the key to addressing this issue lies in learning unified multi-sensor representations, thereby integrating the sensors and promoting tactile knowledge transfer between them. To achieve unified representation of this nature, we introduce TacQuad, an aligned multi-modal multi-sensor tactile dataset from four different visuo-tactile sensors, which enables the explicit integration of various sensors. Recognizing that humans perceive the physical environment by acquiring diverse tactile information such as texture and pressure changes, we further propose to learn unified multi-sensor representations from both static and dynamic perspectives. By integrating tactile images and videos, we present AnyTouch, a unified static-dynamic multi-sensor representation learning framework with a multi-level structure, aimed at both enhancing comprehensive perceptual abilities and enabling effective cross-sensor transfer. This multi-level architecture captures pixel-level details from tactile data via masked modeling and enhances perception and transferability by learning semantic-level sensor-agnostic features through multi-modal alignment and cross-sensor matching. We provide a comprehensive analysis of multi-sensor transferability, and validate our method on various datasets and in the real-world pouring task. Experimental results show that our method outperforms existing methods, exhibits outstanding static and dynamic perception capabilities across various sensors.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00722">Communication-Efficient l_0 Penalized Least Square</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Chenqi Gong, Hu Yang</div>
        <div class="preprint-abstract">arXiv:2504.00722v1 Announce Type: cross 
Abstract: In this paper, we propose a communication-efficient penalized regression algorithm for high-dimensional sparse linear regression models with massive data. This approach incorporates an optimized distributed system communication algorithm, named CESDAR algorithm, based on the Enhanced Support Detection and Root finding algorithm. The CESDAR algorithm leverages data distributed across multiple machines to compute and update the active set and introduces the communication-efficient surrogate likelihood framework to approximate the optimal solution for the full sample on the active set, resulting in the avoidance of raw data transmission, which enhances privacy and data security, while significantly improving algorithm execution speed and substantially reducing communication costs. Notably, this approach achieves the same statistical accuracy as the global estimator. Furthermore, this paper explores an extended version of CESDAR and an adaptive version of CESDAR to enhance algorithmic speed and optimize parameter selection, respectively. Simulations and real data benchmarks experiments demonstrate the efficiency and accuracy of the CESDAR algorithm.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.15752">Using Language Models to Decipher the Motivation Behind Human Behaviors</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Yutong Xie, Qiaozhu Mei, Walter Yuan, Matthew O. Jackson</div>
        <div class="preprint-abstract">arXiv:2503.15752v2 Announce Type: replace 
Abstract: AI presents a novel tool for deciphering the motivations behind human behaviors. We show that by varying prompts to a large language model, we can elicit a full range of human behaviors in a variety of different scenarios in terms of classic economic games. Then by analyzing which prompts are needed to elicit which behaviors, we can infer (decipher) the motivations behind the human behaviors. We also show how one can analyze the prompts to reveal relationships between the classic economic games, providing new insight into what different economic scenarios induce people to think about. We also show how this deciphering process can be used to understand differences in the behavioral tendencies of different populations.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00050">JudgeLRM: Large Reasoning Models as a Judge</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Nuo Chen, Zhiyuan Hu, Qingyun Zou, Jiaying Wu, Qian Wang, Bryan Hooi, Bingsheng He</div>
        <div class="preprint-abstract">arXiv:2504.00050v1 Announce Type: cross 
Abstract: The rise of Large Language Models (LLMs) as evaluators offers a scalable alternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for judges approaches often fall short in domains requiring complex reasoning. In this work, we investigate whether LLM judges truly benefit from enhanced reasoning capabilities. Through a detailed analysis of reasoning requirements across evaluation tasks, we reveal a negative correlation between SFT performance gains and the proportion of reasoning-demanding samples - highlighting the limitations of SFT in such scenarios. To address this, we introduce JudgeLRM, a family of judgment-oriented LLMs trained using reinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM models consistently outperform both SFT-tuned and state-of-the-art reasoning models. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms DeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks requiring deep reasoning.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2411.08314">Conditional Variable Flow Matching: Transforming Conditional Densities with Amortized Conditional Optimal Transport</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Adam P. Generale, Andreas E. Robertson, Surya R. Kalidindi</div>
        <div class="preprint-abstract">arXiv:2411.08314v4 Announce Type: replace 
Abstract: Forecasting conditional stochastic nonlinear dynamical systems is a fundamental challenge repeatedly encountered across the biological and physical sciences. While flow-based models can impressively predict the temporal evolution of probability distributions representing possible outcomes of a specific process, existing frameworks cannot satisfactorily account for the impact of conditioning variables on these dynamics. Amongst several limitations, existing methods require training data with paired conditions and are developed for discrete conditioning variables. We propose Conditional Variable Flow Matching (CVFM), a framework for learning flows transforming conditional distributions with amortization across continuous conditioning variables - permitting predictions across the conditional density manifold. This is accomplished through several novel advances. In particular, simultaneous sample conditioned flows over the main and conditioning variables, alongside a conditional Wasserstein distance combined with a loss reweighting kernel facilitating conditional optimal transport. Collectively, these advances allow for learning system dynamics provided measurement data whose states and conditioning variables are not in correspondence. We demonstrate CVFM on a suite of increasingly challenging problems, including discrete and continuous conditional mapping benchmarks, image-to-image domain transfer, and modeling the temporal evolution of materials internal structure during manufacturing processes. We observe that CVFM results in improved performance and convergence characteristics over alternative conditional variants.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2410.22322">Optimizing Posterior Samples for Bayesian Optimization via Rootfinding</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Taiwo A. Adebiyi, Bach Do, Ruda Zhang</div>
        <div class="preprint-abstract">arXiv:2410.22322v4 Announce Type: replace 
Abstract: Bayesian optimization devolves the global optimization of a costly objective function to the global optimization of a sequence of acquisition functions. This inner-loop optimization can be catastrophically difficult if it involves posterior sample paths, especially in higher dimensions. We introduce an efficient global optimization strategy for posterior samples based on global rootfinding. It provides gradient-based optimizers with two sets of judiciously selected starting points, designed to combine exploration and exploitation. The number of starting points can be kept small without sacrificing optimization quality. Remarkably, even with just one point from each set, the global optimum is discovered most of the time. The algorithm scales practically linearly to high dimensions, breaking the curse of dimensionality. For Gaussian process Thompson sampling (GP-TS), we demonstrate remarkable improvement in both inner- and outer-loop optimization, surprisingly outperforming alternatives like EI and GP-UCB in most cases. Our approach also improves the performance of other posterior sample-based acquisition functions, such as variants of entropy search. Furthermore, we propose a sample-average formulation of GP-TS, which has a parameter to explicitly control exploitation and can be computed at the cost of one posterior sample. Our implementation is available at https://github.com/UQUH/TSRoots .</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00438">Suite-IN++: A FlexiWear BodyNet Integrating Global and Local Motion Features from Apple Suite for Robust Inertial Navigation</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Lan Sun, Songpengcheng Xia, Jiarui Yang, Ling Pei</div>
        <div class="preprint-abstract">arXiv:2504.00438v1 Announce Type: cross 
Abstract: The proliferation of wearable technology has established multi-device ecosystems comprising smartphones, smartwatches, and headphones as critical enablers for ubiquitous pedestrian localization. However, traditional pedestrian dead reckoning (PDR) struggles with diverse motion modes, while data-driven methods, despite improving accuracy, often lack robustness due to their reliance on a single-device setup. Therefore, a promising solution is to fully leverage existing wearable devices to form a flexiwear bodynet for robust and accurate pedestrian localization. This paper presents Suite-IN++, a deep learning framework for flexiwear bodynet-based pedestrian localization. Suite-IN++ integrates motion data from wearable devices on different body parts, using contrastive learning to separate global and local motion features. It fuses global features based on the data reliability of each device to capture overall motion trends and employs an attention mechanism to uncover cross-device correlations in local features, extracting motion details helpful for accurate localization. To evaluate our method, we construct a real-life flexiwear bodynet dataset, incorporating Apple Suite (iPhone, Apple Watch, and AirPods) across diverse walking modes and device configurations. Experimental results demonstrate that Suite-IN++ achieves superior localization accuracy and robustness, significantly outperforming state-of-the-art models in real-life pedestrian tracking scenarios.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00461">Efficient Near-Optimal Algorithm for Online Shortest Paths in Directed Acyclic Graphs with Bandit Feedback Against Adaptive Adversaries</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Arnab Maiti, Zhiyuan Fan, Kevin Jamieson, Lillian J. Ratliff, Gabriele Farina</div>
        <div class="preprint-abstract">arXiv:2504.00461v1 Announce Type: new 
Abstract: In this paper, we study the online shortest path problem in directed acyclic graphs (DAGs) under bandit feedback against an adaptive adversary. Given a DAG $G = (V, E)$ with a source node $v_{\mathsf{s}}$ and a sink node $v_{\mathsf{t}}$, let $X \subseteq \{0,1\}^{|E|}$ denote the set of all paths from $v_{\mathsf{s}}$ to $v_{\mathsf{t}}$. At each round $t$, we select a path $\mathbf{x}_t \in X$ and receive bandit feedback on our loss $\langle \mathbf{x}_t, \mathbf{y}_t \rangle \in [-1,1]$, where $\mathbf{y}_t$ is an adversarially chosen loss vector. Our goal is to minimize regret with respect to the best path in hindsight over $T$ rounds. We propose the first computationally efficient algorithm to achieve a near-minimax optimal regret bound of $\tilde O(\sqrt{|E|T\log |X|})$ with high probability against any adaptive adversary, where $\tilde O(\cdot)$ hides logarithmic factors in the number of edges $|E|$. Our algorithm leverages a novel loss estimator and a centroid-based decomposition in a nontrivial manner to attain this regret bound.
  As an application, we show that our algorithm for DAGs provides state-of-the-art efficient algorithms for $m$-sets, extensive-form games, the Colonel Blotto game, shortest walks in directed graphs, hypercubes, and multi-task multi-armed bandits, achieving improved high-probability regret guarantees in all these settings.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00975">Resource Allocation for RIS-Assisted CoMP-NOMA Networks using Reinforcement Learning</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Muhammad Umer, Muhammad Ahmed Mohsin, Huma Ghafoor, Syed Ali Hassan</div>
        <div class="preprint-abstract">arXiv:2504.00975v1 Announce Type: cross 
Abstract: This thesis delves into the forefront of wireless communication by exploring the synergistic integration of three transformative technologies: STAR-RIS, CoMP, and NOMA. Driven by the ever-increasing demand for higher data rates, improved spectral efficiency, and expanded coverage in the evolving landscape of 6G development, this research investigates the potential of these technologies to revolutionize future wireless networks.
  The thesis analyzes the performance gains achievable through strategic deployment of STAR-RIS, focusing on mitigating inter-cell interference, enhancing signal strength, and extending coverage to cell-edge users. Resource sharing strategies for STAR-RIS elements are explored, optimizing both transmission and reflection functionalities. Analytical frameworks are developed to quantify the benefits of STAR-RIS assisted CoMP-NOMA networks under realistic channel conditions, deriving key performance metrics such as ergodic rates and outage probabilities. Additionally, the research delves into energy-efficient design approaches for CoMP-NOMA networks incorporating RIS, proposing novel RIS configurations and optimization algorithms to achieve a balance between performance and energy consumption. Furthermore, the application of Deep Reinforcement Learning (DRL) techniques for intelligent and adaptive optimization in aerial RIS-assisted CoMP-NOMA networks is explored, aiming to maximize network sum rate while meeting user quality of service requirements. Through a comprehensive investigation of these technologies and their synergistic potential, this thesis contributes valuable insights into the future of wireless communication, paving the way for the development of more efficient, reliable, and sustainable networks capable of meeting the demands of our increasingly connected world.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00070">Enhancing Time Series Forecasting with Fuzzy Attention-Integrated Transformers</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Sanjay Chakraborty, Fredrik Heintz</div>
        <div class="preprint-abstract">arXiv:2504.00070v1 Announce Type: new 
Abstract: This paper introduces FANTF (Fuzzy Attention Network-Based Transformers), a novel approach that integrates fuzzy logic with existing transformer architectures to advance time series forecasting, classification, and anomaly detection tasks. FANTF leverages a proposed fuzzy attention mechanism incorporating fuzzy membership functions to handle uncertainty and imprecision in noisy and ambiguous time series data. The FANTF approach enhances its ability to capture complex temporal dependencies and multivariate relationships by embedding fuzzy logic principles into the self-attention module of the existing transformer's architecture. The framework combines fuzzy-enhanced attention with a set of benchmark existing transformer-based architectures to provide efficient predictions, classification and anomaly detection. Specifically, FANTF generates learnable fuzziness attention scores that highlight the relative importance of temporal features and data points, offering insights into its decision-making process. Experimental evaluatios on some real-world datasets reveal that FANTF significantly enhances the performance of forecasting, classification, and anomaly detection tasks over traditional transformer-based models.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.24305">Evaluating machine learning models for predicting pesticides toxicity to honey bees</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jakub Adamczyk, Jakub Poziemski, Pawel Siedlecki</div>
        <div class="preprint-abstract">arXiv:2503.24305v2 Announce Type: replace 
Abstract: Small molecules play a critical role in the biomedical, environmental, and agrochemical domains, each with distinct physicochemical requirements and success criteria. Although biomedical research benefits from extensive datasets and established benchmarks, agrochemical data remain scarce, particularly with respect to species-specific toxicity. This work focuses on ApisTox, the most comprehensive dataset of experimentally validated chemical toxicity to the honey bee (Apis mellifera), an ecologically vital pollinator. We evaluate ApisTox using a diverse suite of machine learning approaches, including molecular fingerprints, graph kernels, and graph neural networks, as well as pretrained models. Comparative analysis with medicinal datasets from the MoleculeNet benchmark reveals that ApisTox represents a distinct chemical space. Performance degradation on non-medicinal datasets, such as ApisTox, demonstrates their limited generalizability of current state-of-the-art algorithms trained solely on biomedical data. Our study highlights the need for more diverse datasets and for targeted model development geared toward the agrochemical domain.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2312.08194">SVInvNet: A Densely Connected Encoder-Decoder Architecture for Seismic Velocity Inversion</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Mojtaba Najafi Khatounabad, Hacer Yalim Keles, Selma Kadioglu</div>
        <div class="preprint-abstract">arXiv:2312.08194v2 Announce Type: replace 
Abstract: This study presents a deep learning-based approach to seismic velocity inversion problem, focusing on both noisy and noiseless training datasets of varying sizes. Our Seismic Velocity Inversion Network (SVInvNet) introduces a novel architecture that contains a multi-connection encoder-decoder structure enhanced with dense blocks. This design is specifically tuned to effectively process time series data, which is essential for addressing the challenges of non-linear seismic velocity inversion. For training and testing, we created diverse seismic velocity models, including multi-layered, faulty, and salt dome categories. We also investigated how different kinds of ambient noise, both coherent and stochastic, and the size of the training dataset affect learning outcomes. SVInvNet is trained on datasets ranging from 750 to 6,000 samples and is tested using a large benchmark dataset of 12,000 samples. Despite its fewer parameters compared to the baseline model, SVInvNet achieves superior performance with this dataset. The performance of SVInvNet was further evaluated using the OpenFWI dataset and Marmousi-derived velocity models. The comparative analysis clearly reveals the effectiveness of the proposed model.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2405.04912">GP-MoLFormer: A Foundation Model For Molecular Generation</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jerret Ross, Brian Belgodere, Samuel C. Hoffman, Vijil Chenthamarakshan, Jiri Navratil, Youssef Mroueh, Payel Das</div>
        <div class="preprint-abstract">arXiv:2405.04912v2 Announce Type: replace-cross 
Abstract: Transformer-based models trained on large and general purpose datasets consisting of molecular strings have recently emerged as a powerful tool for successfully modeling various structure-property relations. Inspired by this success, we extend the paradigm of training chemical language transformers on large-scale chemical datasets to generative tasks in this work. Specifically, we propose GP-MoLFormer, an autoregressive molecular string generator that is trained on more than 1.1B (billion) chemical SMILES. GP-MoLFormer uses a 46.8M parameter transformer decoder model with linear attention and rotary positional encodings as the base architecture. GP-MoLFormer's utility is evaluated and compared with that of existing baselines on three different tasks: de novo generation, scaffold-constrained molecular decoration, and unconstrained property-guided optimization. While the first two are handled with no additional training, we propose a parameter-efficient fine-tuning method for the last task, which uses property-ordered molecular pairs as input. We call this new approach pair-tuning. Our results show GP-MoLFormer performs better or comparable with baselines across all three tasks, demonstrating its general utility for a variety of molecular generation tasks. We further report strong memorization of training data in GP-MoLFormer generations, which has so far remained unexplored for chemical language models. Our analyses reveal that training data memorization and novelty in generations are impacted by the quality and scale of the training data; duplication bias in training data can enhance memorization at the cost of lowering novelty. We further establish a scaling law relating inference compute and novelty in generations.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2410.16608">Assessing and improving reliability of neighbor embedding methods: a map-continuity perspective</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Zhexuan Liu, Rong Ma, Yiqiao Zhong</div>
        <div class="preprint-abstract">arXiv:2410.16608v2 Announce Type: replace-cross 
Abstract: Visualizing high-dimensional data is essential for understanding biomedical data and deep learning models. Neighbor embedding methods, such as t-SNE and UMAP, are widely used but can introduce misleading visual artifacts. We find that the manifold learning interpretations from many prior works are inaccurate and that the misuse stems from a lack of data-independent notions of embedding maps, which project high-dimensional data into a lower-dimensional space. Leveraging the leave-one-out principle, we introduce LOO-map, a framework that extends embedding maps beyond discrete points to the entire input space. We identify two forms of map discontinuity that distort visualizations: one exaggerates cluster separation and the other creates spurious local structures. As a remedy, we develop two types of point-wise diagnostic scores to detect unreliable embedding points and improve hyperparameter selection, which are validated on datasets from computer vision and single-cell omics.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.13275">Knowledge-Aware Iterative Retrieval for Multi-Agent Systems</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Seyoung Song</div>
        <div class="preprint-abstract">arXiv:2503.13275v2 Announce Type: replace 
Abstract: We introduce a novel large language model (LLM)-driven agent framework, which iteratively refines queries and filters contextual evidence by leveraging dynamically evolving knowledge. A defining feature of the system is its decoupling of external sources from an internal knowledge cache that is progressively updated to guide both query generation and evidence selection. This design mitigates bias-reinforcement loops and enables dynamic, trackable search exploration paths, thereby optimizing the trade-off between exploring diverse information and maintaining accuracy through autonomous agent decision-making. Our approach is evaluated on a broad range of open-domain question answering benchmarks, including multi-step tasks that mirror real-world scenarios where integrating information from multiple sources is critical, especially given the vulnerabilities of LLMs that lack explicit reasoning or planning capabilities. The results show that the proposed system not only outperforms single-step baselines regardless of task difficulty but also, compared to conventional iterative retrieval methods, demonstrates pronounced advantages in complex tasks through precise evidence-based reasoning and enhanced efficiency. The proposed system supports both competitive and collaborative sharing of updated context, enabling multi-agent extension. The benefits of multi-agent configurations become especially prominent as task difficulty increases. The number of convergence steps scales with task difficulty, suggesting cost-effective scalability.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2310.10559">Causal Dynamic Variational Autoencoder for Counterfactual Regression in Longitudinal Data</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Mouad El Bouchattaoui, Myriam Tami, Benoit Lepetit, Paul-Henry Courn\`ede</div>
        <div class="preprint-abstract">arXiv:2310.10559v2 Announce Type: replace-cross 
Abstract: Estimating treatment effects over time is relevant in many real-world applications, such as precision medicine, epidemiology, economy, and marketing. Many state-of-the-art methods either assume the observations of all confounders or seek to infer the unobserved ones. We take a different perspective by assuming unobserved risk factors, i.e., adjustment variables that affect only the sequence of outcomes. Under unconfoundedness, we target the Individual Treatment Effect (ITE) estimation with unobserved heterogeneity in the treatment response due to missing risk factors. We address the challenges posed by time-varying effects and unobserved adjustment variables. Led by theoretical results over the validity of the learned adjustment variables and generalization bounds over the treatment effect, we devise Causal DVAE (CDVAE). This model combines a Dynamic Variational Autoencoder (DVAE) framework with a weighting strategy using propensity scores to estimate counterfactual responses. The CDVAE model allows for accurate estimation of ITE and captures the underlying heterogeneity in longitudinal data. Evaluations of our model show superior performance over state-of-the-art models.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2302.14368">Enhanced Controllability of Diffusion Models via Feature Disentanglement and Realism-Enhanced Sampling Methods</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Wonwoong Cho, Hareesh Ravi, Midhun Harikumar, Vinh Khuc, Krishna Kumar Singh, Jingwan Lu, David I. Inouye, Ajinkya Kale</div>
        <div class="preprint-abstract">arXiv:2302.14368v4 Announce Type: replace-cross 
Abstract: As Diffusion Models have shown promising performance, a lot of efforts have been made to improve the controllability of Diffusion Models. However, how to train Diffusion Models to have the disentangled latent spaces and how to naturally incorporate the disentangled conditions during the sampling process have been underexplored. In this paper, we present a training framework for feature disentanglement of Diffusion Models (FDiff). We further propose two sampling methods that can boost the realism of our Diffusion Models and also enhance the controllability. Concisely, we train Diffusion Models conditioned on two latent features, a spatial content mask, and a flattened style embedding. We rely on the inductive bias of the denoising process of Diffusion Models to encode pose/layout information in the content feature and semantic/style information in the style feature. Regarding the sampling methods, we first generalize Composable Diffusion Models (GCDM) by breaking the conditional independence assumption to allow for some dependence between conditional inputs, which is shown to be effective in realistic generation in our experiments. Second, we propose timestep-dependent weight scheduling for content and style features to further improve the performance. We also observe better controllability of our proposed methods compared to existing methods in image manipulation and image translation.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2407.10380">NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Pranshu Pandya, Vatsal Gupta, Agney S Talwarr, Tushar Kataria, Dan Roth, Vivek Gupta</div>
        <div class="preprint-abstract">arXiv:2407.10380v3 Announce Type: replace-cross 
Abstract: Cognitive textual and visual reasoning tasks, including puzzles, series, and analogies, demand the ability to quickly reason, decipher, and evaluate patterns both textually and spatially. Due to extensive training on vast amounts of human-curated data, LLMs and VLMs excel in common-sense reasoning tasks, however still struggle with more complex reasoning that demands deeper cognitive understanding. We introduce NTSEBench, a new dataset designed to evaluate cognitive multi-modal reasoning and problem-solving skills of large models. The dataset contains 2728 multiple-choice questions, accompanied by a total of 4,642 images, categorized into 26 different types. These questions are drawn from the nationwide NTSE examination in India and feature a mix of visual and textual general aptitude challenges, designed to assess intelligence and critical thinking skills beyond mere rote learning. We establish baselines on the dataset using state-of-the-art LLMs and VLMs. To facilitate a comparison between open source and propriety models, we propose four distinct modeling strategies to handle different modalities -- text and images -- in the dataset instances.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00310">Detecting and Mitigating Bias in LLMs through Knowledge Graph-Augmented Training</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Rajeev Kumar, Harishankar Kumar, Kumari Shalini</div>
        <div class="preprint-abstract">arXiv:2504.00310v1 Announce Type: cross 
Abstract: Large language models have revolutionized natural language processing with their surprising capability to understand and generate human-like text. However, many of these models inherit and further amplify the biases present in their training data, raising ethical and fairness concerns. The detection and mitigation of such biases are vital to ensuring that LLMs act responsibly and equitably across diverse domains. This work investigates Knowledge Graph-Augmented Training (KGAT) as a novel method to mitigate bias in LLM. Using structured domain-specific knowledge from real-world knowledge graphs, we improve the understanding of the model and reduce biased output. Public datasets for bias assessment include Gender Shades, Bias in Bios, and FairFace, while metrics such as demographic parity and equal opportunity facilitate rigorous detection. We also performed targeted mitigation strategies to correct biased associations, leading to a significant drop in biased output and improved bias metrics. Equipped with real-world datasets and knowledge graphs, our framework is both scalable and effective, paving the way toward responsible deployment in sensitive and high-stakes applications.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00347">Using machine learning method for variable star classification using the TESS Sectors 1-57 data</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Li-Heng Wang, Kai Li, Xiang Gao, Ya-Ni Guo, Guo-You Sun</div>
        <div class="preprint-abstract">arXiv:2504.00347v1 Announce Type: cross 
Abstract: The Transiting Exoplanet Survey Satellite (TESS) is a wide-field all-sky survey mission designed to detect Earth-sized exoplanets. After over four years photometric surveys, data from sectors 1-57, including approximately 1,050,000 light curves with a 2-minute cadence, were collected. By cross-matching the data with Gaia's variable star catalogue, we obtained labeled datasets for further analysis. Using a random forest classifier, we performed classification of variable stars and designed distinct classification processes for each subclass, 6770 EA, 2971 EW, 980 CEP, 8347 DSCT, 457 RRab, 404 RRc and 12348 ROT were identified. Each variable star was visually inspected to ensure the reliability and accuracy of the compiled catalog. Subsequently, we ultimately obtained 6046 EA, 3859 EW, 2058 CEP, 8434 DSCT, 482 RRab, 416 RRc, and 9694 ROT, and a total of 14092 new variable stars were discovered.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00784">CellVTA: Enhancing Vision Foundation Models for Accurate Cell Segmentation and Classification</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Yang Yang, Xijie Xu, Yixun Zhou, Jie Zheng</div>
        <div class="preprint-abstract">arXiv:2504.00784v1 Announce Type: cross 
Abstract: Cell instance segmentation is a fundamental task in digital pathology with broad clinical applications. Recently, vision foundation models, which are predominantly based on Vision Transformers (ViTs), have achieved remarkable success in pathology image analysis. However, their improvements in cell instance segmentation remain limited. A key challenge arises from the tokenization process in ViTs, which substantially reduces the spatial resolution of input images, leading to suboptimal segmentation quality, especially for small and densely packed cells. To address this problem, we propose CellVTA (Cell Vision Transformer with Adapter), a novel method that improves the performance of vision foundation models for cell instance segmentation by incorporating a CNN-based adapter module. This adapter extracts high-resolution spatial information from input images and injects it into the ViT through a cross-attention mechanism. Our method preserves the core architecture of ViT, ensuring seamless integration with pretrained foundation models. Extensive experiments show that CellVTA achieves 0.538 mPQ on the CoNIC dataset and 0.506 mPQ on the PanNuke dataset, which significantly outperforms the state-of-the-art cell segmentation methods. Ablation studies confirm the superiority of our approach over other fine-tuning strategies, including decoder-only fine-tuning and full fine-tuning. Our code and models are publicly available at https://github.com/JieZheng-ShanghaiTech/CellVTA.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00993">MedReason: Eliciting Factual Medical Reasoning Steps in LLMs via Knowledge Graphs</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Juncheng Wu, Wenlong Deng, Xingxuan Li, Sheng Liu, Taomian Mi, Yifan Peng, Ziyang Xu, Yi Liu, Hyunjin Cho, Chang-In Choi, Yihan Cao, Hui Ren, Xiang Li, Xiaoxiao Li, Yuyin Zhou</div>
        <div class="preprint-abstract">arXiv:2504.00993v1 Announce Type: cross 
Abstract: Medical tasks such as diagnosis and treatment planning require precise and complex reasoning, particularly in life-critical domains. Unlike mathematical reasoning, medical reasoning demands meticulous, verifiable thought processes to ensure reliability and accuracy. However, there is a notable lack of datasets that provide transparent, step-by-step reasoning to validate and enhance the medical reasoning ability of AI models. To bridge this gap, we introduce MedReason, a large-scale high-quality medical reasoning dataset designed to enable faithful and explainable medical problem-solving in large language models (LLMs). We utilize a structured medical knowledge graph (KG) to convert clinical QA pairs into logical chains of reasoning, or ``thinking paths'', which trace connections from question elements to answers via relevant KG entities. Each path is validated for consistency with clinical logic and evidence-based medicine. Our pipeline generates detailed reasoning for various medical questions from 7 medical datasets, resulting in a dataset of 32,682 question-answer pairs, each with detailed, step-by-step explanations. Experiments demonstrate that fine-tuning with our dataset consistently boosts medical problem-solving capabilities, achieving significant gains of up to 7.7% for DeepSeek-Ditill-8B. Our top-performing model, MedReason-8B, outperforms the Huatuo-o1-8B, a state-of-the-art medical reasoning model, by up to 4.2% on the clinical benchmark MedBullets. We also engage medical professionals from diverse specialties to assess our dataset's quality, ensuring MedReason offers accurate and coherent medical reasoning. Our data, models, and code will be publicly available.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2409.06366">One Policy to Run Them All: an End-to-end Learning Approach to Multi-Embodiment Locomotion</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Nico Bohlinger, Grzegorz Czechmanowski, Maciej Krupka, Piotr Kicki, Krzysztof Walas, Jan Peters, Davide Tateo</div>
        <div class="preprint-abstract">arXiv:2409.06366v3 Announce Type: replace-cross 
Abstract: Deep Reinforcement Learning techniques are achieving state-of-the-art results in robust legged locomotion. While there exists a wide variety of legged platforms such as quadruped, humanoids, and hexapods, the field is still missing a single learning framework that can control all these different embodiments easily and effectively and possibly transfer, zero or few-shot, to unseen robot embodiments. We introduce URMA, the Unified Robot Morphology Architecture, to close this gap. Our framework brings the end-to-end Multi-Task Reinforcement Learning approach to the realm of legged robots, enabling the learned policy to control any type of robot morphology. The key idea of our method is to allow the network to learn an abstract locomotion controller that can be seamlessly shared between embodiments thanks to our morphology-agnostic encoders and decoders. This flexible architecture can be seen as a potential first step in building a foundation model for legged robot locomotion. Our experiments show that URMA can learn a locomotion policy on multiple embodiments that can be easily transferred to unseen robot platforms in simulation and the real world.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00300">From Chaos to Coherence: Effects of High-Order Synaptic Correlations on Neural Dynamics</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Nimrod Sherf, Xaq Pitkow, Kre\v{s}imir Josi\'c, Kevin E. Bassler</div>
        <div class="preprint-abstract">arXiv:2504.00300v1 Announce Type: cross 
Abstract: Recurrent Neural Network models have elucidated the interplay between structure and dynamics in biological neural networks, particularly the emergence of irregular and rhythmic activities in cortex. However, most studies have focused on networks with random or simple connectivity structures. Experimental observations find that high-order cortical connectivity patterns affect the temporal patterns of network activity, but a theory that relates such complex structure to network dynamics has yet to be developed. Here, we show that third- and higher-order cyclic correlations in synaptic connectivities greatly impact neuronal dynamics. Specifically, strong cyclic correlations in a network suppress chaotic dynamics, and promote oscillatory or fixed activity. The change in dynamics is related to the form of the unstable eigenvalues of the random connectivity matrix. A phase transition from chaotic to fixed or oscillatory activity coincides with the development of a cusp at the leading edge of the eigenvalue support. We also relate the dimensions of activity to the network structure.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00522">MARIOH: Multiplicity-Aware Hypergraph Reconstruction</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Kyuhan Lee, Geon Lee, Kijung Shin</div>
        <div class="preprint-abstract">arXiv:2504.00522v1 Announce Type: cross 
Abstract: Hypergraphs offer a powerful framework for modeling higher-order interactions that traditional pairwise graphs cannot fully capture. However, practical constraints often lead to their simplification into projected graphs, resulting in substantial information loss and ambiguity in representing higher-order relationships. In this work, we propose MARIOH, a supervised approach for reconstructing the original hypergraph from its projected graph by leveraging edge multiplicity. To overcome the difficulties posed by the large search space, MARIOH integrates several key ideas: (a) identifying provable size-2 hyperedges, which reduces the candidate search space, (b) predicting the likelihood of candidates being hyperedges by utilizing both structural and multiplicity-related features, and (c) not only targeting promising hyperedge candidates but also examining less confident ones to explore alternative possibilities. Together, these ideas enable MARIOH to efficiently and effectively explore the search space. In our experiments using 10 real-world datasets, MARIOH achieves up to 74.51% higher reconstruction accuracy compared to state-of-the-art methods.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00277">Rack Position Optimization in Large-Scale Heterogeneous Data Centers</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Chang-Lin Chen, Jiayu Chen, Tian Lan, Zhaoxia Zhao, Hongbo Dong, Vaneet Aggarwal</div>
        <div class="preprint-abstract">arXiv:2504.00277v1 Announce Type: new 
Abstract: As rapidly growing AI computational demands accelerate the need for new hardware installation and maintenance, this work explores optimal data center resource management by balancing operational efficiency with fault tolerance through strategic rack positioning considering diverse resources and locations. Traditional mixed-integer programming (MIP) approaches often struggle with scalability, while heuristic methods may result in significant sub-optimality. To address these issues, this paper presents a novel two-tier optimization framework using a high-level deep reinforcement learning (DRL) model to guide a low-level gradient-based heuristic for local search. The high-level DRL agent employs Leader Reward for optimal rack type ordering, and the low-level heuristic efficiently maps racks to positions, minimizing movement counts and ensuring fault-tolerant resource distribution. This approach allows scalability to over 100,000 positions and 100 rack types. Our method outperformed the gradient-based heuristic by 7\% on average and the MIP solver by over 30\% in objective value. It achieved a 100\% success rate versus MIP's 97.5\% (within a 20-minute limit), completing in just 2 minutes compared to MIP's 1630 minutes (i.e., almost 4 orders of magnitude improvement). Unlike the MIP solver, which showed performance variability under time constraints and high penalties, our algorithm consistently delivered stable, efficient results - an essential feature for large-scale data center management.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.01007">Data-Driven Safety Verification using Barrier Certificates and Matrix Zonotopes</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Mohammed Adib Oumer, Amr Alanwar, Majid Zamani</div>
        <div class="preprint-abstract">arXiv:2504.01007v1 Announce Type: cross 
Abstract: Ensuring safety in cyber-physical systems (CPSs) is a critical challenge, especially when system models are difficult to obtain or cannot be fully trusted due to uncertainty, modeling errors, or environmental disturbances. Traditional model-based approaches rely on precise system dynamics, which may not be available in real-world scenarios. To address this, we propose a data-driven safety verification framework that leverages matrix zonotopes and barrier certificates to verify system safety directly from noisy data. Instead of trusting a single unreliable model, we construct a set of models that capture all possible system dynamics that align with the observed data, ensuring that the true system model is always contained within this set. This model set is compactly represented using matrix zonotopes, enabling efficient computation and propagation of uncertainty. By integrating this representation into a barrier certificate framework, we establish rigorous safety guarantees without requiring an explicit system model. Numerical experiments demonstrate the effectiveness of our approach in verifying safety for dynamical systems with unknown models, showcasing its potential for real-world CPS applications.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2412.19867">Data-Free Group-Wise Fully Quantized Winograd Convolution via Learnable Scales</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Shuokai Pan, Gerti Tuzi, Sudarshan Sreeram, Dibakar Gope</div>
        <div class="preprint-abstract">arXiv:2412.19867v2 Announce Type: replace-cross 
Abstract: Despite the revolutionary breakthroughs of large-scale text-to-image diffusion models for complex vision and downstream tasks, their extremely high computational and storage costs limit their usability. Quantization of diffusion models has been explored in recent works to reduce compute costs and memory bandwidth usage. To further improve inference time, fast convolution algorithms such as Winograd can be used for convolution layers, which account for a significant portion of computations in diffusion models. However, the significant quality loss of fully quantized Winograd using existing coarser-grained post-training quantization methods, combined with the complexity and cost of finetuning the Winograd transformation matrices for such large models to recover quality, makes them unsuitable for large-scale foundation models. Motivated by the presence of a large range of values in them, we investigate the impact of finer-grained group-wise quantization in quantizing diffusion models. While group-wise quantization can largely handle the fully quantized Winograd convolution, it struggles to deal with the large distribution imbalance in a sizable portion of the Winograd domain computation. To reduce range differences in the Winograd domain, we propose finetuning only the scale parameters of the Winograd transform matrices without using any domain-specific training data. Because our method does not depend on any training data, the generalization performance of quantized diffusion models is safely guaranteed. For text-to-image generation task, the 8-bit fully-quantized diffusion model with Winograd provides near-lossless quality (FID and CLIP scores) in comparison to the full-precision model. For image classification, our method outperforms the state-of-the-art Winograd PTQ method by 1.62% and 2.56% in top-1 ImageNet accuracy on ResNet18 and ResNet-34, respectively, with Winograd F(6, 3).</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00031">Leaking LoRa: An Evaluation of Password Leaks and Knowledge Storage in Large Language Models</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Ryan Marinelli, Magnus Eckhoff</div>
        <div class="preprint-abstract">arXiv:2504.00031v1 Announce Type: cross 
Abstract: To effectively deploy Large Language Models (LLMs) in application-specific settings, fine-tuning techniques are applied to enhance performance on specialized tasks. This process often involves fine-tuning on user data data, which may contain sensitive information. Although not recommended, it is not uncommon for users to send passwords in messages, and fine-tuning models on this could result in passwords being leaked. In this study, a Large Language Model is fine-tuned with customer support data and passwords from the RockYou password wordlist using Low-Rank Adaptation (LoRA). Out of the first 200 passwords from the list, 37 were successfully recovered. Further, causal tracing is used to identify that password information is largely located in a few layers. Lastly, Rank One Model Editing (ROME) is used to remove the password information from the model, resulting in the number of passwords recovered going from 37 to 0.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2412.01095">VERA: Explainable Video Anomaly Detection via Verbalized Learning of Vision-Language Models</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Muchao Ye, Weiyang Liu, Pan He</div>
        <div class="preprint-abstract">arXiv:2412.01095v3 Announce Type: replace 
Abstract: The rapid advancement of vision-language models (VLMs) has established a new paradigm in video anomaly detection (VAD): leveraging VLMs to simultaneously detect anomalies and provide comprehendible explanations for the decisions. Existing work in this direction often assumes the complex reasoning required for VAD exceeds the capabilities of pretrained VLMs. Consequently, these approaches either incorporate specialized reasoning modules during inference or rely on instruction tuning datasets through additional training to adapt VLMs for VAD. However, such strategies often incur substantial computational costs or data annotation overhead. To address these challenges in explainable VAD, we introduce a verbalized learning framework named VERA that enables VLMs to perform VAD without model parameter modifications. Specifically, VERA automatically decomposes the complex reasoning required for VAD into reflections on simpler, more focused guiding questions capturing distinct abnormal patterns. It treats these reflective questions as learnable parameters and optimizes them through data-driven verbal interactions between learner and optimizer VLMs, using coarsely labeled training data. During inference, VERA embeds the learned questions into model prompts to guide VLMs in generating segment-level anomaly scores, which are then refined into frame-level scores via the fusion of scene and temporal contexts. Experimental results on challenging benchmarks demonstrate that the learned questions of VERA are highly adaptable, significantly improving both detection performance and explainability of VLMs for VAD.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00068">Integrating Quantum-Classical Attention in Patch Transformers for Enhanced Time Series Forecasting</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Sanjay Chakraborty, Fredrik Heintz</div>
        <div class="preprint-abstract">arXiv:2504.00068v1 Announce Type: new 
Abstract: QCAAPatchTF is a quantum attention network integrated with an advanced patch-based transformer, designed for multivariate time series forecasting, classification, and anomaly detection. Leveraging quantum superpositions, entanglement, and variational quantum eigensolver principles, the model introduces a quantum-classical hybrid self-attention mechanism to capture multivariate correlations across time points. For multivariate long-term time series, the quantum self-attention mechanism can reduce computational complexity while maintaining temporal relationships. It then applies the quantum-classical hybrid self-attention mechanism alongside a feed-forward network in the encoder stage of the advanced patch-based transformer. While the feed-forward network learns nonlinear representations for each variable frame, the quantum self-attention mechanism processes individual series to enhance multivariate relationships. The advanced patch-based transformer computes the optimized patch length by dividing the sequence length into a fixed number of patches instead of using an arbitrary set of values. The stride is then set to half of the patch length to ensure efficient overlapping representations while maintaining temporal continuity. QCAAPatchTF achieves state-of-the-art performance in both long-term and short-term forecasting, classification, and anomaly detection tasks, demonstrating state-of-the-art accuracy and efficiency on complex real-world datasets.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00428">LLM-Assisted Proactive Threat Intelligence for Automated Reasoning</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Shuva Paul, Farhad Alemi, Richard Macwan</div>
        <div class="preprint-abstract">arXiv:2504.00428v1 Announce Type: cross 
Abstract: Successful defense against dynamically evolving cyber threats requires advanced and sophisticated techniques. This research presents a novel approach to enhance real-time cybersecurity threat detection and response by integrating large language models (LLMs) and Retrieval-Augmented Generation (RAG) systems with continuous threat intelligence feeds. Leveraging recent advancements in LLMs, specifically GPT-4o, and the innovative application of RAG techniques, our approach addresses the limitations of traditional static threat analysis by incorporating dynamic, real-time data sources. We leveraged RAG to get the latest information in real-time for threat intelligence, which is not possible in the existing GPT-4o model. We employ the Patrowl framework to automate the retrieval of diverse cybersecurity threat intelligence feeds, including Common Vulnerabilities and Exposures (CVE), Common Weakness Enumeration (CWE), Exploit Prediction Scoring System (EPSS), and Known Exploited Vulnerabilities (KEV) databases, and integrate these with the all-mpnet-base-v2 model for high-dimensional vector embeddings, stored and queried in Milvus. We demonstrate our system's efficacy through a series of case studies, revealing significant improvements in addressing recently disclosed vulnerabilities, KEVs, and high-EPSS-score CVEs compared to the baseline GPT-4o. This work not only advances the role of LLMs in cybersecurity but also establishes a robust foundation for the development of automated intelligent cyberthreat information management systems, addressing crucial gaps in current cybersecurity practices.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00584">Enhancing Negation Awareness in Universal Text Embeddings: A Data-efficient and Computational-efficient Approach</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Hongliu Cao</div>
        <div class="preprint-abstract">arXiv:2504.00584v1 Announce Type: cross 
Abstract: Negation plays an important role in various natural language processing tasks such as Natural Language Inference and Sentiment Analysis tasks. Numerous prior studies have found that contextual text embedding models such as BERT, ELMO, RoBERTa or XLNet face challenges in accurately understanding negation. Recent advancements in universal text embeddings have demonstrated superior performance over contextual text embeddings in various tasks. However, due to the bias in popular evaluation benchmarks, the negation awareness capacity of these models remains unclear. To bridge the gap in existing literature, an in-depth analysis is initiated in this work to study the negation awareness of cutting-edge universal text embedding models. Our findings reveal a significant lack of negation awareness in these models, often interpreting negated text pairs as semantically similar. To efficiently deal with the conflict that different tasks need different trade-offs between topic and negation information among other semantic information, a data-efficient and computational-efficient embedding re-weighting method is proposed without modifying the parameters of text embedding models. The proposed solution is able to improve text embedding models' negation awareness significantly on both simple negation understanding task and complex negation understanding task. Furthermore, the proposed solution can also significantly improve the negation awareness of Large Language Model based task-specific high dimensional universal text embeddings.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2407.00805">Towards shutdownable agents via stochastic choice</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Elliott Thornley, Alexander Roman, Christos Ziakas, Leyton Ho, Louis Thomson</div>
        <div class="preprint-abstract">arXiv:2407.00805v5 Announce Type: replace 
Abstract: The Incomplete Preferences Proposal (IPP) is an idea for ensuring that advanced artificial agents never resist shutdown. A key part of the IPP is using a novel `Discounted Reward for Same-Length Trajectories (DReST)' reward function to train agents to (1) pursue goals effectively conditional on each trajectory-length (be `USEFUL'), and (2) choose stochastically between different trajectory-lengths (be `NEUTRAL' about trajectory-lengths). In this paper, we propose evaluation metrics for USEFULNESS and NEUTRALITY. We use a DReST reward function to train simple agents to navigate gridworlds, and we find that these agents learn to be USEFUL and NEUTRAL. Our results thus provide some initial evidence that DReST reward functions could train advanced agents to be USEFUL and NEUTRAL. Our theoretical work suggests that these agents would be useful and shutdownable.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00149">Towards Precise Action Spotting: Addressing Temporal Misalignment in Labels with Dynamic Label Assignment</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Masato Tamura</div>
        <div class="preprint-abstract">arXiv:2504.00149v1 Announce Type: cross 
Abstract: Precise action spotting has attracted considerable attention due to its promising applications. While existing methods achieve substantial performance by employing well-designed model architecture, they overlook a significant challenge: the temporal misalignment inherent in ground-truth labels. This misalignment arises when frames labeled as containing events do not align accurately with the actual event times, often as a result of human annotation errors or the inherent difficulties in precisely identifying event boundaries across neighboring frames. To tackle this issue, we propose a novel dynamic label assignment strategy that allows predictions to have temporal offsets from ground-truth action times during training, ensuring consistent event spotting. Our method extends the concept of minimum-cost matching, which is utilized in the spatial domain for object detection, to the temporal domain. By calculating matching costs based on predicted action class scores and temporal offsets, our method dynamically assigns labels to the most likely predictions, even when the predicted times of these predictions deviate from ground-truth times, alleviating the negative effects of temporal misalignment in labels. We conduct extensive experiments and demonstrate that our method achieves state-of-the-art performance, particularly in conditions where events are visually distinct and temporal misalignment in labels is common.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2408.04667">Non-Determinism of "Deterministic" LLM Settings</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Berk Atil, Sarp Aykent, Alexa Chittams, Lisheng Fu, Rebecca J. Passonneau, Evan Radcliffe, Guru Rajan Rajagopal, Adam Sloan, Tomasz Tudrej, Ferhan Ture, Zhe Wu, Lixinyu Xu, Breck Baldwin</div>
        <div class="preprint-abstract">arXiv:2408.04667v4 Announce Type: replace-cross 
Abstract: LLM (large language model) practitioners commonly notice that outputs can vary for the same inputs under settings expected to be deterministic. Yet the questions of how pervasive this is, and with what impact on results, have not to our knowledge been systematically investigated. We investigate non-determinism in five LLMs configured to be deterministic when applied to eight common tasks in across 10 runs, in both zero-shot and few-shot settings. We see accuracy variations up to 15% across naturally occurring runs with a gap of best possible performance to worst possible performance up to 70%. In fact, none of the LLMs consistently delivers repeatable accuracy across all tasks, much less identical output strings. Sharing preliminary results with insiders has revealed that non-determinism perhaps essential to the efficient use of compute resources via co-mingled data in input buffers so this issue is not going away anytime soon. To better quantify our observations, we introduce metrics focused on quantifying determinism, TARr@N for the total agreement rate at N runs over raw output, and TARa@N for total agreement rate of parsed-out answers. Our code and data are publicly available at http://github.com/REDACTED.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2407.12787">GameVibe: A Multimodal Affective Game Corpus</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Matthew Barthet, Maria Kaselimi, Kosmas Pinitas, Konstantinos Makantasis, Antonios Liapis, Georgios N. Yannakakis</div>
        <div class="preprint-abstract">arXiv:2407.12787v2 Announce Type: replace-cross 
Abstract: As online video and streaming platforms continue to grow, affective computing research has undergone a shift towards more complex studies involving multiple modalities. However, there is still a lack of readily available datasets with high-quality audiovisual stimuli. In this paper, we present GameVibe, a novel affect corpus which consists of multimodal audiovisual stimuli, including in-game behavioural observations and third-person affect traces for viewer engagement. The corpus consists of videos from a diverse set of publicly available gameplay sessions across 30 games, with particular attention to ensure high-quality stimuli with good audiovisual and gameplay diversity. Furthermore, we present an analysis on the reliability of the annotators in terms of inter-annotator agreement.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00409">Semantic Mastery: Enhancing LLMs with Advanced Natural Language Understanding</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Mohanakrishnan Hariharan</div>
        <div class="preprint-abstract">arXiv:2504.00409v1 Announce Type: cross 
Abstract: Large language models (LLMs) have greatly improved their capability in performing NLP tasks. However, deeper semantic understanding, contextual coherence, and more subtle reasoning are still difficult to obtain. The paper discusses state-of-the-art methodologies that advance LLMs with more advanced NLU techniques, such as semantic parsing, knowledge integration, and contextual reinforcement learning. We analyze the use of structured knowledge graphs, retrieval-augmented generation (RAG), and fine-tuning strategies that match models with human-level understanding. Furthermore, we address the incorporation of transformer-based architectures, contrastive learning, and hybrid symbolic-neural methods that address problems like hallucinations, ambiguity, and inconsistency in the factual perspectives involved in performing complex NLP tasks, such as question-answering text summarization and dialogue generation. Our findings show the importance of semantic precision for enhancing AI-driven language systems and suggest future research directions to bridge the gap between statistical language models and true natural language understanding.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00020">Celler:A Genomic Language Model for Long-Tailed Single-Cell Annotation</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Huan Zhao, Yiming Liu, Jina Yao, Ling Xiong, Zexin Zhou, Zixing Zhang</div>
        <div class="preprint-abstract">arXiv:2504.00020v1 Announce Type: cross 
Abstract: Recent breakthroughs in single-cell technology have ushered in unparalleled opportunities to decode the molecular intricacy of intricate biological systems, especially those linked to diseases unique to humans. However, these progressions have also ushered in novel obstacles-specifically, the efficient annotation of extensive, long-tailed single-cell data pertaining to disease conditions. To effectively surmount this challenge, we introduce Celler, a state-of-the-art generative pre-training model crafted specifically for the annotation of single-cell data. Celler incorporates two groundbreaking elements: First, we introduced the Gaussian Inflation (GInf) Loss function. By dynamically adjusting sample weights, GInf Loss significantly enhances the model's ability to learn from rare categories while reducing the risk of overfitting for common categories. Secondly, we introduce an innovative Hard Data Mining (HDM) strategy into the training process, specifically targeting the challenging-to-learn minority data samples, which significantly improved the model's predictive accuracy. Additionally, to further advance research in this field, we have constructed a large-scale single-cell dataset: Celler-75, which encompasses 40 million cells distributed across 80 human tissues and 75 specific diseases. This dataset provides critical support for comprehensively exploring the potential of single-cell technology in disease research. Our code is available at https://github.com/AI4science-ym/HiCeller.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.22829">Nonhuman Primate Brain Tissue Segmentation Using a Transfer Learning Approach</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Zhen Lin, Hongyu Yuan, Richard Barcus, Qing Lyu, Sucheta Chakravarty, Megan E. Lipford, Carol A. Shively, Suzanne Craft, Mohammad Kawas, Jeongchul Kim, Christopher T. Whitlow</div>
        <div class="preprint-abstract">arXiv:2503.22829v2 Announce Type: replace-cross 
Abstract: Non-human primates (NHPs) serve as critical models for understanding human brain function and neurological disorders due to their close evolutionary relationship with humans. Accurate brain tissue segmentation in NHPs is critical for understanding neurological disorders, but challenging due to the scarcity of annotated NHP brain MRI datasets, the small size of the NHP brain, the limited resolution of available imaging data and the anatomical differences between human and NHP brains. To address these challenges, we propose a novel approach utilizing STU-Net with transfer learning to leverage knowledge transferred from human brain MRI data to enhance segmentation accuracy in the NHP brain MRI, particularly when training data is limited. The combination of STU-Net and transfer learning effectively delineates complex tissue boundaries and captures fine anatomical details specific to NHP brains. Notably, our method demonstrated improvement in segmenting small subcortical structures such as putamen and thalamus that are challenging to resolve with limited spatial resolution and tissue contrast, and achieved DSC of over 0.88, IoU over 0.8 and HD95 under 7. This study introduces a robust method for multi-class brain tissue segmentation in NHPs, potentially accelerating research in evolutionary neuroscience and preclinical studies of neurological disorders relevant to human health.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00294">Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies Ahead</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Vidhisha Balachandran, Jingya Chen, Lingjiao Chen, Shivam Garg, Neel Joshi, Yash Lara, John Langford, Besmira Nushi, Vibhav Vineet, Yue Wu, Safoora Yousefi</div>
        <div class="preprint-abstract">arXiv:2504.00294v1 Announce Type: new 
Abstract: Inference-time scaling can enhance the reasoning capabilities of large language models (LLMs) on complex problems that benefit from step-by-step problem solving. Although lengthening generated scratchpads has proven effective for mathematical tasks, the broader impact of this approach on other tasks remains less clear. In this work, we investigate the benefits and limitations of scaling methods across nine state-of-the-art models and eight challenging tasks, including math and STEM reasoning, calendar planning, NP-hard problems, navigation, and spatial reasoning. We compare conventional models (e.g., GPT-4o) with models fine-tuned for inference-time scaling (e.g., o1) through evaluation protocols that involve repeated model calls, either independently or sequentially with feedback. These evaluations approximate lower and upper performance bounds and potential for future performance improvements for each model, whether through enhanced training or multi-model inference systems. Our extensive empirical analysis reveals that the advantages of inference-time scaling vary across tasks and diminish as problem complexity increases. In addition, simply using more tokens does not necessarily translate to higher accuracy in these challenging regimes. Results from multiple independent runs with conventional models using perfect verifiers show that, for some tasks, these models can achieve performance close to the average performance of today's most advanced reasoning models. However, for other tasks, a significant performance gap remains, even in very high scaling regimes. Encouragingly, all models demonstrate significant gains when inference is further scaled with perfect verifiers or strong feedback, suggesting ample potential for future improvements.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00163">Does "Reasoning" with Large Language Models Improve Recognizing, Generating, and Reframing Unhelpful Thoughts?</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Yilin Qi, Dong Won Lee, Cynthia Breazeal, Hae Won Park</div>
        <div class="preprint-abstract">arXiv:2504.00163v1 Announce Type: cross 
Abstract: Cognitive Reframing, a core element of Cognitive Behavioral Therapy (CBT), helps individuals reinterpret negative experiences by finding positive meaning. Recent advances in Large Language Models (LLMs) have demonstrated improved performance through reasoning-based strategies. This inspires a promising direction of leveraging the reasoning capabilities of LLMs to improve CBT and mental reframing by simulating the process of critical thinking, potentially enabling more effective recognition, generation, and reframing of cognitive distortions. In this work, we investigate the role of various reasoning methods, including pre-trained reasoning LLMs and augmented reasoning strategies such as CoT and self-consistency in enhancing LLMs' ability to perform cognitive reframing tasks. We find that augmented reasoning methods, even when applied to "outdated" LLMs like GPT-3.5, consistently outperform state-of-the-art pretrained reasoning models on recognizing, generating and reframing unhelpful thoughts.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.02806">Viscosity of polymer melts using non-affine theory based on vibrational modes</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Ankit Singh, Vinay Vaibhav, Alessio Zaccone</div>
        <div class="preprint-abstract">arXiv:2503.02806v3 Announce Type: replace-cross 
Abstract: Viscosity, a fundamental transport and rheological property of liquids, quantifies the resistance to relative motion between molecular layers and plays a critical role in understanding material behavior. Conventional methods, such as the Green-Kubo (GK) approach, rely on time integration of correlation functions, which becomes computationally intensive near the glass transition due to slow correlation decay. A recently proposed method based on non-affine lattice dynamics (NALD) and instantaneous normal mode analysis offers a promising alternative for estimating the viscosity. In this study, we apply the NALD approach to compute the viscosity of the Kremer-Grest polymer system over a range of temperatures and compare these results with those from the GK method and non-equilibrium molecular dynamics simulations. Our findings reveal that all vibration modes, including the instantaneous normal modes, contribute to the viscosity. This work presents an efficient framework for calculating viscosity across diverse systems, including near the glass transition where the GK method is no longer applicable. Also, it opens the avenue to understanding the role of different vibrational modes linked with structure, facilitating the design of materials with tunable rheological properties.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00336">SeizureTransformer: Scaling U-Net with Transformer for Simultaneous Time-Step Level Seizure Detection from Long EEG Recordings</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Kerui Wu, Ziyue Zhao, B\"ulent Yener</div>
        <div class="preprint-abstract">arXiv:2504.00336v1 Announce Type: new 
Abstract: Epilepsy is a common neurological disorder that affects around 65 million people worldwide. Detecting seizures quickly and accurately is vital, given the prevalence and severity of the associated complications. Recently, deep learning-based automated seizure detection methods have emerged as solutions; however, most existing methods require extensive post-processing and do not effectively handle the crucial long-range patterns in EEG data. In this work, we propose SeizureTransformer, a simple model comprised of (i) a deep encoder comprising 1D convolutions (ii) a residual CNN stack and a transformer encoder to embed previous output into high-level representation with contextual information, and (iii) streamlined decoder which converts these features into a sequence of probabilities, directly indicating the presence or absence of seizures at every time step. Extensive experiments on public and private EEG seizure detection datasets demonstrate that our model significantly outperforms existing approaches (ranked in the first place in the 2025 "seizure detection challenge" organized in the International Conference on Artificial Intelligence in Epilepsy and Other Neurological Disorders), underscoring its potential for real-time, precise seizure detection.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2410.03463">Diffusion State-Guided Projected Gradient for Inverse Problems</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Rayhan Zirvi, Bahareh Tolooshams, Anima Anandkumar</div>
        <div class="preprint-abstract">arXiv:2410.03463v5 Announce Type: replace 
Abstract: Recent advancements in diffusion models have been effective in learning data priors for solving inverse problems. They leverage diffusion sampling steps for inducing a data prior while using a measurement guidance gradient at each step to impose data consistency. For general inverse problems, approximations are needed when an unconditionally trained diffusion model is used since the measurement likelihood is intractable, leading to inaccurate posterior sampling. In other words, due to their approximations, these methods fail to preserve the generation process on the data manifold defined by the diffusion prior, leading to artifacts in applications such as image restoration. To enhance the performance and robustness of diffusion models in solving inverse problems, we propose Diffusion State-Guided Projected Gradient (DiffStateGrad), which projects the measurement gradient onto a subspace that is a low-rank approximation of an intermediate state of the diffusion process. DiffStateGrad, as a module, can be added to a wide range of diffusion-based inverse solvers to improve the preservation of the diffusion process on the prior manifold and filter out artifact-inducing components. We highlight that DiffStateGrad improves the robustness of diffusion models in terms of the choice of measurement guidance step size and noise while improving the worst-case performance. Finally, we demonstrate that DiffStateGrad improves upon the state-of-the-art on linear and nonlinear image restoration inverse problems. Our code is available at https://github.com/Anima-Lab/DiffStateGrad.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00349">Reducing Smoothness with Expressive Memory Enhanced Hierarchical Graph Neural Networks</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Thomas Bailie, Yun Sing Koh, S. Karthik Mukkavilli, Varvara Vetrova</div>
        <div class="preprint-abstract">arXiv:2504.00349v1 Announce Type: new 
Abstract: Graphical forecasting models learn the structure of time series data via projecting onto a graph, with recent techniques capturing spatial-temporal associations between variables via edge weights. Hierarchical variants offer a distinct advantage by analysing the time series across multiple resolutions, making them particularly effective in tasks like global weather forecasting, where low-resolution variable interactions are significant. A critical challenge in hierarchical models is information loss during forward or backward passes through the hierarchy. We propose the Hierarchical Graph Flow (HiGFlow) network, which introduces a memory buffer variable of dynamic size to store previously seen information across variable resolutions. We theoretically show two key results: HiGFlow reduces smoothness when mapping onto new feature spaces in the hierarchy and non-strictly enhances the utility of message-passing by improving Weisfeiler-Lehman (WL) expressivity. Empirical results demonstrate that HiGFlow outperforms state-of-the-art baselines, including transformer models, by at least an average of 6.1% in MAE and 6.2% in RMSE. Code is available at https://github.com/TB862/ HiGFlow.git.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00624">Feature Subset Weighting for Distance-based Supervised Learning through Choquet Integration</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Adnan Theerens, Yvan Saeys, Chris Cornelis</div>
        <div class="preprint-abstract">arXiv:2504.00624v1 Announce Type: new 
Abstract: This paper introduces feature subset weighting using monotone measures for distance-based supervised learning. The Choquet integral is used to define a distance metric that incorporates these weights. This integration enables the proposed distances to effectively capture non-linear relationships and account for interactions both between conditional and decision attributes and among conditional attributes themselves, resulting in a more flexible distance measure. In particular, we show how this approach ensures that the distances remain unaffected by the addition of duplicate and strongly correlated features. Another key point of this approach is that it makes feature subset weighting computationally feasible, since only $m$ feature subset weights should be calculated each time instead of calculating all feature subset weights ($2^m$), where $m$ is the number of attributes. Next, we also examine how the use of the Choquet integral for measuring similarity leads to a non-equivalent definition of distance. The relationship between distance and similarity is further explored through dual measures. Additionally, symmetric Choquet distances and similarities are proposed, preserving the classical symmetry between similarity and distance. Finally, we introduce a concrete feature subset weighting distance, evaluate its performance in a $k$-nearest neighbors (KNN) classification setting, and compare it against Mahalanobis distances and weighted distance methods.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00046">Multi-Stakeholder Disaster Insights from Social Media Using Large Language Models</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Loris Belcastro, Cristian Cosentino, Fabrizio Marozzo, Merve G\"und\"uz-C\"ure, \c{S}ule \"Ozt\"urk-Birim</div>
        <div class="preprint-abstract">arXiv:2504.00046v1 Announce Type: cross 
Abstract: In recent years, social media has emerged as a primary channel for users to promptly share feedback and issues during disasters and emergencies, playing a key role in crisis management. While significant progress has been made in collecting and analyzing social media content, there remains a pressing need to enhance the automation, aggregation, and customization of this data to deliver actionable insights tailored to diverse stakeholders, including the press, police, EMS, and firefighters. This effort is essential for improving the coordination of activities such as relief efforts, resource distribution, and media communication. This paper presents a methodology that leverages the capabilities of LLMs to enhance disaster response and management. Our approach combines classification techniques with generative AI to bridge the gap between raw user feedback and stakeholder-specific reports. Social media posts shared during catastrophic events are analyzed with a focus on user-reported issues, service interruptions, and encountered challenges. We employ full-spectrum LLMs, using analytical models like BERT for precise, multi-dimensional classification of content type, sentiment, emotion, geolocation, and topic. Generative models such as ChatGPT are then used to produce human-readable, informative reports tailored to distinct audiences, synthesizing insights derived from detailed classifications. We compare standard approaches, which analyze posts directly using prompts in ChatGPT, to our advanced method, which incorporates multi-dimensional classification, sub-event selection, and tailored report generation. Our methodology demonstrates superior performance in both quantitative metrics, such as text coherence scores and latent representations, and qualitative assessments by automated tools and field experts, delivering precise insights for diverse disaster response stakeholders.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2403.13164">VL-ICL Bench: The Devil in the Details of Multimodal In-Context Learning</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Yongshuo Zong, Ondrej Bohdal, Timothy Hospedales</div>
        <div class="preprint-abstract">arXiv:2403.13164v4 Announce Type: replace 
Abstract: Large language models (LLMs) famously exhibit emergent in-context learning (ICL) -- the ability to rapidly adapt to new tasks using few-shot examples provided as a prompt, without updating the model's weights. Built on top of LLMs, vision large language models (VLLMs) have advanced significantly in areas such as recognition, reasoning, and grounding. However, investigations into \emph{multimodal ICL} have predominantly focused on few-shot visual question answering (VQA), and image captioning, which we will show neither exploit the strengths of ICL, nor test its limitations. The broader capabilities and limitations of multimodal ICL remain under-explored. In this study, we introduce a comprehensive benchmark VL-ICL Bench for multimodal in-context learning, encompassing a broad spectrum of tasks that involve both images and text as inputs and outputs, and different types of challenges, from {perception to reasoning and long context length}. We evaluate the abilities of state-of-the-art VLLMs against this benchmark suite, revealing their diverse strengths and weaknesses, and showing that even the most advanced models, such as GPT-4, find the tasks challenging. By highlighting a range of new ICL tasks, and the associated strengths and limitations of existing models, we hope that our dataset will inspire future work on enhancing the in-context learning capabilities of VLLMs, as well as inspire new applications that leverage VLLM ICL. The code and dataset are available at https://github.com/ys-zong/VL-ICL.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2403.02645">DT-DDNN: A Physical Layer Security Attack Detector in 5G RF Domain for CAVs</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Ghazal Asemian, Mohammadreza Amini, Burak Kantarci, Melike Erol-Kantarci</div>
        <div class="preprint-abstract">arXiv:2403.02645v3 Announce Type: replace-cross 
Abstract: The Synchronization Signal Block (SSB) is a fundamental component of the 5G New Radio (NR) air interface, crucial for the initial access procedure of Connected and Automated Vehicles (CAVs), and serves several key purposes in the network's operation. However, due to the predictable nature of SSB transmission, including the Primary and Secondary Synchronization Signals (PSS and SSS), jamming attacks are critical threats. These attacks, which can be executed without requiring high power or complex equipment, pose substantial risks to the 5G network, particularly as a result of the unencrypted transmission of control signals. Leveraging RF domain knowledge, this work presents a novel deep learning-based technique for detecting jammers in CAV networks. Unlike the existing jamming detection algorithms that mostly rely on network parameters, we introduce a double-threshold deep learning jamming detector by focusing on the SSB. The detection method is focused on RF domain features and improves the robustness of the network without requiring integration with the pre-existing network infrastructure. By integrating a preprocessing block to extract PSS correlation and energy per null resource elements (EPNRE) characteristics, our method distinguishes between normal and jammed received signals with high precision. Additionally, by incorporating of Discrete Wavelet Transform (DWT), the efficacy of training and detection are optimized. A double-threshold double Deep Neural Network (DT-DDNN) is also introduced to the architecture complemented by a deep cascade learning model to increase the sensitivity of the model to variations of signal-to-jamming noise ratio (SJNR). Results show that the proposed method achieves 96.4% detection rate in extra low jamming power, i.e., SJNR between 15 to 30 dB. Further, performance of DT-DDNN is validated by analyzing real 5G signals obtained from a practical testbed.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.22988">DC-SGD: Differentially Private SGD with Dynamic Clipping through Gradient Norm Distribution Estimation</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Chengkun Wei, Weixian Li, Chen Gong, Wenzhi Chen</div>
        <div class="preprint-abstract">arXiv:2503.22988v2 Announce Type: replace 
Abstract: Differentially Private Stochastic Gradient Descent (DP-SGD) is a widely adopted technique for privacy-preserving deep learning. A critical challenge in DP-SGD is selecting the optimal clipping threshold C, which involves balancing the trade-off between clipping bias and noise magnitude, incurring substantial privacy and computing overhead during hyperparameter tuning.
  In this paper, we propose Dynamic Clipping DP-SGD (DC-SGD), a framework that leverages differentially private histograms to estimate gradient norm distributions and dynamically adjust the clipping threshold C. Our framework includes two novel mechanisms: DC-SGD-P and DC-SGD-E. DC-SGD-P adjusts the clipping threshold based on a percentile of gradient norms, while DC-SGD-E minimizes the expected squared error of gradients to optimize C. These dynamic adjustments significantly reduce the burden of hyperparameter tuning C. The extensive experiments on various deep learning tasks, including image classification and natural language processing, show that our proposed dynamic algorithms achieve up to 9 times acceleration on hyperparameter tuning than DP-SGD. And DC-SGD-E can achieve an accuracy improvement of 10.62% on CIFAR10 than DP-SGD under the same privacy budget of hyperparameter tuning. We conduct rigorous theoretical privacy and convergence analyses, showing that our methods seamlessly integrate with the Adam optimizer. Our results highlight the robust performance and efficiency of DC-SGD, offering a practical solution for differentially private deep learning with reduced computational overhead and enhanced privacy guarantees.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00053">Integrating Large Language Models with Human Expertise for Disease Detection in Electronic Health Records</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jie Pan, Seungwon Lee, Cheligeer Cheligeer, Elliot A. Martin, Kiarash Riazi, Hude Quan, Na Li</div>
        <div class="preprint-abstract">arXiv:2504.00053v1 Announce Type: cross 
Abstract: Objective: Electronic health records (EHR) are widely available to complement administrative data-based disease surveillance and healthcare performance evaluation. Defining conditions from EHR is labour-intensive and requires extensive manual labelling of disease outcomes. This study developed an efficient strategy based on advanced large language models to identify multiple conditions from EHR clinical notes. Methods: We linked a cardiac registry cohort in 2015 with an EHR system in Alberta, Canada. We developed a pipeline that leveraged a generative large language model (LLM) to analyze, understand, and interpret EHR notes by prompts based on specific diagnosis, treatment management, and clinical guidelines. The pipeline was applied to detect acute myocardial infarction (AMI), diabetes, and hypertension. The performance was compared against clinician-validated diagnoses as the reference standard and widely adopted International Classification of Diseases (ICD) codes-based methods. Results: The study cohort accounted for 3,088 patients and 551,095 clinical notes. The prevalence was 55.4%, 27.7%, 65.9% and for AMI, diabetes, and hypertension, respectively. The performance of the LLM-based pipeline for detecting conditions varied: AMI had 88% sensitivity, 63% specificity, and 77% positive predictive value (PPV); diabetes had 91% sensitivity, 86% specificity, and 71% PPV; and hypertension had 94% sensitivity, 32% specificity, and 72% PPV. Compared with ICD codes, the LLM-based method demonstrated improved sensitivity and negative predictive value across all conditions. The monthly percentage trends from the detected cases by LLM and reference standard showed consistent patterns.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2410.13727">LLM-Human Pipeline for Cultural Context Grounding of Conversations</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Rajkumar Pujari, Dan Goldwasser</div>
        <div class="preprint-abstract">arXiv:2410.13727v2 Announce Type: replace-cross 
Abstract: Conversations often adhere to well-understood social norms that vary across cultures. For example, while "addressing parents by name" is commonplace in the West, it is rare in most Asian cultures. Adherence or violation of such norms often dictates the tenor of conversations. Humans are able to navigate social situations requiring cultural awareness quite adeptly. However, it is a hard task for NLP models.
  In this paper, we tackle this problem by introducing a "Cultural Context Schema" for conversations. It comprises (1) conversational information such as emotions, dialogue acts, etc., and (2) cultural information such as social norms, violations, etc. We generate ~110k social norm and violation descriptions for ~23k conversations from Chinese culture using LLMs. We refine them using automated verification strategies which are evaluated against culturally aware human judgements. We organize these descriptions into meaningful structures we call "Norm Concepts", using an interactive human-in-loop framework. We ground the norm concepts and the descriptions in conversations using symbolic annotation. Finally, we use the obtained dataset for downstream tasks such as emotion, sentiment, and dialogue act detection. We show that it significantly improves the empirical performance.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00249">Plane-Wave Decomposition and Randomised Training; a Novel Path to Generalised PINNs for SHM</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Rory Clements, James Ellis, Geoff Hassall, Simon Horsley</div>
        <div class="preprint-abstract">arXiv:2504.00249v1 Announce Type: cross 
Abstract: In this paper, we introduce a formulation of Physics-Informed Neural Networks (PINNs), based on learning the form of the Fourier decomposition, and a training methodology based on a spread of randomly chosen boundary conditions. By training in this way we produce a PINN that generalises; after training it can be used to correctly predict the solution for an arbitrary set of boundary conditions and interpolate this solution between the samples that spanned the training domain. We demonstrate for a toy system of two coupled oscillators that this gives the PINN formulation genuine predictive capability owing to an effective reduction of the training to evaluation times ratio due to this decoupling of the solution from specific boundary conditions.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00460">MetaLoRA: Tensor-Enhanced Adaptive Low-Rank Fine-tuning</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Maolin Wang, Xiangyu Zhao</div>
        <div class="preprint-abstract">arXiv:2504.00460v1 Announce Type: new 
Abstract: There has been a significant increase in the deployment of neural network models, presenting substantial challenges in model adaptation and fine-tuning. Efficient adaptation is crucial in maintaining model performance across diverse tasks and domains. While Low-Rank Adaptation (LoRA) has emerged as a promising parameter-efficient fine-tuning method, its fixed parameter nature limits its ability to handle dynamic task requirements effectively. Adapting models to new tasks can be challenging due to the need for extensive fine-tuning. Current LoRA variants primarily focus on general parameter reduction while overlooking the importance of dynamic parameter adjustment and meta-learning capabilities. Moreover, existing approaches mainly address static adaptations, neglecting the potential benefits of task-aware parameter generation in handling diverse task distributions. To address these limitations, this Ph.D. research proposes a LoRA generation approach to model task relationships and introduces MetaLoRA, a novel parameter-efficient adaptation framework incorporating meta-learning principles. This work develops a comprehensive architecture that integrates meta-parameter generation with adaptive low-rank decomposition, enabling efficient handling of both task-specific and task-agnostic features. MetaLoRA accurately captures task patterns by incorporating meta-learning mechanisms and dynamic parameter adjustment strategies. To our knowledge, this research represents the first attempt to provide a meta-learning enhanced LoRA variant, offering improved adaptation capability while maintaining computational efficiency in model fine-tuning.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2405.17956">Unified Preference Optimization: Language Model Alignment Beyond the Preference Frontier</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Anirudhan Badrinath, Prabhat Agarwal, Jiajing Xu</div>
        <div class="preprint-abstract">arXiv:2405.17956v3 Announce Type: replace 
Abstract: For aligning large language models (LLMs), prior work has leveraged reinforcement learning via human feedback (RLHF) or variations of direct preference optimization (DPO). While DPO offers a simpler framework based on maximum likelihood estimation, it compromises on the ability to easily tune language models to maximize auxiliary, non-preferential objectives according to the LLM designer's preferences (e.g., tuning lexical style or minimizing specific kinds of harmful content). Critically, these designer objectives may not be amply human-labeled or represented in available data, align with user preferences, or even be able to be captured tractably by binary preference pairs. To leverage the simplicity and performance of DPO with the generality of RL, we propose a unified approach. Based on a simple decomposition of preference and auxiliary objectives, we allow for tuning LLMs to optimize user and designer preferences without any additional specialized or preference data, computational cost, stability ``tweaks'', or training instability. The proposed method, Unified Preference Optimization, shows the ability to effectively generalize to user preferences and auxiliary objectives, while preserving or surpassing alignment performance on challenging benchmarks across a range of model sizes.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00370">Spatiotemporal Attention Learning Framework for Event-Driven Object Recognition</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Tiantian Xie, Pengpai Wang, Rosa H. M. Chan</div>
        <div class="preprint-abstract">arXiv:2504.00370v1 Announce Type: cross 
Abstract: Event-based vision sensors, inspired by biological neural systems, asynchronously capture local pixel-level intensity changes as a sparse event stream containing position, polarity, and timestamp information. These neuromorphic sensors offer significant advantages in dynamic range, latency, and power efficiency. Their working principle inherently addresses traditional camera limitations such as motion blur and redundant background information, making them particularly suitable for dynamic vision tasks. While recent works have proposed increasingly complex event-based architectures, the computational overhead and parameter complexity of these approaches limit their practical deployment. This paper presents a novel spatiotemporal learning framework for event-based object recognition, utilizing a VGG network enhanced with Convolutional Block Attention Module (CBAM). Our approach achieves comparable performance to state-of-the-art ResNet-based methods while reducing parameter count by 2.3% compared to the original VGG model. Specifically, it outperforms ResNet-based methods like MVF-Net, achieving the highest Top-1 accuracy of 76.4% (pretrained) and 71.3% (not pretrained) on CIFAR10-DVS, and 72.4% (not pretrained) on N-Caltech101. These results highlight the robustness of our method when pretrained weights are not used, making it suitable for scenarios where transfer learning is unavailable. Moreover, our approach reduces reliance on data augmentation. Experimental results on standard event-based datasets demonstrate the framework's efficiency and effectiveness for real-world applications.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00118">Times2D: Multi-Period Decomposition and Derivative Mapping for General Time Series Forecasting</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Reza Nematirad, Anil Pahwa, Balasubramaniam Natarajan</div>
        <div class="preprint-abstract">arXiv:2504.00118v1 Announce Type: new 
Abstract: Time series forecasting is an important application in various domains such as energy management, traffic planning, financial markets, meteorology, and medicine. However, real-time series data often present intricate temporal variability and sharp fluctuations, which pose significant challenges for time series forecasting. Previous models that rely on 1D time series representations usually struggle with complex temporal variations. To address the limitations of 1D time series, this study introduces the Times2D method that transforms the 1D time series into 2D space. Times2D consists of three main parts: first, a Periodic Decomposition Block (PDB) that captures temporal variations within a period and between the same periods by converting the time series into a 2D tensor in the frequency domain. Second, the First and Second Derivative Heatmaps (FSDH) capture sharp changes and turning points, respectively. Finally, an Aggregation Forecasting Block (AFB) integrates the output tensors from PDB and FSDH for accurate forecasting. This 2D transformation enables the utilization of 2D convolutional operations to effectively capture long and short characteristics of the time series. Comprehensive experimental results across large-scale data in the literature demonstrate that the proposed Times2D model achieves state-of-the-art performance in both short-term and long-term forecasting. The code is available in this repository: https://github.com/Tims2D/Times2D.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00874">P2NIA: Privacy-Preserving Non-Iterative Auditing</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jade Garcia Bourr\'ee, Hadrien Lautraite, S\'ebastien Gambs, Gilles Tredan, Erwan Le Merrer, Beno\^it Rottembourg</div>
        <div class="preprint-abstract">arXiv:2504.00874v1 Announce Type: new 
Abstract: The emergence of AI legislation has increased the need to assess the ethical compliance of high-risk AI systems. Traditional auditing methods rely on platforms' application programming interfaces (APIs), where responses to queries are examined through the lens of fairness requirements. However, such approaches put a significant burden on platforms, as they are forced to maintain APIs while ensuring privacy, facing the possibility of data leaks. This lack of proper collaboration between the two parties, in turn, causes a significant challenge to the auditor, who is subject to estimation bias as they are unaware of the data distribution of the platform. To address these two issues, we present P2NIA, a novel auditing scheme that proposes a mutually beneficial collaboration for both the auditor and the platform. Extensive experiments demonstrate P2NIA's effectiveness in addressing both issues. In summary, our work introduces a privacy-preserving and non-iterative audit scheme that enhances fairness assessments using synthetic or local data, avoiding the challenges associated with traditional API-based audits.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.16872">Lie Detector: Unified Backdoor Detection via Cross-Examination Framework</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Xuan Wang, Siyuan Liang, Dongping Liao, Han Fang, Aishan Liu, Xiaochun Cao, Yu-liang Lu, Ee-Chien Chang, Xitong Gao</div>
        <div class="preprint-abstract">arXiv:2503.16872v2 Announce Type: replace 
Abstract: Institutions with limited data and computing resources often outsource model training to third-party providers in a semi-honest setting, assuming adherence to prescribed training protocols with pre-defined learning paradigm (e.g., supervised or semi-supervised learning). However, this practice can introduce severe security risks, as adversaries may poison the training data to embed backdoors into the resulting model. Existing detection approaches predominantly rely on statistical analyses, which often fail to maintain universally accurate detection accuracy across different learning paradigms. To address this challenge, we propose a unified backdoor detection framework in the semi-honest setting that exploits cross-examination of model inconsistencies between two independent service providers. Specifically, we integrate central kernel alignment to enable robust feature similarity measurements across different model architectures and learning paradigms, thereby facilitating precise recovery and identification of backdoor triggers. We further introduce backdoor fine-tuning sensitivity analysis to distinguish backdoor triggers from adversarial perturbations, substantially reducing false positives. Extensive experiments demonstrate that our method achieves superior detection performance, improving accuracy by 5.4%, 1.6%, and 11.9% over SoTA baselines across supervised, semi-supervised, and autoregressive learning tasks, respectively. Notably, it is the first to effectively detect backdoors in multimodal large language models, further highlighting its broad applicability and advancing secure deep learning.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00280">Exploration and Adaptation in Non-Stationary Tasks with Diffusion Policies</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Gunbir Singh Baveja</div>
        <div class="preprint-abstract">arXiv:2504.00280v1 Announce Type: new 
Abstract: This paper investigates the application of Diffusion Policy in non-stationary, vision-based RL settings, specifically targeting environments where task dynamics and objectives evolve over time. Our work is grounded in practical challenges encountered in dynamic real-world scenarios such as robotics assembly lines and autonomous navigation, where agents must adapt control strategies from high-dimensional visual inputs. We apply Diffusion Policy -- which leverages iterative stochastic denoising to refine latent action representations-to benchmark environments including Procgen and PointMaze. Our experiments demonstrate that, despite increased computational demands, Diffusion Policy consistently outperforms standard RL methods such as PPO and DQN, achieving higher mean and maximum rewards with reduced variability. These findings underscore the approach's capability to generate coherent, contextually relevant action sequences in continuously shifting conditions, while also highlighting areas for further improvement in handling extreme non-stationarity.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2412.01487">FastRM: An efficient and automatic explainability framework for multimodal generative models</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Gabriela Ben-Melech Stan, Estelle Aflalo, Man Luo, Shachar Rosenman, Tiep Le, Sayak Paul, Shao-Yen Tseng, Vasudev Lal</div>
        <div class="preprint-abstract">arXiv:2412.01487v3 Announce Type: replace 
Abstract: Large Vision Language Models (LVLMs) have demonstrated remarkable reasoning capabilities over textual and visual inputs. However, these models remain prone to generating misinformation. Identifying and mitigating ungrounded responses is crucial for developing trustworthy AI. Traditional explainability methods such as gradient-based relevancy maps, offer insight into the decision process of models, but are often computationally expensive and unsuitable for real-time output validation. In this work, we introduce FastRM, an efficient method for predicting explainable Relevancy Maps of LVLMs. Furthermore, FastRM provides both quantitative and qualitative assessment of model confidence. Experimental results demonstrate that FastRM achieves a 99.8% reduction in computation time and a 44.4% reduction in memory footprint compared to traditional relevancy map generation. FastRM allows explainable AI to be more practical and scalable, thereby promoting its deployment in real-world applications and enabling users to more effectively evaluate the reliability of model outputs.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2412.10545">Identifying Predictions That Influence the Future: Detecting Performative Concept Drift in Data Streams</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Brandon Gower-Winter, Georg Krempl, Sergey Dragomiretskiy, Tineke Jelsma, Arno Siebes</div>
        <div class="preprint-abstract">arXiv:2412.10545v2 Announce Type: replace 
Abstract: Concept Drift has been extensively studied within the context of Stream Learning. However, it is often assumed that the deployed model's predictions play no role in the concept drift the system experiences. Closer inspection reveals that this is not always the case. Automated trading might be prone to self-fulfilling feedback loops. Likewise, malicious entities might adapt to evade detectors in the adversarial setting resulting in a self-negating feedback loop that requires the deployed models to constantly retrain. Such settings where a model may induce concept drift are called performative. In this work, we investigate this phenomenon. Our contributions are as follows: First, we define performative drift within a stream learning setting and distinguish it from other causes of drift. We introduce a novel type of drift detection task, aimed at identifying potential performative concept drift in data streams. We propose a first such performative drift detection approach, called CheckerBoard Performative Drift Detection (CB-PDD). We apply CB-PDD to both synthetic and semi-synthetic datasets that exhibit varying degrees of self-fulfilling feedback loops. Results are positive with CB-PDD showing high efficacy, low false detection rates, resilience to intrinsic drift, comparability to other drift detection techniques, and an ability to effectively detect performative drift in semi-synthetic datasets. Secondly, we highlight the role intrinsic (traditional) drift plays in obfuscating performative drift and discuss the implications of these findings as well as the limitations of CB-PDD.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00051">The Cursive Transformer</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Sam Greydanus, Zachary Wimpee</div>
        <div class="preprint-abstract">arXiv:2504.00051v1 Announce Type: new 
Abstract: Transformers trained on tokenized text, audio, and images can generate high-quality autoregressive samples. But handwriting data, represented as sequences of pen coordinates, remains underexplored. We introduce a novel tokenization scheme that converts pen stroke offsets to polar coordinates, discretizes them into bins, and then turns them into sequences of tokens with which to train a standard GPT model. This allows us to capture complex stroke distributions without using any specialized architectures (eg. the mixture density network or the self-advancing ASCII attention head from Graves 2014). With just 3,500 handwritten words and a few simple data augmentations, we are able to train a model that can generate realistic cursive handwriting. Our approach is simpler and more performant than previous RNN-based methods.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2409.12788">Optimal or Greedy Decision Trees? Revisiting their Objectives, Tuning, and Performance</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jacobus G. M. van der Linden, Dani\"el Vos, Mathijs M. de Weerdt, Sicco Verwer, Emir Demirovi\'c</div>
        <div class="preprint-abstract">arXiv:2409.12788v2 Announce Type: replace 
Abstract: Recently there has been a surge of interest in optimal decision tree (ODT) methods that globally optimize accuracy directly, in contrast to traditional approaches that locally optimize an impurity or information metric. However, the value of optimal methods is not well understood yet, as the literature provides conflicting results, with some demonstrating superior out-of-sample performance of ODTs over greedy approaches, while others show the opposite. Through a novel extensive experimental study, we provide new insights into the design and behavior of learning decision trees. In particular, we identify and analyze two relatively unexplored aspects of ODTs: the objective function used in training trees, and tuning techniques. Thus, we address these three questions: what objective to optimize in ODTs; how to tune ODTs; and how do optimal and greedy methods compare? Our experimental evaluation examines 11 objective functions, six tuning methods, and six claims from the literature on optimal and greedy methods on 180 real and synthetic data sets. Through our analysis, both conceptually and experimentally, we show the effect of (non-)concave objectives in greedy and optimal approaches; we highlight the importance of proper tuning of ODTs; support and refute several claims from the literature; provide clear recommendations for researchers and practitioners on the usage of greedy and optimal methods; and code for future comparisons.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00698">Command A: An Enterprise-Ready Large Language Model</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Team Cohere,  Aakanksha, Arash Ahmadian, Marwan Ahmed, Jay Alammar, Yazeed Alnumay, Sophia Althammer, Arkady Arkhangorodsky, Viraat Aryabumi, Dennis Aumiller, Rapha\"el Avalos, Zahara Aviv, Sammie Bae, Saurabh Baji, Alexandre Barbet, Max Bartolo, Bj\"orn Bebensee, Neeral Beladia, Walter Beller-Morales, Alexandre B\'erard, Andrew Berneshawi, Anna Bialas, Phil Blunsom, Matt Bobkin, Adi Bongale, Sam Braun, Maxime Brunet, Samuel Cahyawijaya, David Cairuz, Jon Ander Campos, Cassie Cao, Kris Cao, Roman Castagn\'e, Juli\'an Cendrero, Leila Chan Currie, Yash Chandak, Diane Chang, Giannis Chatziveroglou, Hongyu Chen, Claire Cheng, Alexis Chevalier, Justin T. Chiu, Eugene Cho, Eugene Choi, Eujeong Choi, Tim Chung, Volkan Cirik, Ana Cismaru, Pierre Clavier, Henry Conklin, Lucas Crawhall-Stein, Devon Crouse, Andres Felipe Cruz-Salinas, Ben Cyrus, Daniel D'souza, Hugo Dalla-Torre, John Dang, William Darling, Omar Darwiche Domingues, Saurabh Dash, Antoine Debugne, Th\'eo Dehaze, Shaan Desai, Joan Devassy, Rishit Dholakia, Kyle Duffy, Ali Edalati, Ace Eldeib, Abdullah Elkady, Sarah Elsharkawy, Irem Erg\"un, Beyza Ermis, Marzieh Fadaee, Boyu Fan, Lucas Fayoux, Yannis Flet-Berliac, Nick Frosst, Matthias Gall\'e, Wojciech Galuba, Utsav Garg, Matthieu Geist, Mohammad Gheshlaghi Azar, Seraphina Goldfarb-Tarrant, Tomas Goldsack, Aidan Gomez, Victor Machado Gonzaga, Nithya Govindarajan, Manoj Govindassamy, Nathan Grinsztajn, Nikolas Gritsch, Patrick Gu, Shangmin Guo, Kilian Haefeli, Rod Hajjar, Tim Hawes, Jingyi He, Sebastian Hofst\"atter, Sungjin Hong, Sara Hooker, Tom Hosking, Stephanie Howe, Eric Hu, Renjie Huang, Hemant Jain, Ritika Jain, Nick Jakobi, Madeline Jenkins, JJ Jordan, Dhruti Joshi, Jason Jung, Trushant Kalyanpur, Siddhartha Rao Kamalakara, Julia Kedrzycki, Gokce Keskin, Edward Kim, Joon Kim, Wei-Yin Ko, Tom Kocmi, Michael Kozakov, Wojciech Kry\'sci\'nski, Arnav Kumar Jain, Komal Kumar Teru, Sander Land, Michael Lasby, Olivia Lasche, Justin Lee, Patrick Lewis, Jeffrey Li, Jonathan Li, Hangyu Lin, Acyr Locatelli, Kevin Luong, Raymond Ma, Lukas Mach, Marina Machado, Joanne Magbitang, Brenda Malacara Lopez, Aryan Mann, Kelly Marchisio, Olivia Markham, Alexandre Matton, Alex McKinney, Dominic McLoughlin, Jozef Mokry, Adrien Morisot, Autumn Moulder, Harry Moynehan, Maximilian Mozes, Vivek Muppalla, Lidiya Murakhovska, Hemangani Nagarajan, Alekhya Nandula, Hisham Nasir, Shauna Nehra, Josh Netto-Rosen, Daniel Ohashi, James Owers-Bardsley, Jason Ozuzu, Dennis Padilla, Gloria Park, Sam Passaglia, Jeremy Pekmez, Laura Penstone, Aleksandra Piktus, Case Ploeg, Andrew Poulton, Youran Qi, Shubha Raghvendra, Miguel Ramos, Ekagra Ranjan, Pierre Richemond, C\'ecile Robert-Michon, Aur\'elien Rodriguez, Sudip Roy, Laura Ruis, Louise Rust, Anubhav Sachan, Alejandro Salamanca, Kailash Karthik Saravanakumar, Isha Satyakam, Alice Schoenauer Sebag, Priyanka Sen, Sholeh Sepehri, Preethi Seshadri, Ye Shen, Tom Sherborne, Sylvie Chang Shi, Sanal Shivaprasad, Vladyslav Shmyhlo, Anirudh Shrinivason, Inna Shteinbuk, Amir Shukayev, Mathieu Simard, Ella Snyder, Ava Spataru, Victoria Spooner, Trisha Starostina, Florian Strub, Yixuan Su, Jimin Sun, Dwarak Talupuru, Eugene Tarassov, Elena Tommasone, Jennifer Tracey, Billy Trend, Evren Tumer, Ahmet \"Ust\"un, Bharat Venkitesh, David Venuto, Pat Verga, Maxime Voisin, Alex Wang, Donglu Wang, Shijian Wang, Edmond Wen, Naomi White, Jesse Willman, Marysia Winkels, Chen Xia, Jessica Xie, Minjie Xu, Bowen Yang, Tan Yi-Chern, Ivan Zhang, Zhenyu Zhao, Zhoujie Zhao</div>
        <div class="preprint-abstract">arXiv:2504.00698v1 Announce Type: cross 
Abstract: In this report we describe the development of Command A, a powerful large language model purpose-built to excel at real-world enterprise use cases. Command A is an agent-optimised and multilingual-capable model, with support for 23 languages of global business, and a novel hybrid architecture balancing efficiency with top of the range performance. It offers best-in-class Retrieval Augmented Generation (RAG) capabilities with grounding and tool use to automate sophisticated business processes. These abilities are achieved through a decentralised training approach, including self-refinement algorithms and model merging techniques. We also include results for Command R7B which shares capability and architectural similarities to Command A. Weights for both models have been released for research purposes. This technical report details our original training pipeline and presents an extensive evaluation of our models across a suite of enterprise-relevant tasks and public benchmarks, demonstrating excellent performance and efficiency.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2404.07181">A predictive machine learning force field framework for liquid electrolyte development</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Sheng Gong, Yumin Zhang, Zhenliang Mu, Zhichen Pu, Hongyi Wang, Zhiao Yu, Mengyi Chen, Tianze Zheng, Zhi Wang, Lifei Chen, Zhenze Yang, Xiaojie Wu, Shaochen Shi, Weihao Gao, Wen Yan, Liang Xiang</div>
        <div class="preprint-abstract">arXiv:2404.07181v5 Announce Type: replace-cross 
Abstract: Despite the widespread applications of machine learning force fields (MLFF) in solids and small molecules, there is a notable gap in applying MLFF to simulate liquid electrolyte, a critical component of the current commercial lithium-ion battery. In this work, we introduce BAMBOO (\textbf{B}yteDance \textbf{A}I \textbf{M}olecular Simulation \textbf{Boo}ster), a predictive framework for molecular dynamics (MD) simulations, with a demonstration of its capability in the context of liquid electrolyte for lithium batteries. We design a physics-inspired graph equivariant transformer architecture as the backbone of BAMBOO to learn from quantum mechanical simulations. Additionally, we introduce an ensemble knowledge distillation approach and apply it to MLFFs to reduce the fluctuation of observations from MD simulations. Finally, we propose a density alignment algorithm to align BAMBOO with experimental measurements. BAMBOO demonstrates state-of-the-art accuracy in predicting key electrolyte properties such as density, viscosity, and ionic conductivity across various solvents and salt combinations. The current model, trained on more than 15 chemical species, achieves the average density error of 0.01 g/cm$^3$ on various compositions compared with experiment.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00156">Nuclear Microreactor Control with Deep Reinforcement Learning</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Leo Tunkle, Kamal Abdulraheem, Linyu Lin, Majdi I. Radaideh</div>
        <div class="preprint-abstract">arXiv:2504.00156v1 Announce Type: cross 
Abstract: The economic feasibility of nuclear microreactors will depend on minimizing operating costs through advancements in autonomous control, especially when these microreactors are operating alongside other types of energy systems (e.g., renewable energy). This study explores the application of deep reinforcement learning (RL) for real-time drum control in microreactors, exploring performance in regard to load-following scenarios. By leveraging a point kinetics model with thermal and xenon feedback, we first establish a baseline using a single-output RL agent, then compare it against a traditional proportional-integral-derivative (PID) controller. This study demonstrates that RL controllers, including both single- and multi-agent RL (MARL) frameworks, can achieve similar or even superior load-following performance as traditional PID control across a range of load-following scenarios. In short transients, the RL agent was able to reduce the tracking error rate in comparison to PID. Over extended 300-minute load-following scenarios in which xenon feedback becomes a dominant factor, PID maintained better accuracy, but RL still remained within a 1% error margin despite being trained only on short-duration scenarios. This highlights RL's strong ability to generalize and extrapolate to longer, more complex transients, affording substantial reductions in training costs and reduced overfitting. Furthermore, when control was extended to multiple drums, MARL enabled independent drum control as well as maintained reactor symmetry constraints without sacrificing performance -- an objective that standard single-agent RL could not learn. We also found that, as increasing levels of Gaussian noise were added to the power measurements, the RL controllers were able to maintain lower error rates than PID, and to do so with less control effort.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00287">A Deep Learning Approach to Anomaly Detection in High-Frequency Trading Data</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Qiuliuyang Bao, Jiawei Wang, Hao Gong, Yiwei Zhang, Xiaojun Guo, Hanrui Feng</div>
        <div class="preprint-abstract">arXiv:2504.00287v1 Announce Type: new 
Abstract: This paper proposes an algorithm based on a staged sliding window Transformer architecture to detect abnormal behaviors in the microstructure of the foreign exchange market, focusing on high-frequency EUR/USD trading data. The method captures multi-scale temporal features through a staged sliding window, extracts global and local dependencies by combining the self-attention mechanism and weighted attention mechanism of the Transformer, and uses a classifier to identify abnormal events. Experimental results on a real high-frequency dataset containing order book depth, spread, and trading volume show that the proposed method significantly outperforms traditional machine learning (such as decision trees and random forests) and deep learning methods (such as MLP, CNN, RNN, LSTM) in terms of accuracy (0.93), F1-Score (0.91), and AUC-ROC (0.95). Ablation experiments verify the contribution of each component, and the visualization of order book depth and anomaly detection further reveals the effectiveness of the model under complex market dynamics. Despite the false positive problem, the model still provides important support for market supervision. In the future, noise processing can be optimized and extended to other markets to improve generalization and real-time performance.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00592">NeuraLUT-Assemble: Hardware-aware Assembling of Sub-Neural Networks for Efficient LUT Inference</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Marta Andronic, George A. Constantinides</div>
        <div class="preprint-abstract">arXiv:2504.00592v1 Announce Type: new 
Abstract: Efficient neural networks (NNs) leveraging lookup tables (LUTs) have demonstrated significant potential for emerging AI applications, particularly when deployed on field-programmable gate arrays (FPGAs) for edge computing. These architectures promise ultra-low latency and reduced resource utilization, broadening neural network adoption in fields such as particle physics. However, existing LUT-based designs suffer from accuracy degradation due to the large fan-in required by neurons being limited by the exponential scaling of LUT resources with input width. In practice, in prior work this tension has resulted in the reliance on extremely sparse models.
  We present NeuraLUT-Assemble, a novel framework that addresses these limitations by combining mixed-precision techniques with the assembly of larger neurons from smaller units, thereby increasing connectivity while keeping the number of inputs of any given LUT manageable. Additionally, we introduce skip-connections across entire LUT structures to improve gradient flow. NeuraLUT-Assemble closes the accuracy gap between LUT-based methods and (fully-connected) MLP-based models, achieving competitive accuracy on tasks such as network intrusion detection, digit classification, and jet classification, demonstrating up to $8.42\times$ reduction in the area-delay product compared to the state-of-the-art at the time of the publication.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00494">Flow Matching on Lie Groups</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Finn M. Sherry, Bart M. N. Smets</div>
        <div class="preprint-abstract">arXiv:2504.00494v1 Announce Type: cross 
Abstract: Flow Matching (FM) is a recent generative modelling technique: we aim to learn how to sample from distribution $\mathfrak{X}_1$ by flowing samples from some distribution $\mathfrak{X}_0$ that is easy to sample from. The key trick is that this flow field can be trained while conditioning on the end point in $\mathfrak{X}_1$: given an end point, simply move along a straight line segment to the end point (Lipman et al. 2022). However, straight line segments are only well-defined on Euclidean space. Consequently, Chen and Lipman (2023) generalised the method to FM on Riemannian manifolds, replacing line segments with geodesics or their spectral approximations. We take an alternative point of view: we generalise to FM on Lie groups by instead substituting exponential curves for line segments. This leads to a simple, intrinsic, and fast implementation for many matrix Lie groups, since the required Lie group operations (products, inverses, exponentials, logarithms) are simply given by the corresponding matrix operations. FM on Lie groups could then be used for generative modelling with data consisting of sets of features (in $\mathbb{R}^n$) and poses (in some Lie group), e.g. the latent codes of Equivariant Neural Fields (Wessels et al. 2025).</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2407.01100">Eliminating Position Bias of Language Models: A Mechanistic Approach</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Ziqi Wang, Hanlin Zhang, Xiner Li, Kuan-Hao Huang, Chi Han, Shuiwang Ji, Sham M. Kakade, Hao Peng, Heng Ji</div>
        <div class="preprint-abstract">arXiv:2407.01100v3 Announce Type: replace-cross 
Abstract: Position bias has proven to be a prevalent issue of modern language models (LMs), where the models prioritize content based on its position within the given context. This bias often leads to unexpected model failures and hurts performance, robustness, and reliability across various applications. Our mechanistic analysis attributes the position bias to two components employed in nearly all state-of-the-art LMs: causal attention and relative positional encodings. Based on the analyses, we propose to eliminate position bias (e.g., different retrieved documents' orders in QA affect performance) with a training-free zero-shot approach. Our method changes the causal attention to bidirectional attention between documents and utilizes model attention values to decide the relative orders of documents instead of using the order provided in input prompts, therefore enabling Position-INvariant inferencE (PINE) at the document level. By eliminating position bias, models achieve better performance and reliability in downstream tasks, including LM-as-a-judge, retrieval-augmented QA, molecule generation, and math reasoning. Notably, PINE is especially useful when adapting LMs for evaluating reasoning pairs: it consistently provides 8 to 10 percentage points performance gains, making Llama-3-70B-Instruct perform even better than GPT-4-0125-preview and GPT-4o-2024-08-06 on the RewardBench reasoning set.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2406.18012">View-Invariant Pixelwise Anomaly Detection in Multi-object Scenes with Adaptive View Synthesis</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Subin Varghese, Vedhus Hoskere</div>
        <div class="preprint-abstract">arXiv:2406.18012v2 Announce Type: replace-cross 
Abstract: Visual anomaly detection in the built environment is a valuable tool for applications such as infrastructure assessment, construction monitoring, security surveillance, and urban planning. Anomaly detection approaches are typically unsupervised and work by detecting deviations from an expected state where no assumptions are made exact type of deviation. Unsupervised pixel-level anomaly detection methods have been developed to successfully recognize and segment anomalies; however, existing techniques are designed for industrial settings with a fixed camera position. In the built environment, images are periodically captured by a camera operated manually or mounted on aerial or ground vehicles. The camera pose between successive collections may vary widely voiding a fundamental assumption in existing anomaly detection approaches. To address this gap, we introduce the problem of Scene Anomaly Detection (Scene AD), where the goal is to detect anomalies from two sets of images: one set without anomalies and one set that may or may not contain anomalies. No labeled semantic segmentation data are provided for training. We propose a novel network, OmniAD, to tackle Scene AD by refining the reverse distillation anomaly detection method, leading to a 40\% improvement in pixel-level anomaly detection. Additionally, we introduce two new data augmentation strategies that leverage novel view synthesis and camera localization to enhance generalization. We evaluate our approach both qualitatively and quantitatively on a new dataset, ToyCity the first Scene AD dataset featuring multiple objects as well as on the established single object centric dataset, MAD. Our method demonstrates marked improvement over baseline approaches, paving the way for robust anomaly detection in scenes with real-world camera pose variations commonly observed in the built environment. https://drags99.github.io/OmniAD/</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.13837">Self-Vocabularizing Training for Neural Machine Translation</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Pin-Jie Lin, Ernie Chang, Yangyang Shi, Vikas Chandra</div>
        <div class="preprint-abstract">arXiv:2503.13837v4 Announce Type: replace-cross 
Abstract: Past vocabulary learning techniques identify relevant vocabulary before training, relying on statistical and entropy-based assumptions that largely neglect the role of model training. Empirically, we observe that trained translation models are induced to use a byte-pair encoding (BPE) vocabulary subset distinct from the original BPE vocabulary, leading to performance improvements when retrained with the induced vocabulary. In this paper, we analyze this discrepancy in neural machine translation by examining vocabulary and entropy shifts during self-training--where each iteration generates a labeled dataset by pairing source sentences with the model's predictions to define a new vocabulary. Building on these insights, we propose self-vocabularizing training, an iterative method that self-selects a smaller, more optimal vocabulary, yielding up to a 1.49 BLEU improvement. Moreover, we find that deeper model architectures lead to both an increase in unique token usage and a 6-8% reduction in vocabulary size.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2408.06621">Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Sungmin Cha, Sungjun Cho, Dasol Hwang, Moontae Lee</div>
        <div class="preprint-abstract">arXiv:2408.06621v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated strong reasoning and memorization capabilities via pretraining on massive textual corpora. However, this poses risk of privacy and copyright violations, highlighting the need for efficient machine unlearning methods that remove sensitive data without retraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn by reducing the likelihood of generating unwanted content, it leads to unstable optimization and catastrophic forgetting of retrained knowledge. We find that combining GA with low-rank adaptation results in poor trade-offs between computational cost and generative performance. To address these challenges, we propose Low-rank Knowledge Unlearning (LoKU), a novel framework that enables robust and efficient unlearning for LLMs. First, we introduce Inverted Hinge Loss, which suppresses unwanted tokens while maintaining fluency by boosting the probability of the next most likely token. Second, we develop a data-adaptive initialization for LoRA adapters via low-rank approximation weighted with relative Fisher information, thereby focusing updates on parameters critical for removing targeted knowledge. Experiments on the Training Data Extraction Challenge dataset using GPT-Neo models as well as on the TOFU benchmark with Phi-1.5B and Llama2-7B models demonstrate that our approach effectively removes sensitive information while maintaining reasoning and generative capabilities with minimal impact. Our implementation can be found in https://github.com/csm9493/efficient-llm-unlearning.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2408.15953">Modeling and Analyzing the Influence of Non-Item Pages on Sequential Next-Item Prediction</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Elisabeth Fischer, Albin Zehe, Andreas Hotho, Daniel Schl\"or</div>
        <div class="preprint-abstract">arXiv:2408.15953v4 Announce Type: replace-cross 
Abstract: Analyzing sequences of interactions between users and items, sequential recommendation models can learn user intent and make predictions about the next item. Next to item interactions, most systems also have interactions with what we call non-item pages: these pages are not related to specific items but still can provide insights into the user's interests, as, for example, navigation pages. We therefore propose a general way to include these non-item pages in sequential recommendation models to enhance next-item prediction.
  First, we demonstrate the influence of non-item pages on following interactions using the hypotheses testing framework HypTrails and propose methods for representing non-item pages in sequential recommendation models. Subsequently, we adapt popular sequential recommender models to integrate non-item pages and investigate their performance with different item representation strategies as well as their ability to handle noisy data. To show the general capabilities of the models to integrate non-item pages, we create a synthetic dataset for a controlled setting and then evaluate the improvements from including non-item pages on two real-world datasets.
  Our results show that non-item pages are a valuable source of information, and incorporating them in sequential recommendation models increases the performance of next-item prediction across all analyzed model architectures.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00434">HERA: Hybrid Edge-cloud Resource Allocation for Cost-Efficient AI Agents</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Shiyi Liu, Haiying Shen, Shuai Che, Mahdi Ghandi, Mingqin Li</div>
        <div class="preprint-abstract">arXiv:2504.00434v1 Announce Type: new 
Abstract: In the realm of AI, large language models (LLMs) like GPT-4, central to the operation of AI agents, predominantly operate in the cloud, incurring high operational costs. With local-based small language models (SLMs) becoming more accurate, the necessity of cloud-exclusive processing is being reconsidered. An AI agent's response to a user's request comprises a series of subtasks or iterations. Existing approaches only allocate a single request between SLM and LLM to ensure their outputs are similar, but adopting this approach in the AI agent scenario for assigning each subtask is not effective since SLM will output a different subsequent subtask, which affects the accuracy of the final output. In this paper, we first conduct experimental analysis to understand the features of AI agent operations. Leveraging our findings, we propose the Adaptive Iteration-level Model Selector (AIMS), a lightweight scheduler to automatically partition AI agent's subtasks between local-based SLM and cloud-based LLM. AIMS considers the varying subtask features and strategically decides the location for each subtask in order to use SLM as much as possible while attaining the accuracy level. Our experimental results demonstrate that AIMS increases accuracy by up to 9.1% and SLM usage by up to 10.8% compared to HybridLLM. It offloads 45.67% of subtasks to a local SLM while attaining similar accuracy on average compared with the cloud-only LLM approach.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2412.05447">TOBUGraph: Knowledge Graph-Based Retrieval for Enhanced LLM Performance Beyond RAG</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Savini Kashmira, Jayanaka L. Dantanarayana, Joshua Brodsky, Ashish Mahendra, Yiping Kang, Krisztian Flautner, Lingjia Tang, Jason Mars</div>
        <div class="preprint-abstract">arXiv:2412.05447v2 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) is one of the leading and most widely used techniques for enhancing LLM retrieval capabilities, but it still faces significant limitations in commercial use cases. RAG primarily relies on the query-chunk text-to-text similarity in the embedding space for retrieval and can fail to capture deeper semantic relationships across chunks, is highly sensitive to chunking strategies, and is prone to hallucinations. To address these challenges, we propose TOBUGraph, a graph-based retrieval framework that first constructs the knowledge graph from unstructured data dynamically and automatically. Using LLMs, TOBUGraph extracts structured knowledge and diverse relationships among data, going beyond RAG's text-to-text similarity. Retrieval is achieved through graph traversal, leveraging the extracted relationships and structures to enhance retrieval accuracy, eliminating the need for chunking configurations while reducing hallucination. We demonstrate TOBUGraph's effectiveness in TOBU, a real-world application in production for personal memory organization and retrieval. Our evaluation using real user data demonstrates that TOBUGraph outperforms multiple RAG implementations in both precision and recall, significantly improving user experience through improved retrieval accuracy.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2412.00596">PhyT2V: LLM-Guided Iterative Self-Refinement for Physics-Grounded Text-to-Video Generation</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Qiyao Xue, Xiangyu Yin, Boyuan Yang, Wei Gao</div>
        <div class="preprint-abstract">arXiv:2412.00596v2 Announce Type: replace-cross 
Abstract: Text-to-video (T2V) generation has been recently enabled by transformer-based diffusion models, but current T2V models lack capabilities in adhering to the real-world common knowledge and physical rules, due to their limited understanding of physical realism and deficiency in temporal modeling. Existing solutions are either data-driven or require extra model inputs, but cannot be generalizable to out-of-distribution domains. In this paper, we present PhyT2V, a new data-independent T2V technique that expands the current T2V model's capability of video generation to out-of-distribution domains, by enabling chain-of-thought and step-back reasoning in T2V prompting. Our experiments show that PhyT2V improves existing T2V models' adherence to real-world physical rules by 2.3x, and achieves 35% improvement compared to T2V prompt enhancers. The source codes are available at: https://github.com/pittisl/PhyT2V.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00441">No Free Lunch with Guardrails</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Divyanshu Kumar, Nitin Aravind Birur, Tanay Baswa, Sahil Agarwal, Prashanth Harshangi</div>
        <div class="preprint-abstract">arXiv:2504.00441v1 Announce Type: cross 
Abstract: As large language models (LLMs) and generative AI become widely adopted, guardrails have emerged as a key tool to ensure their safe use. However, adding guardrails isn't without tradeoffs; stronger security measures can reduce usability, while more flexible systems may leave gaps for adversarial attacks. In this work, we explore whether current guardrails effectively prevent misuse while maintaining practical utility. We introduce a framework to evaluate these tradeoffs, measuring how different guardrails balance risk, security, and usability, and build an efficient guardrail.
  Our findings confirm that there is no free lunch with guardrails; strengthening security often comes at the cost of usability. To address this, we propose a blueprint for designing better guardrails that minimize risk while maintaining usability. We evaluate various industry guardrails, including Azure Content Safety, Bedrock Guardrails, OpenAI's Moderation API, Guardrails AI, Nemo Guardrails, and our own custom-built guardrails. Additionally, we assess how LLMs like GPT-4o, Gemini 2.0-Flash, Claude 3.5-Sonnet, and Mistral Large-Latest respond under different system prompts, including simple prompts, detailed prompts, and detailed prompts with chain-of-thought (CoT) reasoning. Our study provides a clear comparison of how different guardrails perform, highlighting the challenges in balancing security and usability.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00286">Digital Twins in Biopharmaceutical Manufacturing: Review and Perspective on Human-Machine Collaborative Intelligence</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Mohammed Aatif Shahab, Francesco Destro, Richard D. Braatz</div>
        <div class="preprint-abstract">arXiv:2504.00286v1 Announce Type: cross 
Abstract: The biopharmaceutical industry is increasingly developing digital twins to digitalize and automate the manufacturing process in response to the growing market demands. However, this shift presents significant challenges for human operators, as the complexity and volume of information can overwhelm their ability to manage the process effectively. These issues are compounded when digital twins are designed without considering interaction and collaboration with operators, who are responsible for monitoring processes and assessing situations, particularly during abnormalities. Our review of current trends in biopharma digital twin development reveals a predominant focus on technology and often overlooks the critical role of human operators. To bridge this gap, this article proposes a collaborative intelligence framework that emphasizes the integration of operators with digital twins. Approaches to system design that can enhance operator trust and human-machine interface usability are presented. Moreover, innovative training programs for preparing operators to understand and utilize digital twins are discussed. The framework outlined in this article aims to enhance collaboration between operators and digital twins effectively by using their full capabilities to boost resilience and productivity in biopharmaceutical manufacturing.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2410.14758">Improving Vector-Quantized Image Modeling with Latent Consistency-Matching Diffusion</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Bac Nguyen, Chieh-Hsin Lai, Yuhta Takida, Naoki Murata, Toshimitsu Uesaka, Stefano Ermon, Yuki Mitsufuji</div>
        <div class="preprint-abstract">arXiv:2410.14758v2 Announce Type: replace 
Abstract: By embedding discrete representations into a continuous latent space, we can leverage continuous-space latent diffusion models to handle generative modeling of discrete data. However, despite their initial success, most latent diffusion methods rely on fixed pretrained embeddings, limiting the benefits of joint training with the diffusion model. While jointly learning the embedding (via reconstruction loss) and the latent diffusion model (via score matching loss) could enhance performance, end-to-end training risks embedding collapse, degrading generation quality. To mitigate this issue, we introduce VQ-LCMD, a continuous-space latent diffusion framework within the embedding space that stabilizes training. VQ-LCMD uses a novel training objective combining the joint embedding-diffusion variational lower bound with a consistency-matching (CM) loss, alongside a shifted cosine noise schedule and random dropping strategy. Experiments on several benchmarks show that the proposed VQ-LCMD yields superior results on FFHQ, LSUN Churches, and LSUN Bedrooms compared to discrete-state latent diffusion models. In particular, VQ-LCMD achieves an FID of 6.81 for class-conditional image generation on ImageNet with 50 steps.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.21074">Rerouting Connection: Hybrid Computer Vision Analysis Reveals Visual Similarity Between Indus and Tibetan-Yi Corridor Writing Systems</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Ooha Lakkadi Reddy</div>
        <div class="preprint-abstract">arXiv:2503.21074v2 Announce Type: replace-cross 
Abstract: This thesis employs a hybrid CNN-Transformer architecture, in conjunction with a detailed anthropological framework, to investigate potential historical connections between the visual morphology of the Indus Valley script and pictographic systems of the Tibetan-Yi Corridor. Through an ensemble methodology of three target scripts across 15 independently trained models, we demonstrate that Tibetan-Yi Corridor scripts exhibit approximately six-fold higher visual similarity to the Indus script (61.7%-63.5%) than to the Bronze Age Proto-Cuneiform (10.2%-10.9%) or Proto-Elamite (7.6%-8.7%) systems. Additionally and contrarily to our current understanding of the networks of the Indus Valley Civilization, the Indus script unexpectedly maps closer to Tibetan-Yi Corridor scripts, with a mean cosine similarity of 0.629, than to the aforementioned contemporaneous West Asian signaries, both of which recorded mean cosine similarities of 0.104 and 0.080 despite their close geographic proximity and evident trade relations. Across various dimensionality reduction practices and clustering methodologies, the Indus script consistently clusters closest to Tibetan-Yi Corridor scripts. Our computational results align with qualitative observations of specific pictorial parallels in numeral systems, gender markers, and key iconographic elements; this is further supported by archaeological evidence of sustained contact networks along the ancient Shu-Shendu road in tandem with the Indus Valley Civilization's decline, providing a plausible transmission pathway. While alternative explanations cannot be ruled out, the specificity and consistency of observed similarities challenge conventional narratives of isolated script development and suggest more complex ancient cultural transmission networks between South and East Asia than previously recognized.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2410.15164">SPA-Bench: A Comprehensive Benchmark for SmartPhone Agent Evaluation</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jingxuan Chen, Derek Yuen, Bin Xie, Yuhao Yang, Gongwei Chen, Zhihao Wu, Li Yixing, Xurui Zhou, Weiwen Liu, Shuai Wang, Kaiwen Zhou, Rui Shao, Liqiang Nie, Yasheng Wang, Jianye Hao, Jun Wang, Kun Shao</div>
        <div class="preprint-abstract">arXiv:2410.15164v3 Announce Type: replace 
Abstract: Smartphone agents are increasingly important for helping users control devices efficiently, with (Multimodal) Large Language Model (MLLM)-based approaches emerging as key contenders. Fairly comparing these agents is essential but challenging, requiring a varied task scope, the integration of agents with different implementations, and a generalisable evaluation pipeline to assess their strengths and weaknesses. In this paper, we present SPA-Bench, a comprehensive SmartPhone Agent Benchmark designed to evaluate (M)LLM-based agents in an interactive environment that simulates real-world conditions. SPA-Bench offers three key contributions: (1) A diverse set of tasks covering system and third-party apps in both English and Chinese, focusing on features commonly used in daily routines; (2) A plug-and-play framework enabling real-time agent interaction with Android devices, integrating over ten agents with the flexibility to add more; (3) A novel evaluation pipeline that automatically assesses agent performance across multiple dimensions, encompassing seven metrics related to task completion and resource consumption. Our extensive experiments across tasks and agents reveal challenges like interpreting mobile user interfaces, action grounding, memory retention, and execution costs. We propose future research directions to ease these difficulties, moving closer to real-world smartphone agent applications. SPA-Bench is available at https://ai-agents-2030.github.io/SPA-Bench/.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00058">GAL-MAD: Towards Explainable Anomaly Detection in Microservice Applications Using Graph Attention Networks</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Lahiru Akmeemana, Chamodya Attanayake, Husni Faiz, Sandareka Wickramanayake</div>
        <div class="preprint-abstract">arXiv:2504.00058v1 Announce Type: cross 
Abstract: The transition to microservices has revolutionized software architectures, offering enhanced scalability and modularity. However, the distributed and dynamic nature of microservices introduces complexities in ensuring system reliability, making anomaly detection crucial for maintaining performance and functionality. Anomalies stemming from network and performance issues must be swiftly identified and addressed. Existing anomaly detection techniques often rely on statistical models or machine learning methods that struggle with the high-dimensional, interdependent data inherent in microservice applications. Current techniques and available datasets predominantly focus on system traces and logs, limiting their ability to support advanced detection models. This paper addresses these gaps by introducing the RS-Anomic dataset generated using the open-source RobotShop microservice application. The dataset captures multivariate performance metrics and response times under normal and anomalous conditions, encompassing ten types of anomalies. We propose a novel anomaly detection model called Graph Attention and LSTM-based Microservice Anomaly Detection (GAL-MAD), leveraging Graph Attention and Long Short-Term Memory architectures to capture spatial and temporal dependencies in microservices. We utilize SHAP values to localize anomalous services and identify root causes to enhance explainability. Experimental results demonstrate that GAL-MAD outperforms state-of-the-art models on the RS-Anomic dataset, achieving higher accuracy and recall across varying anomaly rates. The explanations provide actionable insights into service anomalies, which benefits system administrators.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.19326">Process or Result? Manipulated Ending Tokens Can Mislead Reasoning LLMs to Ignore the Correct Reasoning Steps</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Yu Cui, Bryan Hooi, Yujun Cai, Yiwei Wang</div>
        <div class="preprint-abstract">arXiv:2503.19326v2 Announce Type: replace 
Abstract: Recent reasoning large language models (LLMs) have demonstrated remarkable improvements in mathematical reasoning capabilities through long Chain-of-Thought. The reasoning tokens of these models enable self-correction within reasoning chains, enhancing robustness. This motivates our exploration: how vulnerable are reasoning LLMs to subtle errors in their input reasoning chains? We introduce "Compromising Thought" (CPT), a vulnerability where models presented with reasoning tokens containing manipulated calculation results tend to ignore correct reasoning steps and adopt incorrect results instead. Through systematic evaluation across multiple reasoning LLMs, we design three increasingly explicit prompting methods to measure CPT resistance, revealing that models struggle significantly to identify and correct these manipulations. Notably, contrary to existing research suggesting structural alterations affect model performance more than content modifications, we find that local ending token manipulations have greater impact on reasoning outcomes than structural changes. Moreover, we discover a security vulnerability in DeepSeek-R1 where tampered reasoning tokens can trigger complete reasoning cessation. Our work enhances understanding of reasoning robustness and highlights security considerations for reasoning-intensive applications.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2311.02467">Individualized Policy Evaluation and Learning under Clustered Network Interference</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Yi Zhang, Kosuke Imai</div>
        <div class="preprint-abstract">arXiv:2311.02467v3 Announce Type: replace-cross 
Abstract: Although there is now a large literature on policy evaluation and learning, much of the prior work assumes that the treatment assignment of one unit does not affect the outcome of another unit. Unfortunately, ignoring interference can lead to biased policy evaluation and ineffective learned policies. For example, treating influential individuals who have many friends can generate positive spillover effects, thereby improving the overall performance of an individualized treatment rule (ITR). We consider the problem of evaluating and learning an optimal ITR under clustered network interference (also known as partial interference), where clusters of units are sampled from a population and units may influence one another within each cluster. Unlike previous methods that impose strong restrictions on spillover effects, such as anonymous interference, the proposed methodology only assumes a semiparametric structural model, where each unit's outcome is an additive function of individual treatments within the cluster. Under this model, we propose an estimator that can be used to evaluate the empirical performance of an ITR. We show that this estimator is substantially more efficient than the standard inverse probability weighting estimator, which does not impose any assumption about spillover effects. We derive the finite-sample regret bound for a learned ITR, showing that the use of our efficient evaluation estimator leads to the improved performance of learned policies. We consider both experimental and observational studies, and for the latter, we develop a doubly robust estimator that is semiparametrically efficient and yields an optimal regret bound. Finally, we conduct simulation and empirical studies to illustrate the advantages of the proposed methodology.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.23488">$p$-Adic Polynomial Regression as Alternative to Neural Network for Approximating $p$-Adic Functions of Many Variables</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Alexander P. Zubarev</div>
        <div class="preprint-abstract">arXiv:2503.23488v2 Announce Type: replace-cross 
Abstract: A method for approximating continuous functions $\mathbb{Z}_{p}^{n}\rightarrow\mathbb{Z}_{p}$ by a linear superposition of continuous functions $\mathbb{Z}_{p}\rightarrow\mathbb{Z}_{p}$ is presented and a polynomial regression model is constructed that allows approximating such functions with any degree of accuracy. A physical interpretation of such a model is given and possible methods for its training are discussed. The proposed model can be considered as a simple alternative to possible $p$-adic models based on neural network architecture.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2407.14561">NNsight and NDIF: Democratizing Access to Open-Weight Foundation Model Internals</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jaden Fiotto-Kaufman, Alexander R. Loftus, Eric Todd, Jannik Brinkmann, Koyena Pal, Dmitrii Troitskii, Michael Ripa, Adam Belfki, Can Rager, Caden Juang, Aaron Mueller, Samuel Marks, Arnab Sen Sharma, Francesca Lucchetti, Nikhil Prakash, Carla Brodley, Arjun Guha, Jonathan Bell, Byron C. Wallace, David Bau</div>
        <div class="preprint-abstract">arXiv:2407.14561v4 Announce Type: replace 
Abstract: We introduce NNsight and NDIF, technologies that work in tandem to enable scientific study of the representations and computations learned by very large neural networks. NNsight is an open-source system that extends PyTorch to introduce deferred remote execution. The National Deep Inference Fabric (NDIF) is a scalable inference service that executes NNsight requests, allowing users to share GPU resources and pretrained models. These technologies are enabled by the Intervention Graph, an architecture developed to decouple experimental design from model runtime. Together, this framework provides transparent and efficient access to the internals of deep neural networks such as very large language models (LLMs) without imposing the cost or complexity of hosting customized models individually. We conduct a quantitative survey of the machine learning literature that reveals a growing gap in the study of the internals of large-scale AI. We demonstrate the design and use of our framework to address this gap by enabling a range of research methods on huge models. Finally, we conduct benchmarks to compare performance with previous approaches.
  Code, documentation, and tutorials are available at https://nnsight.net/.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00176">Discriminative Subspace Emersion from learning feature relevances across different populations</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Marco Canducci, Lida Abdi, Alessandro Prete, Roland J. Veen, Michael Biehl, Wiebke Arlt, Peter Tino</div>
        <div class="preprint-abstract">arXiv:2504.00176v1 Announce Type: new 
Abstract: In a given classification task, the accuracy of the learner is often hampered by finiteness of the training set, high-dimensionality of the feature space and severe overlap between classes. In the context of interpretable learners, with (piecewise) linear separation boundaries, these issues can be mitigated by careful construction of optimization procedures and/or estimation of relevant features for the task. However, when the task is shared across two disjoint populations the main interest is shifted towards estimating a set of features that discriminate the most between the two, when performing classification. We propose a new Discriminative Subspace Emersion (DSE) method to extend subspace learning toward a general relevance learning framework. DSE allows us to identify the most relevant features in distinguishing the classification task across two populations, even in cases of high overlap between classes. The proposed methodology is designed to work with multiple sets of labels and is derived in principle without being tied to a specific choice of base learner. Theoretical and empirical investigations over synthetic and real-world datasets indicate that DSE accurately identifies a common subspace for the classification across different populations. This is shown to be true for a surprisingly high degree of overlap between classes.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00847">Logical perspectives on learning statistical objects</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Aaron Anderson, Michael Benedikt</div>
        <div class="preprint-abstract">arXiv:2504.00847v1 Announce Type: cross 
Abstract: We consider the relationship between learnability of a ``base class'' of functions on a set X and learnability of a class of statistical functions derived from the base class. For example, we refine results showing that learnability of a family of functions implies learnability of the family of functions mapping a function in the class to its expectation under a distribution. We will look at both Probably Approximately Correct (PAC) learning, where example inputs and outputs are chosen at random, and online learning, where the examples are chosen adversarially. We establish improved bounds on the sample complexity of learning for statistical classes, stated in terms of combinatorial dimensions of the base class. We do this by adapting techniques introduced in model theory for ``randomizing a structure''. We give particular attention to classes derived from logical formulas, and relate learnability of the statistical classes to properties of the formula. Finally, we provide bounds on the complexity of learning the statistical classes built on top of a logic-based hypothesis class.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00043">CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jixuan Leng, Chengsong Huang, Langlin Huang, Bill Yuchen Lin, William W. Cohen, Haohan Wang, Jiaxin Huang</div>
        <div class="preprint-abstract">arXiv:2504.00043v1 Announce Type: cross 
Abstract: Existing reasoning evaluation frameworks for Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) predominantly either assess text-based reasoning or vision-language understanding capabilities, with limited dynamic interplay between textual and visual constraints. To address this limitation, we introduce CrossWordBench, a benchmark designed to evaluate the reasoning capabilities of both LLMs and LVLMs through the medium of crossword puzzles-a task requiring multimodal adherence to semantic constraints from text-based clues and intersectional constraints from visual grid structures. CrossWordBench leverages a controllable puzzle generation framework that produces puzzles in multiple formats (text and image) and offers different evaluation strategies ranging from direct puzzle solving to interactive modes. Our extensive evaluation of over 20 models reveals that reasoning LLMs outperform non-reasoning models substantially by effectively leveraging crossing-letter constraints. We further demonstrate that LVLMs struggle with the task, showing a strong correlation between their puzzle-solving performance and grid-parsing accuracy. Our findings offer insights into the limitations of the reasoning capabilities of current LLMs and LVLMs, and provide an effective approach for creating multimodal constrained tasks for future evaluations.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00999">MergeVQ: A Unified Framework for Visual Generation and Representation with Disentangled Token Merging and Quantization</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Siyuan Li, Luyuan Zhang, Zedong Wang, Juanxi Tian, Cheng Tan, Zicheng Liu, Chang Yu, Qingsong Xie, Haonan Lu, Haoqian Wang, Zhen Lei</div>
        <div class="preprint-abstract">arXiv:2504.00999v1 Announce Type: cross 
Abstract: Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great success in both self-supervised pre-training and image generation. However, most existing methods struggle to address the trade-off in shared latent space for generation quality vs. representation learning and efficiency. To push the limits of this paradigm, we propose MergeVQ, which incorporates token merging techniques into VQ-based generative models to bridge the gap between image generation and visual representation learning in a unified architecture. During pre-training, MergeVQ decouples top-k semantics from latent space with the token merge module after self-attention blocks in the encoder for subsequent Look-up Free Quantization (LFQ) and global alignment and recovers their fine-grained details through cross-attention in the decoder for reconstruction. As for the second-stage generation, we introduce MergeAR, which performs KV Cache compression for efficient raster-order prediction. Extensive experiments on ImageNet verify that MergeVQ as an AR generative model achieves competitive performance in both visual representation learning and image generation tasks while maintaining favorable token efficiency and inference speed. The code and model will be available at https://apexgen-x.github.io/MergeVQ.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00424">Hawkeye:Efficient Reasoning with Model Collaboration</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jianshu She, Zhuohao Li, Zhemin Huang, Qi Li, Peiran Xu, Haonan Li, Qirong Ho</div>
        <div class="preprint-abstract">arXiv:2504.00424v1 Announce Type: new 
Abstract: Chain-of-Thought (CoT) reasoning has demonstrated remarkable effectiveness in enhancing the reasoning abilities of large language models (LLMs). However, its efficiency remains a challenge due to the generation of excessive intermediate reasoning tokens, which introduce semantic redundancy and overly detailed reasoning steps. Moreover, computational expense and latency are significant concerns, as the cost scales with the number of output tokens, including those intermediate steps. In this work, we observe that most CoT tokens are unnecessary, and retaining only a small portion of them is sufficient for producing high-quality responses. Inspired by this, we propose HAWKEYE, a novel post-training and inference framework where a large model produces concise CoT instructions to guide a smaller model in response generation. HAWKEYE quantifies redundancy in CoT reasoning and distills high-density information via reinforcement learning. By leveraging these concise CoTs, HAWKEYE is able to expand responses while reducing token usage and computational cost significantly. Our evaluation shows that HAWKEYE can achieve comparable response quality using only 35% of the full CoTs, while improving clarity, coherence, and conciseness by approximately 10%. Furthermore, HAWKEYE can accelerate end-to-end reasoning by up to 3.4x on complex math tasks while reducing inference cost by up to 60%. HAWKEYE will be open-sourced and the models will be available soon.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2409.18132">Decomposition of one-layer neural networks via the infinite sum of reproducing kernel Banach spaces</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Seungcheol Shin, Myungjoo Kang</div>
        <div class="preprint-abstract">arXiv:2409.18132v2 Announce Type: replace-cross 
Abstract: In this paper, we define the sum of RKBSs using the characterization theorem of RKBSs and show that the sum of RKBSs is compatible with the direct sum of feature spaces. Moreover, we decompose the integral RKBS into the sum of $p$-norm RKBSs. Finally, we provide applications for the structural understanding of the integral RKBS class.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2502.14050">Diversity-driven Data Selection for Language Model Tuning through Sparse Autoencoder</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Xianjun Yang, Shaoliang Nie, Lijuan Liu, Suchin Gururangan, Ujjwal Karn, Rui Hou, Madian Khabsa, Yuning Mao</div>
        <div class="preprint-abstract">arXiv:2502.14050v2 Announce Type: replace-cross 
Abstract: Instruction tuning data are often quantity-saturated due to the large volume of data collection and fast model iteration, leaving data selection important but underexplored. Existing quality-driven data selection methods, such as LIMA (NeurIPS 2023 \citep{zhou2024lima}) and AlpaGasus (ICLR 2024 \citep{chenalpagasus}) generally ignore the equal importance of data diversity and complexity. In this work, we aim to design a diversity-aware data selection strategy and creatively propose using sparse autoencoders (SAEs) to tackle the challenge of data diversity measure. In addition, SAEs can also provide more interpretability of model behavior and explain, e.g., the surprising effectiveness of selecting the longest response (ICML 2024 \citep{zhaolong}). Using effective data selection, we experimentally prove that models trained on our selected data can outperform other methods in terms of model capabilities, reduce training cost, and potentially gain more control over model behaviors. We prove that SAEs can serve as a good alternative to diversity measure and design our method to be scalable for potential industrial large-scale pruning, and we will also release our trained SAEs for use by the broader community.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.01005">When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Nishad Singhi, Hritik Bansal, Arian Hosseini, Aditya Grover, Kai-Wei Chang, Marcus Rohrbach, Anna Rohrbach</div>
        <div class="preprint-abstract">arXiv:2504.01005v1 Announce Type: cross 
Abstract: Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most common answer via majority voting. Another common method involves scoring each solution with a reward model (verifier) and choosing the best one. Recent advancements in Generative Reward Models (GenRM) reframe verification as a next-token prediction task, enabling inference-time scaling along a new axis. Specifically, GenRM generates multiple verification chains-of-thought to score each solution. Under a limited inference budget, this introduces a fundamental trade-off: should you spend the budget on scaling solutions via SC or generate fewer solutions and allocate compute to verification via GenRM? To address this, we evaluate GenRM against SC under a fixed inference budget. Interestingly, we find that SC is more compute-efficient than GenRM for most practical inference budgets across diverse models and datasets. For instance, GenRM first matches SC after consuming up to 8x the inference compute and requires significantly more compute to outperform it. Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. Our work provides practical guidance on optimizing test-time scaling by balancing solution generation and verification. The code is available at https://github.com/nishadsinghi/sc-genrm-scaling.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.23757">Time-Series Forecasting via Topological Information Supervised Framework with Efficient Topological Feature Learning</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">ZiXin Lin, Nur Fariha Syaqina Zulkepli</div>
        <div class="preprint-abstract">arXiv:2503.23757v2 Announce Type: replace 
Abstract: Topological Data Analysis (TDA) has emerged as a powerful tool for extracting meaningful features from complex data structures, driving significant advancements in fields such as neuroscience, biology, machine learning, and financial modeling. Despite its success, the integration of TDA with time-series prediction remains underexplored due to three primary challenges: the limited utilization of temporal dependencies within topological features, computational bottlenecks associated with persistent homology, and the deterministic nature of TDA pipelines restricting generalized feature learning. This study addresses these challenges by proposing the Topological Information Supervised (TIS) Prediction framework, which leverages neural networks and Conditional Generative Adversarial Networks (CGANs) to generate synthetic topological features, preserving their distribution while significantly reducing computational time. We propose a novel training strategy that integrates topological consistency loss to improve the predictive accuracy of deep learning models. Specifically, we introduce two state-of-the-art models, TIS-BiGRU and TIS-Informer, designed to capture short-term and long-term temporal dependencies, respectively. Comparative experimental results demonstrate the superior performance of TIS models over conventional predictors, validating the effectiveness of integrating topological information. This work not only advances TDA-based time-series prediction but also opens new avenues for utilizing topological features in deep learning architectures.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.21788">PharMolixFM: All-Atom Foundation Models for Molecular Modeling and Generation</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Yizhen Luo, Jiashuo Wang, Siqi Fan, Zaiqing Nie</div>
        <div class="preprint-abstract">arXiv:2503.21788v3 Announce Type: replace-cross 
Abstract: Structural biology relies on accurate three-dimensional biomolecular structures to advance our understanding of biological functions, disease mechanisms, and therapeutics. While recent advances in deep learning have enabled the development of all-atom foundation models for molecular modeling and generation, existing approaches face challenges in generalization due to the multi-modal nature of atomic data and the lack of comprehensive analysis of training and sampling strategies. To address these limitations, we propose PharMolixFM, a unified framework for constructing all-atom foundation models based on multi-modal generative techniques. Our framework includes three variants using state-of-the-art multi-modal generative models. By formulating molecular tasks as a generalized denoising process with task-specific priors, PharMolixFM achieves robust performance across various structural biology applications. Experimental results demonstrate that PharMolixFM-Diff achieves competitive prediction accuracy in protein-small-molecule docking (83.9% vs. 90.2% RMSD < 2{\AA}, given pocket) with significantly improved inference speed. Moreover, we explore the empirical inference scaling law by introducing more sampling repeats or steps. Our code and model are available at https://github.com/PharMolix/OpenBioMed.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00146">Why risk matters for protein binder design</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Tudor Cotet, Igor Krawczuk</div>
        <div class="preprint-abstract">arXiv:2504.00146v1 Announce Type: new 
Abstract: Bayesian optimization (BO) has recently become more prevalent in protein engineering applications and hence has become a fruitful target of benchmarks. However, current BO comparisons often overlook real-world considerations like risk and cost constraints. In this work, we compare 72 model combinations of encodings, surrogate models, and acquisition functions on 11 protein binder fitness landscapes, specifically from this perspective. Drawing from the portfolio optimization literature, we adopt metrics to quantify the cold-start performance relative to a random baseline, to assess the risk of an optimization campaign, and to calculate the overall budget required to reach a fitness threshold. Our results suggest the existence of Pareto-optimal models on the risk-performance axis, the shift of this preference depending on the landscape explored, and the robust correlation between landscape properties such as epistasis with the average and worst-case model performance. They also highlight that rigorous model selection requires substantial computational and statistical efforts.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00467">Informed Greedy Algorithm for Scalable Bayesian Network Fusion via Minimum Cut Analysis</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Pablo Torrijos, Jos\'e M. Puerta, Jos\'e A. G\'amez, Juan A. Aledo</div>
        <div class="preprint-abstract">arXiv:2504.00467v1 Announce Type: new 
Abstract: This paper presents the Greedy Min-Cut Bayesian Consensus (GMCBC) algorithm for the structural fusion of Bayesian Networks (BNs). The method is designed to preserve essential dependencies while controlling network complexity. It addresses the limitations of traditional fusion approaches, which often lead to excessively complex models that are impractical for inference, reasoning, or real-world applications. As the number and size of input networks increase, this issue becomes even more pronounced. GMCBC integrates principles from flow network theory into BN fusion, adapting the Backward Equivalence Search (BES) phase of the Greedy Equivalence Search (GES) algorithm and applying the Ford-Fulkerson algorithm for minimum cut analysis. This approach removes non-essential edges, ensuring that the fused network retains key dependencies while minimizing unnecessary complexity. Experimental results on synthetic Bayesian Networks demonstrate that GMCBC achieves near-optimal network structures. In federated learning simulations, GMCBC produces a consensus network that improves structural accuracy and dependency preservation compared to the average of the input networks, resulting in a structure that better captures the real underlying (in)dependence relationships. This consensus network also maintains a similar size to the original networks, unlike unrestricted fusion methods, where network size grows exponentially.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2502.19231">AI-Powered Bayesian Inference</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Veronika Ro\v{c}kov\'a, Sean O'Hagan</div>
        <div class="preprint-abstract">arXiv:2502.19231v2 Announce Type: replace-cross 
Abstract: The advent of Generative Artificial Intelligence (GAI) has heralded an inflection point that changed how society thinks about knowledge acquisition. While GAI cannot be fully trusted for decision-making, it may still provide valuable information that can be integrated into a decision pipeline. Rather than seeing the lack of certitude and inherent randomness of GAI as a problem, we view it as an opportunity. Indeed, variable answers to given prompts can be leveraged to construct a prior distribution which reflects assuredness of AI predictions. This prior distribution may be combined with tailored datasets for a fully Bayesian analysis with an AI-driven prior. In this paper, we explore such a possibility within a non-parametric Bayesian framework. The basic idea consists of assigning a Dirichlet process prior distribution on the data-generating distribution with AI generative model as its baseline. Hyper-parameters of the prior can be tuned out-of-sample to assess the informativeness of the AI prior. Posterior simulation is achieved by computing a suitably randomized functional on an augmented data that consists of observed (labeled) data as well as fake data whose labels have been imputed using AI. This strategy can be parallelized and rapidly produces iid samples from the posterior by optimization as opposed to sampling from conditionals. Our method enables (predictive) inference and uncertainty quantification leveraging AI predictions in a coherent probabilistic manner.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.20290">QualiSpeech: A Speech Quality Assessment Dataset with Natural Language Reasoning and Descriptions</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Siyin Wang, Wenyi Yu, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Lu Lu, Yu Tsao, Junichi Yamagishi, Yuxuan Wang, Chao Zhang</div>
        <div class="preprint-abstract">arXiv:2503.20290v2 Announce Type: replace-cross 
Abstract: This paper explores a novel perspective to speech quality assessment by leveraging natural language descriptions, offering richer, more nuanced insights than traditional numerical scoring methods. Natural language feedback provides instructive recommendations and detailed evaluations, yet existing datasets lack the comprehensive annotations needed for this approach. To bridge this gap, we introduce QualiSpeech, a comprehensive low-level speech quality assessment dataset encompassing 11 key aspects and detailed natural language comments that include reasoning and contextual insights. Additionally, we propose the QualiSpeech Benchmark to evaluate the low-level speech understanding capabilities of auditory large language models (LLMs). Experimental results demonstrate that finetuned auditory LLMs can reliably generate detailed descriptions of noise and distortion, effectively identifying their types and temporal characteristics. The results further highlight the potential for incorporating reasoning to enhance the accuracy and reliability of quality assessments. The dataset will be released at https://huggingface.co/datasets/tsinghua-ee/QualiSpeech.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00663">Sim-is-More: Randomizing HW-NAS with Synthetic Devices</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Francesco Capuano, Gabriele Tiboni, Niccol\`o Cavagnero, Giuseppe Averta</div>
        <div class="preprint-abstract">arXiv:2504.00663v1 Announce Type: new 
Abstract: Existing hardware-aware NAS (HW-NAS) methods typically assume access to precise information circa the target device, either via analytical approximations of the post-compilation latency model, or through learned latency predictors. Such approximate approaches risk introducing estimation errors that may prove detrimental in risk-sensitive applications. In this work, we propose a two-stage HW-NAS framework, in which we first learn an architecture controller on a distribution of synthetic devices, and then directly deploy the controller on a target device. At test-time, our network controller deploys directly to the target device without relying on any pre-collected information, and only exploits direct interactions. In particular, the pre-training phase on synthetic devices enables the controller to design an architecture for the target device by interacting with it through a small number of high-fidelity latency measurements. To guarantee accessibility of our method, we only train our controller with training-free accuracy proxies, allowing us to scale the meta-training phase without incurring the overhead of full network training. We benchmark on HW-NATS-Bench, demonstrating that our method generalizes to unseen devices and searches for latency-efficient architectures by in-context adaptation using only a few real-world latency evaluations at test-time.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2502.11381">Without Paired Labeled Data: An End-to-End Self-Supervised Paradigm for UAV-View Geo-Localization</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Zhongwei Chen, Zhao-Xu Yang, Hai-Jun Rong</div>
        <div class="preprint-abstract">arXiv:2502.11381v2 Announce Type: replace-cross 
Abstract: UAV-View Geo-Localization (UVGL) aims to achieve accurate localization of unmanned aerial vehicles (UAVs) by retrieving the most relevant GPS-tagged satellite images. However, existing methods heavily rely on pre-paired UAV-satellite images for supervised learning. Such dependency not only incurs high annotation costs but also severely limits scalability and practical deployment in open-world UVGL scenarios. To address these limitations, we propose an end-to-end self-supervised UVGL method. Our method leverages a shallow backbone network to extract initial features, employs clustering to generate pseudo labels, and adopts a dual-path contrastive learning architecture to learn discriminative intra-view representations. Furthermore, our method incorporates two core modules, the dynamic hierarchical memory learning module and the information consistency evolution learning module. The dynamic hierarchical memory learning module combines short-term and long-term memory to enhance intra-view feature consistency and discriminability. Meanwhile, the information consistency evolution learning module leverages a neighborhood-driven dynamic constraint mechanism to systematically capture implicit cross-view semantic correlations, thereby improving cross-view feature alignment. To further stabilize and strengthen the self-supervised training process, a pseudo-label enhancement strategy is introduced, which refines the quality of pseudo supervision. Our method ultimately constructs a unified cross-view feature representation space under self-supervised settings. Extensive experiments on three public benchmark datasets demonstrate that the proposed method consistently outperforms existing self-supervised methods and even surpasses several state-of-the-art supervised methods. Our code is available at https://github.com/ISChenawei/DMNIL.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2403.10045">Towards Adversarially Robust Dataset Distillation by Curvature Regularization</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Eric Xue, Yijiang Li, Haoyang Liu, Peiran Wang, Yifan Shen, Haohan Wang</div>
        <div class="preprint-abstract">arXiv:2403.10045v3 Announce Type: replace 
Abstract: Dataset distillation (DD) allows datasets to be distilled to fractions of their original size while preserving the rich distributional information so that models trained on the distilled datasets can achieve a comparable accuracy while saving significant computational loads. Recent research in this area has been focusing on improving the accuracy of models trained on distilled datasets. In this paper, we aim to explore a new perspective of DD. We study how to embed adversarial robustness in distilled datasets, so that models trained on these datasets maintain the high accuracy and meanwhile acquire better adversarial robustness. We propose a new method that achieves this goal by incorporating curvature regularization into the distillation process with much less computational overhead than standard adversarial training. Extensive empirical experiments suggest that our method not only outperforms standard adversarial training on both accuracy and robustness with less computation overhead but is also capable of generating robust distilled datasets that can withstand various adversarial attacks.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2409.08877">Influence of erythrocyte density on aggregability as a marker of cell age: Dissociation dynamics in extensional flow</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">M. Puthumana Melepattu, G. Ma\^itrejean, C. Wagner, T. Podgorski</div>
        <div class="preprint-abstract">arXiv:2409.08877v3 Announce Type: replace 
Abstract: Blood rheology and microcirculation are strongly influenced by red blood cell (RBC) aggregation. The aggregability of RBCs can vary significantly due to factors such as their mechanical and membrane surface properties, which are affected by cell aging in vivo. In this study, we investigate RBC aggregability as a function of their density, a marker of cell age and mechanical properties, by separating RBCs from healthy donors into different density fractions using Percoll density gradient centrifugation. We examine the dissociation rates of aggregates in a controlled medium supplemented with Dextran, employing an extensional flow technique based on hyperbolic microfluidic constrictions and image analysis, assisted by a convolutional neural network (CNN). In contrast to other techniques, our microfluidic experimental approach highlights the behavior of RBC aggregates in dynamic flow conditions relevant to microcirculation. Our results demonstrate that aggregate dissociation is strongly correlated with cell density and that aggregates formed from the denser fractions of RBCs are significantly more robust than those from the average cell population. This study provides insight into the effect of RBC aging in vivo on their mechanical properties and aggregability, underscoring the importance of further exploration of RBC aggregation in the context of cellular senescence and its potential implications for hemodynamics. Additionally, it suggests that this technique can complement existing methods for improved evaluation of RBC aggregability in health and disease.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00703">Bridging-induced Aggregation in Neutral Polymers: Dynamics and Morphologies</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Hitesh Garg, Satyavani Vemparala</div>
        <div class="preprint-abstract">arXiv:2504.00703v1 Announce Type: cross 
Abstract: Using molecular dynamics simulations, we investigate the aggregation behavior of neutral stiff (rod-like) and flexible polymer chains mediated by attractive crowders. Attractive crowders serve as bridging agents, inducing aggregation through effective intra-polymer attractions. The critical monomer-crowder attraction strength ($\epsilon_{mc}^*$) required for aggregation differs notably between rigid rods and flexible polymers. Interestingly, this aggregation threshold closely matches the critical attraction required for the extended-to-collapsed (coil-globule) transition of a single flexible polymer chain, suggesting a fundamental connection between single-chain collapse and multi-chain aggregation. Furthermore, we demonstrate that $\epsilon_{mc}^*$ decreases with increasing system density and larger crowder sizes, highlighting the synergistic roles of crowding effects and crowder dimensions. Aggregate morphologies exhibit strong dependence on polymer flexibility: rigid rods predominantly form elongated cylindrical bundles, whereas flexible polymers aggregate into compact spherical clusters. These findings provide comprehensive insights into how bridging interactions driven by attractive crowders regulate polymer aggregation dynamics and morphologies, emphasizing the importance of polymer rigidity, crowder size, and system density.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.23536">A Survey on Unlearnable Data</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jiahao Li, Yiqiang Chen, Yunbing Xing, Yang Gu, Xiangyuan Lan</div>
        <div class="preprint-abstract">arXiv:2503.23536v2 Announce Type: replace 
Abstract: Unlearnable data (ULD) has emerged as an innovative defense technique to prevent machine learning models from learning meaningful patterns from specific data, thus protecting data privacy and security. By introducing perturbations to the training data, ULD degrades model performance, making it difficult for unauthorized models to extract useful representations. Despite the growing significance of ULD, existing surveys predominantly focus on related fields, such as adversarial attacks and machine unlearning, with little attention given to ULD as an independent area of study. This survey fills that gap by offering a comprehensive review of ULD, examining unlearnable data generation methods, public benchmarks, evaluation metrics, theoretical foundations and practical applications. We compare and contrast different ULD approaches, analyzing their strengths, limitations, and trade-offs related to unlearnability, imperceptibility, efficiency and robustness. Moreover, we discuss key challenges, such as balancing perturbation imperceptibility with model degradation and the computational complexity of ULD generation. Finally, we highlight promising future research directions to advance the effectiveness and applicability of ULD, underscoring its potential to become a crucial tool in the evolving landscape of data protection in machine learning.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00564">Geometric Median Matching for Robust k-Subset Selection from Noisy Data</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Anish Acharya, Sujay Sanghavi, Alexandros G Dimakis, Inderjit S Dhillon</div>
        <div class="preprint-abstract">arXiv:2504.00564v1 Announce Type: new 
Abstract: Data pruning -- the combinatorial task of selecting a small and representative subset from a large dataset, is crucial for mitigating the enormous computational costs associated with training data-hungry modern deep learning models at scale. Since large scale data collections are invariably noisy, developing data pruning strategies that remain robust even in the presence of corruption is critical in practice. However, existing data pruning methods often fail under high corruption rates due to their reliance on empirical mean estimation, which is highly sensitive to outliers.
  In response, we propose Geometric Median (GM) Matching, a novel k-subset selection strategy that leverages Geometric Median -- a robust estimator with an optimal breakdown point of 1/2; to enhance resilience against noisy data. Our method iteratively selects a k-subset such that the mean of the subset approximates the GM of the (potentially) noisy dataset, ensuring robustness even under arbitrary corruption. We provide theoretical guarantees, showing that GM Matching enjoys an improved O(1/k) convergence rate -- a quadratic improvement over random sampling, even under arbitrary corruption. Extensive experiments across image classification and image generation tasks demonstrate that GM Matching consistently outperforms existing pruning approaches, particularly in high-corruption settings and at high pruning rates; making it a strong baseline for robust data pruning.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00388">Using complex prompts to identify fine-grained biases in image generation through ChatGPT-4o</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Marinus Ferreira</div>
        <div class="preprint-abstract">arXiv:2504.00388v1 Announce Type: cross 
Abstract: There are not one but two dimensions of bias that can be revealed through the study of large AI models: not only bias in training data or the products of an AI, but also bias in society, such as disparity in employment or health outcomes between different demographic groups. Often training data and AI output is biased for or against certain demographics (i.e. older white people are overrepresented in image datasets), but sometimes large AI models accurately illustrate biases in the real world (i.e. young black men being disproportionately viewed as threatening). These social disparities often appear in image generation AI outputs in the form of 'marked' features, where some feature of an individual or setting is a social marker of disparity, and prompts both humans and AI systems to treat subjects that are marked in this way as exceptional and requiring special treatment. Generative AI has proven to be very sensitive to such marked features, to the extent of over-emphasising them and thus often exacerbating social biases. I briefly discuss how we can use complex prompts to image generation AI to investigate either dimension of bias, emphasising how we can probe the large language models underlying image generation AI through, for example, automated sentiment analysis of the text prompts used to generate images.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2410.02116">Dataset Distillation via Knowledge Distillation: Towards Efficient Self-Supervised Pre-Training of Deep Networks</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Siddharth Joshi, Jiayi Ni, Baharan Mirzasoleiman</div>
        <div class="preprint-abstract">arXiv:2410.02116v3 Announce Type: replace 
Abstract: Dataset distillation (DD) generates small synthetic datasets that can efficiently train deep networks with a limited amount of memory and compute. Despite the success of DD methods for supervised learning, DD for self-supervised pre-training of deep models has remained unaddressed. Pre-training on unlabeled data is crucial for efficiently generalizing to downstream tasks with limited labeled data. In this work, we propose the first effective DD method for SSL pre-training. First, we show, theoretically and empirically, that naive application of supervised DD methods to SSL fails, due to the high variance of the SSL gradient. Then, we address this issue by relying on insights from knowledge distillation (KD) literature. Specifically, we train a small student model to match the representations of a larger teacher model trained with SSL. Then, we generate a small synthetic dataset by matching the training trajectories of the student models. As the KD objective has considerably lower variance than SSL, our approach can generate synthetic datasets that can successfully pre-train high-quality encoders. Through extensive experiments, we show that our distilled sets lead to up to 13% higher accuracy than prior work, on a variety of downstream tasks, in the presence of limited labeled data. Code at https://github.com/BigML-CS-UCLA/MKDT.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00194">Identifying Sparsely Active Circuits Through Local Loss Landscape Decomposition</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Brianna Chrisman, Lucius Bushnaq, Lee Sharkey</div>
        <div class="preprint-abstract">arXiv:2504.00194v1 Announce Type: new 
Abstract: Much of mechanistic interpretability has focused on understanding the activation spaces of large neural networks. However, activation space-based approaches reveal little about the underlying circuitry used to compute features. To better understand the circuits employed by models, we introduce a new decomposition method called Local Loss Landscape Decomposition (L3D). L3D identifies a set of low-rank subnetworks: directions in parameter space of which a subset can reconstruct the gradient of the loss between any sample's output and a reference output vector. We design a series of progressively more challenging toy models with well-defined subnetworks and show that L3D can nearly perfectly recover the associated subnetworks. Additionally, we investigate the extent to which perturbing the model in the direction of a given subnetwork affects only the relevant subset of samples. Finally, we apply L3D to a real-world transformer model and a convolutional neural network, demonstrating its potential to identify interpretable and relevant circuits in parameter space.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00220">Can Diffusion Models Disentangle? A Theoretical Perspective</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Liming Wang, Muhammad Jehanzeb Mirza, Yishu Gong, Yuan Gong, Jiaqi Zhang, Brian H. Tracey, Katerina Placek, Marco Vilela, James R. Glass</div>
        <div class="preprint-abstract">arXiv:2504.00220v1 Announce Type: new 
Abstract: This paper presents a novel theoretical framework for understanding how diffusion models can learn disentangled representations. Within this framework, we establish identifiability conditions for general disentangled latent variable models, analyze training dynamics, and derive sample complexity bounds for disentangled latent subspace models. To validate our theory, we conduct disentanglement experiments across diverse tasks and modalities, including subspace recovery in latent subspace Gaussian mixture models, image colorization, image denoising, and voice conversion for speech classification. Additionally, our experiments show that training strategies inspired by our theory, such as style guidance regularization, consistently enhance disentanglement performance.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2410.18293">1-2-3-Go! Policy Synthesis for Parameterized Markov Decision Processes via Decision-Tree Learning and Generalization</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Muqsit Azeem, Debraj Chakraborty, Sudeep Kanav, Jan Kretinsky, Mohammadsadegh Mohagheghi, Stefanie Mohr, Maximilian Weininger</div>
        <div class="preprint-abstract">arXiv:2410.18293v2 Announce Type: replace 
Abstract: Despite the advances in probabilistic model checking, the scalability of the verification methods remains limited. In particular, the state space often becomes extremely large when instantiating parameterized Markov decision processes (MDPs) even with moderate values. Synthesizing policies for such \emph{huge} MDPs is beyond the reach of available tools. We propose a learning-based approach to obtain a reasonable policy for such huge MDPs.
  The idea is to generalize optimal policies obtained by model-checking small instances to larger ones using decision-tree learning. Consequently, our method bypasses the need for explicit state-space exploration of large models, providing a practical solution to the state-space explosion problem. We demonstrate the efficacy of our approach by performing extensive experimentation on the relevant models from the quantitative verification benchmark set. The experimental results indicate that our policies perform well, even when the size of the model is orders of magnitude beyond the reach of state-of-the-art analysis tools.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00520">SCRec: A Scalable Computational Storage System with Statistical Sharding and Tensor-train Decomposition for Recommendation Models</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jinho Yang, Ji-Hoon Kim, Joo-Young Kim</div>
        <div class="preprint-abstract">arXiv:2504.00520v1 Announce Type: cross 
Abstract: Deep Learning Recommendation Models (DLRMs) play a crucial role in delivering personalized content across web applications such as social networking and video streaming. However, with improvements in performance, the parameter size of DLRMs has grown to terabyte (TB) scales, accompanied by memory bandwidth demands exceeding TB/s levels. Furthermore, the workload intensity within the model varies based on the target mechanism, making it difficult to build an optimized recommendation system. In this paper, we propose SCRec, a scalable computational storage recommendation system that can handle TB-scale industrial DLRMs while guaranteeing high bandwidth requirements. SCRec utilizes a software framework that features a mixed-integer programming (MIP)-based cost model, efficiently fetching data based on data access patterns and adaptively configuring memory-centric and compute-centric cores. Additionally, SCRec integrates hardware acceleration cores to enhance DLRM computations, particularly allowing for the high-performance reconstruction of approximated embedding vectors from extremely compressed tensor-train (TT) format. By combining its software framework and hardware accelerators, while eliminating data communication overhead by being implemented on a single server, SCRec achieves substantial improvements in DLRM inference performance. It delivers up to 55.77$\times$ speedup compared to a CPU-DRAM system with no loss in accuracy and up to 13.35$\times$ energy efficiency gains over a multi-GPU system.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2403.15426">CodingTeachLLM: Empowering LLM's Coding Ability via AST Prior Knowledge</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Zhangquan Chen, Chunjiang Liu, Haobin Duan</div>
        <div class="preprint-abstract">arXiv:2403.15426v2 Announce Type: replace 
Abstract: In this paper, we introduce CodingTeachLLM, a large language model (LLM) designed for coding teaching. Specially, we aim to enhance the coding ability of LLM and lead it to better teaching mode in education context. Thus, we propose an end-to-end prior-based three-phases supervised fine-tuned model, which is proved more competitive than traditional fine-tuning method. More specifically, our model realizes the structural disassembly and incremental guided output of educational knowledge. To this end, we robustify data classification of three types via a sampler and overlap estimation neural network, and inject the preprocessing datasets into pre-trained model in three batches for LORA fine-tuning. Then, we design a prior module couples system prompt, vector databases, and abstract syntax tree task segmentation. Finally, the compression method and regularization constraint are applied to the prior-based fine-tuned model, followed by text filter at the output end to obtain incremental guided results. Our model represents the first research effort to truly embody the tutor role with the features of abundant educational knowledge, step-by-step incremental guided outputs and non-disclosure of answers. Extensive experiments report that our model also achieves state-of-the-art in code abilities compared to open-source models, reaching an impressive 75.10% on the HumanEval (@pass 1) benchmark. Additionally, our model maintains strong conversational capabilities, with the 13B quantized version achieving scores of 56.34, 50.60, and 45.27 respectively on the MMLU, C-Eval, and AGIEval (5 shot) dialogue evaluation benchmarks.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00910">Provably accurate adaptive sampling for collocation points in physics-informed neural networks</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Antoine Caradot, R\'emi Emonet, Amaury Habrard, Abdel-Rahim Mezidi, Marc Sebban</div>
        <div class="preprint-abstract">arXiv:2504.00910v1 Announce Type: new 
Abstract: Despite considerable scientific advances in numerical simulation, efficiently solving PDEs remains a complex and often expensive problem. Physics-informed Neural Networks (PINN) have emerged as an efficient way to learn surrogate solvers by embedding the PDE in the loss function and minimizing its residuals using automatic differentiation at so-called collocation points. Originally uniformly sampled, the choice of the latter has been the subject of recent advances leading to adaptive sampling refinements for PINNs. In this paper, leveraging a new quadrature method for approximating definite integrals, we introduce a provably accurate sampling method for collocation points based on the Hessian of the PDE residuals. Comparative experiments conducted on a set of 1D and 2D PDEs demonstrate the benefits of our method.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00860">Investigating the Capabilities and Limitations of Machine Learning for Identifying Bias in English Language Data with Information and Heritage Professionals</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Lucy Havens, Benjamin Bach, Melissa Terras, Beatrice Alex</div>
        <div class="preprint-abstract">arXiv:2504.00860v1 Announce Type: cross 
Abstract: Despite numerous efforts to mitigate their biases, ML systems continue to harm already-marginalized people. While predominant ML approaches assume bias can be removed and fair models can be created, we show that these are not always possible, nor desirable, goals. We reframe the problem of ML bias by creating models to identify biased language, drawing attention to a dataset's biases rather than trying to remove them. Then, through a workshop, we evaluated the models for a specific use case: workflows of information and heritage professionals. Our findings demonstrate the limitations of ML for identifying bias due to its contextual nature, the way in which approaches to mitigating it can simultaneously privilege and oppress different communities, and its inevitability. We demonstrate the need to expand ML approaches to bias and fairness, providing a mixed-methods approach to investigating the feasibility of removing bias or achieving fairness in a given ML use case.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2410.05506">Privacy Vulnerabilities in Marginals-based Synthetic Data</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Steven Golob, Sikha Pentyala, Anuar Maratkhan, Martine De Cock</div>
        <div class="preprint-abstract">arXiv:2410.05506v2 Announce Type: replace-cross 
Abstract: When acting as a privacy-enhancing technology, synthetic data generation (SDG) aims to maintain a resemblance to the real data while excluding personally-identifiable information. Many SDG algorithms provide robust differential privacy (DP) guarantees to this end. However, we show that the strongest class of SDG algorithms--those that preserve \textit{marginal probabilities}, or similar statistics, from the underlying data--leak information about individuals that can be recovered more efficiently than previously understood. We demonstrate this by presenting a novel membership inference attack, MAMA-MIA, and evaluate it against three seminal DP SDG algorithms: MST, PrivBayes, and Private-GSD. MAMA-MIA leverages knowledge of which SDG algorithm was used, allowing it to learn information about the hidden data more accurately, and orders-of-magnitude faster, than other leading attacks. We use MAMA-MIA to lend insight into existing SDG vulnerabilities. Our approach went on to win the first SNAKE (SaNitization Algorithm under attacK ... $\varepsilon$) competition.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2410.08025">The Computational Complexity of Circuit Discovery for Inner Interpretability</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Federico Adolfi, Martina G. Vilas, Todd Wareham</div>
        <div class="preprint-abstract">arXiv:2410.08025v3 Announce Type: replace 
Abstract: Many proposed applications of neural networks in machine learning, cognitive/brain science, and society hinge on the feasibility of inner interpretability via circuit discovery. This calls for empirical and theoretical explorations of viable algorithmic options. Despite advances in the design and testing of heuristics, there are concerns about their scalability and faithfulness at a time when we lack understanding of the complexity properties of the problems they are deployed to solve. To address this, we study circuit discovery with classical and parameterized computational complexity theory: (1) we describe a conceptual scaffolding to reason about circuit finding queries in terms of affordances for description, explanation, prediction and control; (2) we formalize a comprehensive set of queries for mechanistic explanation, and propose a formal framework for their analysis; (3) we use it to settle the complexity of many query variants and relaxations of practical interest on multi-layer perceptrons. Our findings reveal a challenging complexity landscape. Many queries are intractable, remain fixed-parameter intractable relative to model/circuit features, and inapproximable under additive, multiplicative, and probabilistic approximation schemes. To navigate this landscape, we prove there exist transformations to tackle some of these hard problems with better-understood heuristics, and prove the tractability or fixed-parameter tractability of more modest queries which retain useful affordances. This framework allows us to understand the scope and limits of interpretability queries, explore viable options, and compare their resource demands on existing and future architectures.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00969">HDVIO2.0: Wind and Disturbance Estimation with Hybrid Dynamics VIO</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Giovanni Cioffi, Leonard Bauersfeld, Davide Scaramuzza</div>
        <div class="preprint-abstract">arXiv:2504.00969v1 Announce Type: cross 
Abstract: Visual-inertial odometry (VIO) is widely used for state estimation in autonomous micro aerial vehicles using onboard sensors. Current methods improve VIO by incorporating a model of the translational vehicle dynamics, yet their performance degrades when faced with low-accuracy vehicle models or continuous external disturbances, like wind. Additionally, incorporating rotational dynamics in these models is computationally intractable when they are deployed in online applications, e.g., in a closed-loop control system. We present HDVIO2.0, which models full 6-DoF, translational and rotational, vehicle dynamics and tightly incorporates them into a VIO with minimal impact on the runtime. HDVIO2.0 builds upon the previous work, HDVIO, and addresses these challenges through a hybrid dynamics model combining a point-mass vehicle model with a learning-based component, with access to control commands and IMU history, to capture complex aerodynamic effects. The key idea behind modeling the rotational dynamics is to represent them with continuous-time functions. HDVIO2.0 leverages the divergence between the actual motion and the predicted motion from the hybrid dynamics model to estimate external forces as well as the robot state. Our system surpasses the performance of state-of-the-art methods in experiments using public and new drone dynamics datasets, as well as real-world flights in winds up to 25 km/h. Unlike existing approaches, we also show that accurate vehicle dynamics predictions are achievable without precise knowledge of the full vehicle state.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2502.12063">Low-Rank Thinning</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Annabelle Michael Carrell, Albert Gong, Abhishek Shetty, Raaz Dwivedi, Lester Mackey</div>
        <div class="preprint-abstract">arXiv:2502.12063v3 Announce Type: replace-cross 
Abstract: The goal in thinning is to summarize a dataset using a small set of representative points. Remarkably, sub-Gaussian thinning algorithms like Kernel Halving and Compress can match the quality of uniform subsampling while substantially reducing the number of summary points. However, existing guarantees cover only a restricted range of distributions and kernel-based quality measures and suffer from pessimistic dimension dependence. To address these deficiencies, we introduce a new low-rank analysis of sub-Gaussian thinning that applies to any distribution and any kernel, guaranteeing high-quality compression whenever the kernel or data matrix is approximately low-rank. To demonstrate the broad applicability of the techniques, we design practical sub-Gaussian thinning approaches that improve upon the best known guarantees for approximating attention in transformers, accelerating stochastic gradient training through reordering, and distinguishing distributions in near-linear time.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2412.08878">Multi-objective Combinatorial Methodology for Nuclear Reactor Site Assessment: A Case Study for the United States</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Omer Erdem, Kevin Daley, Gabrielle Hoelzle, Majdi I. Radaideh</div>
        <div class="preprint-abstract">arXiv:2412.08878v2 Announce Type: replace-cross 
Abstract: As clean energy demand grows to meet sustainability and net-zero goals, nuclear energy emerges as a reliable option. However, high capital costs remain a challenge for nuclear power plants (NPP), where repurposing coal power plant sites (CPP) with existing infrastructure is one way to reduce these costs. Additionally, Brownfield sites-previously developed or underutilized lands often impacted by industrial activity-present another compelling alternative. This study introduces a novel multi-objective optimization methodology, leveraging combinatorial search to evaluate over 30,000 potential NPP sites in the United States. Our approach addresses gaps in the current practice of assigning pre-determined weights to each site attribute that could lead to bias in the ranking. Each site is assigned a performance-based score, derived from a detailed combinatorial analysis of its site attributes. The methodology generates a comprehensive database comprising site locations (inputs), attributes (outputs), site score (outputs), and the contribution of each attribute to the site score. We then use this database to train a neural network model, enabling rapid predictions of nuclear siting suitability across any location in the United States. Our findings highlight that CPP sites are highly competitive for nuclear development, but some Brownfield sites are able to compete with them. Notably, four CPP sites in Ohio, North Carolina, and New Hampshire, and two Brownfield sites in Florida and California rank among the most promising locations. These results underscore the potential of integrating machine learning and optimization techniques to transform nuclear siting, paving the way for a cost-effective and sustainable energy future.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.09445">Astrea: A MOE-based Visual Understanding Model with Progressive Alignment</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Xiaoda Yang, JunYu Lu, Hongshun Qiu, Sijing Li, Hao Li, Shengpeng Ji, Xudong Tang, Jiayang Xu, Jiaqi Duan, Ziyue Jiang, Cong Lin, Sihang Cai, Zejian Xie, Zhuoyang Song, Songxin Zhang</div>
        <div class="preprint-abstract">arXiv:2503.09445v2 Announce Type: replace-cross 
Abstract: Vision-Language Models (VLMs) based on Mixture-of-Experts (MoE) architectures have emerged as a pivotal paradigm in multimodal understanding, offering a powerful framework for integrating visual and linguistic information. However, the increasing complexity and diversity of tasks present significant challenges in coordinating load balancing across heterogeneous visual experts, where optimizing one specialist's performance often compromises others' capabilities. To address task heterogeneity and expert load imbalance, we propose Astrea, a novel multi-expert collaborative VLM architecture based on progressive pre-alignment. Astrea introduces three key innovations: 1) A heterogeneous expert coordination mechanism that integrates four specialized models (detection, segmentation, classification, captioning) into a comprehensive expert matrix covering essential visual comprehension elements; 2) A dynamic knowledge fusion strategy featuring progressive pre-alignment to harmonize experts within the VLM latent space through contrastive learning, complemented by probabilistically activated stochastic residual connections to preserve knowledge continuity; 3) An enhanced optimization framework utilizing momentum contrastive learning for long-range dependency modeling and adaptive weight allocators for real-time expert contribution calibration. Extensive evaluations across 12 benchmark tasks spanning VQA, image captioning, and cross-modal retrieval demonstrate Astrea's superiority over state-of-the-art models, achieving an average performance gain of +4.7\%. This study provides the first empirical demonstration that progressive pre-alignment strategies enable VLMs to overcome task heterogeneity limitations, establishing new methodological foundations for developing general-purpose multimodal agents.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2406.09353">Enhancing Domain Adaptation through Prompt Gradient Alignment</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Hoang Phan, Lam Tran, Quyen Tran, Trung Le</div>
        <div class="preprint-abstract">arXiv:2406.09353v3 Announce Type: replace 
Abstract: Prior Unsupervised Domain Adaptation (UDA) methods often aim to train a domain-invariant feature extractor, which may hinder the model from learning sufficiently discriminative features. To tackle this, a line of works based on prompt learning leverages the power of large-scale pre-trained vision-language models to learn both domain-invariant and specific features through a set of domain-agnostic and domain-specific learnable prompts. Those studies typically enforce invariant constraints on representation, output, or prompt space to learn such prompts. In contrast, we cast UDA as a multiple-objective optimization problem in which each objective is represented by a domain loss. Under this new framework, we propose to align per-objective gradients to foster consensus between them. Additionally, to prevent potential overfitting when fine-tuning this deep learning architecture, we penalize the norm of these gradients. To achieve these goals, we devise a practical gradient update procedure that can work under both single-source and multi-source UDA. Empirically, our method consistently outperforms other vision-language model adaptation methods. The implementation is available at https://github.com/VietHoang1512/PGA.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00615">Towards Responsible and Trustworthy Educational Data Mining: Comparing Symbolic, Sub-Symbolic, and Neural-Symbolic AI Methods</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Danial Hooshyar, Eve Kikas, Yeongwook Yang, Gustav \v{S}\'ir, Raija H\"am\"al\"ainen, Tommi K\"arkk\"ainen, Roger Azevedo</div>
        <div class="preprint-abstract">arXiv:2504.00615v1 Announce Type: new 
Abstract: Given the demand for responsible and trustworthy AI for education, this study evaluates symbolic, sub-symbolic, and neural-symbolic AI (NSAI) in terms of generalizability and interpretability. Our extensive experiments on balanced and imbalanced self-regulated learning datasets of Estonian primary school students predicting 7th-grade mathematics national test performance showed that symbolic and sub-symbolic methods performed well on balanced data but struggled to identify low performers in imbalanced datasets. Interestingly, symbolic and sub-symbolic methods emphasized different factors in their decision-making: symbolic approaches primarily relied on cognitive and motivational factors, while sub-symbolic methods focused more on cognitive aspects, learned knowledge, and the demographic variable of gender -- yet both largely overlooked metacognitive factors. The NSAI method, on the other hand, showed advantages by: (i) being more generalizable across both classes -- even in imbalanced datasets -- as its symbolic knowledge component compensated for the underrepresented class; and (ii) relying on a more integrated set of factors in its decision-making, including motivation, (meta)cognition, and learned knowledge, thus offering a comprehensive and theoretically grounded interpretability framework. These contrasting findings highlight the need for a holistic comparison of AI methods before drawing conclusions based solely on predictive performance. They also underscore the potential of hybrid, human-centered NSAI methods to address the limitations of other AI families and move us closer to responsible AI for education. Specifically, by enabling stakeholders to contribute to AI design, NSAI aligns learned patterns with theoretical constructs, incorporates factors like motivation and metacognition, and strengthens the trustworthiness and responsibility of educational data mining.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00247">MultiMorph: On-demand Atlas Construction</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">S. Mazdak Abulnaga, Andrew Hoopes, Neel Dey, Malte Hoffmann, Marianne Rakic, Bruce Fischl, John Guttag, Adrian Dalca</div>
        <div class="preprint-abstract">arXiv:2504.00247v1 Announce Type: cross 
Abstract: We present MultiMorph, a fast and efficient method for constructing anatomical atlases on the fly. Atlases capture the canonical structure of a collection of images and are essential for quantifying anatomical variability across populations. However, current atlas construction methods often require days to weeks of computation, thereby discouraging rapid experimentation. As a result, many scientific studies rely on suboptimal, precomputed atlases from mismatched populations, negatively impacting downstream analyses. MultiMorph addresses these challenges with a feedforward model that rapidly produces high-quality, population-specific atlases in a single forward pass for any 3D brain dataset, without any fine-tuning or optimization. MultiMorph is based on a linear group-interaction layer that aggregates and shares features within the group of input images. Further, by leveraging auxiliary synthetic data, MultiMorph generalizes to new imaging modalities and population groups at test-time. Experimentally, MultiMorph outperforms state-of-the-art optimization-based and learning-based atlas construction methods in both small and large population settings, with a 100-fold reduction in time. This makes MultiMorph an accessible framework for biomedical researchers without machine learning expertise, enabling rapid, high-quality atlas generation for diverse studies.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00002">Are We There Yet? A Measurement Study of Efficiency for LLM Applications on Mobile Devices</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Xiao Yan, Yi Ding</div>
        <div class="preprint-abstract">arXiv:2504.00002v1 Announce Type: cross 
Abstract: Recent advancements in large language models (LLMs) have prompted interest in deploying these models on mobile devices to enable new applications without relying on cloud connectivity. However, the efficiency constraints of deploying LLMs on resource-limited devices present significant challenges. In this paper, we conduct a comprehensive measurement study to evaluate the efficiency tradeoffs between mobile-based, edge-based, and cloud-based deployments for LLM applications. We implement AutoLife-Lite, a simplified LLM-based application that analyzes smartphone sensor data to infer user location and activity contexts. Our experiments reveal that: (1) Only small-size LLMs (<4B parameters) can run successfully on powerful mobile devices, though they exhibit quality limitations compared to larger models; (2) Model compression is effective in lower the hardware requirement, but may lead to significant performance degradation; (3) The latency to run LLMs on mobile devices with meaningful output is significant (>30 seconds), while cloud services demonstrate better time efficiency (<10 seconds); (4) Edge deployments offer intermediate tradeoffs between latency and model capabilities, with different results on CPU-based and GPU-based settings. These findings provide valuable insights for system designers on the current limitations and future directions for on-device LLM applications.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00837">A Survey on Music Generation from Single-Modal, Cross-Modal, and Multi-Modal Perspectives: Data, Methods, and Challenges</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Shuyu Li, Shulei Ji, Zihao Wang, Songruoyao Wu, Jiaxing Yu, Kejun Zhang</div>
        <div class="preprint-abstract">arXiv:2504.00837v1 Announce Type: cross 
Abstract: Multi-modal music generation, using multiple modalities like images, video, and text alongside musical scores and audio as guidance, is an emerging research area with broad applications. This paper reviews this field, categorizing music generation systems from the perspective of modalities. It covers modality representation, multi-modal data alignment, and their utilization to guide music generation. We also discuss current datasets and evaluation methods. Key challenges in this area include effective multi-modal integration, large-scale comprehensive datasets, and systematic evaluation methods. Finally, we provide an outlook on future research directions focusing on multi-modal fusion, alignment, data, and evaluation.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00843">Investigating Large Language Models in Diagnosing Students' Cognitive Skills in Math Problem-solving</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Hyoungwook Jin, Yoonsu Kim, Dongyun Jung, Seungju Kim, Kiyoon Choi, Jinho Son, Juho Kim</div>
        <div class="preprint-abstract">arXiv:2504.00843v1 Announce Type: new 
Abstract: Mathematics learning entails mastery of both content knowledge and cognitive processing of knowing, applying, and reasoning with it. Automated math assessment primarily has focused on grading students' exhibition of content knowledge by finding textual evidence, such as specific numbers, formulas, and statements. Recent advancements in problem-solving, image recognition, and reasoning capabilities of large language models (LLMs) show promise for nuanced evaluation of students' cognitive skills. Diagnosing cognitive skills needs to infer students' thinking processes beyond textual evidence, which is an underexplored task in LLM-based automated assessment. In this work, we investigate how state-of-the-art LLMs diagnose students' cognitive skills in mathematics. We constructed MathCog, a novel benchmark dataset comprising 639 student responses to 110 expert-curated middle school math problems, each annotated with detailed teachers' diagnoses based on cognitive skill checklists. Using MathCog, we evaluated 16 closed and open LLMs of varying model sizes and vendors. Our evaluation reveals that even the state-of-the-art LLMs struggle with the task, all F1 scores below 0.5, and tend to exhibit strong false confidence for incorrect cases ($r_s=.617$). We also found that model size positively correlates with the diagnosis performance ($r_s=.771$). Finally, we discuss the implications of these findings, the overconfidence issue, and directions for improving automated cognitive skill diagnosis.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.01016">GeometryCrafter: Consistent Geometry Estimation for Open-world Videos with Diffusion Priors</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Tian-Xing Xu, Xiangjun Gao, Wenbo Hu, Xiaoyu Li, Song-Hai Zhang, Ying Shan</div>
        <div class="preprint-abstract">arXiv:2504.01016v1 Announce Type: cross 
Abstract: Despite remarkable advancements in video depth estimation, existing methods exhibit inherent limitations in achieving geometric fidelity through the affine-invariant predictions, limiting their applicability in reconstruction and other metrically grounded downstream tasks. We propose GeometryCrafter, a novel framework that recovers high-fidelity point map sequences with temporal coherence from open-world videos, enabling accurate 3D/4D reconstruction, camera parameter estimation, and other depth-based applications. At the core of our approach lies a point map Variational Autoencoder (VAE) that learns a latent space agnostic to video latent distributions for effective point map encoding and decoding. Leveraging the VAE, we train a video diffusion model to model the distribution of point map sequences conditioned on the input videos. Extensive evaluations on diverse datasets demonstrate that GeometryCrafter achieves state-of-the-art 3D accuracy, temporal consistency, and generalization capability.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00881">Detection of Anomalous Vehicular Traffic and Sensor Failures Using Data Clustering Techniques</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Davide Moretti, Elia Onofri, Emiliano Cristiani</div>
        <div class="preprint-abstract">arXiv:2504.00881v1 Announce Type: new 
Abstract: The increasing availability of traffic data from sensor networks has created new opportunities for understanding vehicular dynamics and identifying anomalies. In this study, we employ clustering techniques to analyse traffic flow data with the dual objective of uncovering meaningful traffic patterns and detecting anomalies, including sensor failures and irregular congestion events.
  We explore multiple clustering approaches, i.e partitioning and hierarchical methods, combined with various time-series representations and similarity measures. Our methodology is applied to real-world data from highway sensors, enabling us to assess the impact of different clustering frameworks on traffic pattern recognition. We also introduce a clustering-driven anomaly detection methodology that identifies deviations from expected traffic behaviour based on distance-based anomaly scores.
  Results indicate that hierarchical clustering with symbolic representations provides robust segmentation of traffic patterns, while partitioning methods such as k-means and fuzzy c-means yield meaningful results when paired with Dynamic Time Warping. The proposed anomaly detection strategy successfully identifies sensor malfunctions and abnormal traffic conditions with minimal false positives, demonstrating its practical utility for real-time monitoring.
  Real-world vehicular traffic data are provided by Autostrade Alto Adriatico S.p.A.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00661">DynMoLE: Boosting Mixture of LoRA Experts Fine-Tuning with a Hybrid Routing Mechanism</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Dengchun Li, Naizheng Wang, Zihao Zhang, Haoyang Yin, Lei Duan, Meng Xiao, Mingjie Tang</div>
        <div class="preprint-abstract">arXiv:2504.00661v1 Announce Type: cross 
Abstract: Instruction-based fine-tuning of large language models (LLMs) has achieved remarkable success in various natural language processing (NLP) tasks. Parameter-efficient fine-tuning (PEFT) methods, such as Mixture of LoRA Experts (MoLE), combine the efficiency of Low-Rank Adaptation (LoRA) with the versatility of Mixture of Experts (MoE) models, demonstrating significant potential for handling multiple downstream tasks. However, the existing routing mechanisms for MoLE often involve a trade-off between computational efficiency and predictive accuracy, and they fail to fully address the diverse expert selection demands across different transformer layers. In this work, we propose DynMoLE, a hybrid routing strategy that dynamically adjusts expert selection based on the Tsallis entropy of the router's probability distribution. This approach mitigates router uncertainty, enhances stability, and promotes more equitable expert participation, leading to faster convergence and improved model performance. Additionally, we introduce an auxiliary loss based on Tsallis entropy to further guide the model toward convergence with reduced uncertainty, thereby improving training stability and performance. Our extensive experiments on commonsense reasoning benchmarks demonstrate that DynMoLE achieves substantial performance improvements, outperforming LoRA by 9.6% and surpassing the state-of-the-art MoLE method, MoLA, by 2.3%. We also conduct a comprehensive ablation study to evaluate the contributions of DynMoLE's key components.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00780">Digitally Supported Analysis of Spontaneous Speech (DigiSpon): Benchmarking NLP-Supported Language Sample Analysis of Swiss Children's Speech</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Anja Ryser, Yingqiang Gao, Sarah Ebling</div>
        <div class="preprint-abstract">arXiv:2504.00780v1 Announce Type: cross 
Abstract: Language sample analysis (LSA) is a process that complements standardized psychometric tests for diagnosing, for example, developmental language disorder (DLD) in children. However, its labor-intensive nature has limited its use in speech-language pathology practice. We introduce an approach that leverages natural language processing (NLP) methods not based on commercial large language models (LLMs) applied to transcribed speech data from 119 children in the German speaking part of Switzerland with typical and atypical language development. The study aims to identify optimal practices that support speech-language pathologists in diagnosing DLD more efficiently within a human-in-the-loop framework, without relying on potentially unethical implementations that leverage commercial LLMs. Preliminary findings underscore the potential of integrating locally deployed NLP methods into the process of semi-automatic LSA.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2312.06275">DG-TTA: Out-of-domain Medical Image Segmentation through Augmentation and Descriptor-driven Domain Generalization and Test-Time Adaptation</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Christian Weihsbach, Christian N. Kruse, Alexander Bigalke, Mattias P. Heinrich</div>
        <div class="preprint-abstract">arXiv:2312.06275v4 Announce Type: replace-cross 
Abstract: Purpose: Applying pre-trained medical deep learning segmentation models on out-of-domain images often yields predictions of insufficient quality. In this study, we propose to use a powerful generalizing descriptor along with augmentation to enable domain-generalized pre-training and test-time adaptation, achieving high-quality segmentation in unseen domains.
  Materials and Methods: In this retrospective study five different publicly available datasets (2012 to 2022) including 3D CT and MRI images are used to evaluate segmentation performance in out-of-domain scenarios. The settings include abdominal, spine, and cardiac imaging. The data is randomly split into training and test samples. Domain-generalized pre-training on source data is used to obtain the best initial performance in the target domain. We introduce the combination of the generalizing SSC descriptor and GIN intensity augmentation for optimal generalization. Segmentation results are subsequently optimized at test time, where we propose to adapt the pre-trained models for every unseen scan with a consistency scheme using the same augmentation-descriptor combination. The segmentation is evaluated using Dice similarity and Hausdorff distance and the significance of improvements is tested with the Wilcoxon signed-rank test.
  Results: The proposed generalized pre-training and subsequent test-time adaptation improves model performance significantly in CT to MRI cross-domain prediction for abdominal (+46.2% and +28.2% Dice), spine (+72.9%), and cardiac (+14.2% and +55.7% Dice) scenarios (p<0.001).
  Conclusion: Our method enables optimal, independent usage of medical image source and target data and bridges domain gaps successfully with a compact and efficient methodology. Open-source code available at: https://github.com/multimodallearning/DG-TTA</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.23764">WaveFormer: A 3D Transformer with Wavelet-Driven Feature Representation for Efficient Medical Image Segmentation</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Md Mahfuz Al Hasan, Mahdi Zaman, Abdul Jawad, Alberto Santamaria-Pang, Ho Hin Lee, Ivan Tarapov, Kyle See, Md Shah Imran, Antika Roy, Yaser Pourmohammadi Fallah, Navid Asadizanjani, Reza Forghani</div>
        <div class="preprint-abstract">arXiv:2503.23764v2 Announce Type: replace-cross 
Abstract: Transformer-based architectures have advanced medical image analysis by effectively modeling long-range dependencies, yet they often struggle in 3D settings due to substantial memory overhead and insufficient capture of fine-grained local features. We address these limitations with WaveFormer, a novel 3D-transformer that: i) leverages the fundamental frequency-domain properties of features for contextual representation, and ii) is inspired by the top-down mechanism of the human visual recognition system, making it a biologically motivated architecture. By employing discrete wavelet transformations (DWT) at multiple scales, WaveFormer preserves both global context and high-frequency details while replacing heavy upsampling layers with efficient wavelet-based summarization and reconstruction. This significantly reduces the number of parameters, which is critical for real-world deployment where computational resources and training times are constrained. Furthermore, the model is generic and easily adaptable to diverse applications. Evaluations on BraTS2023, FLARE2021, and KiTS2023 demonstrate performance on par with state-of-the-art methods while offering substantially lower computational complexity.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2502.16119">FedORGP: Guiding Heterogeneous Federated Learning with Orthogonality Regularization on Global Prototypes</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Fucheng Guo, Zeyu Luan, Qing Li, Dan Zhao, Yong Jiang</div>
        <div class="preprint-abstract">arXiv:2502.16119v2 Announce Type: replace 
Abstract: Federated Learning (FL) has emerged as an essential framework for distributed machine learning, especially with its potential for privacy-preserving data processing. However, existing FL frameworks struggle to address statistical and model heterogeneity, which severely impacts model performance. While Heterogeneous Federated Learning (HtFL) introduces prototype-based strategies to address the challenges, current approaches face limitations in achieving optimal separation of prototypes. This paper presents FedORGP, a novel HtFL algorithm designed to improve global prototype separation through orthogonality regularization, which not only encourages intra-class prototype similarity but also significantly expands the inter-class angular separation. With the guidance of the global prototype, each client keeps its embeddings aligned with the corresponding prototype in the feature space, promoting directional independence that integrates seamlessly with the cross-entropy (CE) loss. We provide theoretical proof of FedORGP's convergence under non-convex conditions. Extensive experiments demonstrate that FedORGP outperforms seven state-of-the-art baselines, achieving up to 10.12\% accuracy improvement in scenarios where statistical and model heterogeneity coexist.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00038">Revisiting the Relationship between Adversarial and Clean Training: Why Clean Training Can Make Adversarial Training Better</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">MingWei Zhou, Xiaobing Pei</div>
        <div class="preprint-abstract">arXiv:2504.00038v1 Announce Type: new 
Abstract: Adversarial training (AT) is an effective technique for enhancing adversarial robustness, but it usually comes at the cost of a decline in generalization ability. Recent studies have attempted to use clean training to assist adversarial training, yet there are contradictions among the conclusions. We comprehensively summarize the representative strategies and, with a focus on the multi - view hypothesis, provide a unified explanation for the contradictory phenomena among different studies. In addition, we conduct an in - depth analysis of the knowledge combinations transferred from clean - trained models to adversarially - trained models in previous studies, and find that they can be divided into two categories: reducing the learning difficulty and providing correct guidance. Based on this finding, we propose a new idea of leveraging clean training to further improve the performance of advanced AT methods.We reveal that the problem of generalization degradation faced by AT partly stems from the difficulty of adversarial training in learning certain sample features, and this problem can be alleviated by making full use of clean training.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2405.02437">FastLloyd: Federated, Accurate, Secure, and Tunable $k$-Means Clustering with Differential Privacy</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Abdulrahman Diaa, Thomas Humphries, Florian Kerschbaum</div>
        <div class="preprint-abstract">arXiv:2405.02437v2 Announce Type: replace-cross 
Abstract: We study the problem of privacy-preserving $k$-means clustering in the horizontally federated setting. Existing federated approaches using secure computation suffer from substantial overheads and do not offer output privacy. At the same time, differentially private (DP) $k$-means algorithms either assume a trusted central curator or significantly degrade utility by adding noise in the local DP model. Naively combining the secure and central DP solutions results in a protocol with impractical overhead. Instead, our work provides enhancements to both the DP and secure computation components, resulting in a design that is faster, more private, and more accurate than previous work. By utilizing the computational DP model, we design a lightweight, secure aggregation-based approach that achieves five orders of magnitude speed-up over state-of-the-art related work. Furthermore, we not only maintain the utility of the state-of-the-art in the central model of DP, but we improve the utility further by designing a new DP clustering mechanism.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2502.03717">Efficiently Generating Expressive Quadruped Behaviors via Language-Guided Preference Learning</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jaden Clark, Joey Hejna, Dorsa Sadigh</div>
        <div class="preprint-abstract">arXiv:2502.03717v2 Announce Type: replace-cross 
Abstract: Expressive robotic behavior is essential for the widespread acceptance of robots in social environments. Recent advancements in learned legged locomotion controllers have enabled more dynamic and versatile robot behaviors. However, determining the optimal behavior for interactions with different users across varied scenarios remains a challenge. Current methods either rely on natural language input, which is efficient but low-resolution, or learn from human preferences, which, although high-resolution, is sample inefficient. This paper introduces a novel approach that leverages priors generated by pre-trained LLMs alongside the precision of preference learning. Our method, termed Language-Guided Preference Learning (LGPL), uses LLMs to generate initial behavior samples, which are then refined through preference-based feedback to learn behaviors that closely align with human expectations. Our core insight is that LLMs can guide the sampling process for preference learning, leading to a substantial improvement in sample efficiency. We demonstrate that LGPL can quickly learn accurate and expressive behaviors with as few as four queries, outperforming both purely language-parameterized models and traditional preference learning approaches. Website with videos: https://lgpl-gaits.github.io/</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00406">VerifiAgent: a Unified Verification Agent in Language Model Reasoning</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jiuzhou Han, Wray Buntine, Ehsan Shareghi</div>
        <div class="preprint-abstract">arXiv:2504.00406v1 Announce Type: cross 
Abstract: Large language models demonstrate remarkable reasoning capabilities but often produce unreliable or incorrect responses. Existing verification methods are typically model-specific or domain-restricted, requiring significant computational resources and lacking scalability across diverse reasoning tasks. To address these limitations, we propose VerifiAgent, a unified verification agent that integrates two levels of verification: meta-verification, which assesses completeness and consistency in model responses, and tool-based adaptive verification, where VerifiAgent autonomously selects appropriate verification tools based on the reasoning type, including mathematical, logical, or commonsense reasoning. This adaptive approach ensures both efficiency and robustness across different verification scenarios. Experimental results show that VerifiAgent outperforms baseline verification methods (e.g., deductive verifier, backward verifier) among all reasoning tasks. Additionally, it can further enhance reasoning accuracy by leveraging feedback from verification results. VerifiAgent can also be effectively applied to inference scaling, achieving better results with fewer generated samples and costs compared to existing process reward models in the mathematical reasoning domain. Code is available at https://github.com/Jiuzhouh/VerifiAgent</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2411.06559">Is Your LLM Secretly a World Model of the Internet? Model-Based Planning for Web Agents</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Yu Gu, Kai Zhang, Yuting Ning, Boyuan Zheng, Boyu Gou, Tianci Xue, Cheng Chang, Sanjari Srivastava, Yanan Xie, Peng Qi, Huan Sun, Yu Su</div>
        <div class="preprint-abstract">arXiv:2411.06559v2 Announce Type: replace 
Abstract: Language agents based on large language models (LLMs) have demonstrated great promise in automating web-based tasks. Recent work has shown that incorporating advanced planning algorithms, e.g., tree search, is advantageous over reactive planning for web agents. However, unlike simulated sandbox environments, real-world environments such as the web are rife with irreversible actions. This undermines the feasibility of backtracking, a cornerstone of (tree) search. Overly relying on test-time search also hurts efficiency. We advocate model-based planning for web agents that employs a world model to simulate and deliberate over the outcome of each candidate action before committing to one. We systematically explore this paradigm by (1) Proposing a model-based planning framework, WebDreamer, which employs LLMs to serve as both world models and value functions; (2) Training specialized LLMs as world models with a scalable data synthesis pipeline. Empirical results demonstrate that WebDreamer achieves substantial performance improvements over reactive baselines. It is competitive, while being 4-5 times more efficient, with tree search in sandbox environments (VisualWebArena) and also works effectively on real-world websites (Online-Mind2Web and Mind2Web-Live). Furthermore, our trained world model, Dreamer-7B, performs comparable to GPT-4o, highlighting the potential of specialized world models for efficient and effective planning in complex web environments.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.13208">Improving Complex Reasoning with Dynamic Prompt Corruption: A soft prompt Optimization Approach</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Sinan Fan, Liang Xie, Chen Shen, Ge Teng, Xiaosong Yuan, Xiaofeng Zhang, Chenxi Huang, Wenxiao Wang, Xiaofei He, Jieping Ye</div>
        <div class="preprint-abstract">arXiv:2503.13208v2 Announce Type: replace-cross 
Abstract: Prompt-tuning (PT) for large language models (LLMs) can facilitate the performance on various conventional NLP tasks with significantly fewer trainable parameters. However, our investigation reveals that PT provides limited improvement and may even degrade the primitive performance of LLMs on complex reasoning tasks. Such a phenomenon suggests that soft prompts can positively impact certain instances while negatively affecting others, particularly during the later phases of reasoning. To address these challenges, We first identify an information accumulation within the soft prompts. Through detailed analysis, we demonstrate that this phenomenon is often accompanied by erroneous information flow patterns in the deeper layers of the model, which ultimately lead to incorrect reasoning outcomes. we propose a novel method called Dynamic Prompt Corruption (DPC) to take better advantage of soft prompts in complex reasoning tasks, which dynamically adjusts the influence of soft prompts based on their impact on the reasoning process. Specifically, DPC consists of two stages: Dynamic Trigger and Dynamic Corruption. First, Dynamic Trigger measures the impact of soft prompts, identifying whether beneficial or detrimental. Then, Dynamic Corruption mitigates the negative effects of soft prompts by selectively masking key tokens that interfere with the reasoning process. We validate the proposed approach through extensive experiments on various LLMs and reasoning tasks, including GSM8K, MATH, and AQuA. Experimental results demonstrate that DPC can consistently enhance the performance of PT, achieving 4%-8% accuracy gains compared to vanilla prompt tuning, highlighting the effectiveness of our approach and its potential to enhance complex reasoning in LLMs.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00819">Mixture-of-Experts for Distributed Edge Computing with Channel-Aware Gating Function</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Qiuchen Song, Shusen Jing, Shuai Zhang, Songyang Zhang, Chuan Huang</div>
        <div class="preprint-abstract">arXiv:2504.00819v1 Announce Type: new 
Abstract: In a distributed mixture-of-experts (MoE) system, a server collaborates with multiple specialized expert clients to perform inference. The server extracts features from input data and dynamically selects experts based on their areas of specialization to produce the final output. Although MoE models are widely valued for their flexibility and performance benefits, adapting distributed MoEs to operate effectively in wireless networks has remained unexplored. In this work, we introduce a novel channel-aware gating function for wireless distributed MoE, which incorporates channel conditions into the MoE gating mechanism. To train the channel-aware gating, we simulate various signal-to-noise ratios (SNRs) for each expert's communication channel and add noise to the features distributed to the experts based on these SNRs. The gating function then utilizes both features and SNRs to optimize expert selection. Unlike conventional MoE models which solely consider the alignment of features with the specializations of experts, our approach additionally considers the impact of channel conditions on expert performance. Experimental results demonstrate that the proposed channel-aware gating scheme outperforms traditional MoE models.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2410.09437">MTL-LoRA: Low-Rank Adaptation for Multi-Task Learning</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Yaming Yang, Dilxat Muhtar, Yelong Shen, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Denvy Deng, Feng Sun, Qi Zhang, Weizhu Chen, Yunhai Tong</div>
        <div class="preprint-abstract">arXiv:2410.09437v3 Announce Type: replace 
Abstract: Parameter-efficient fine-tuning (PEFT) has been widely employed for domain adaptation, with LoRA being one of the most prominent methods due to its simplicity and effectiveness. However, in multi-task learning (MTL) scenarios, LoRA tends to obscure the distinction between tasks by projecting sparse high-dimensional features from different tasks into the same dense low-dimensional intrinsic space. This leads to task interference and suboptimal performance for LoRA and its variants. To tackle this challenge, we propose MTL-LoRA, which retains the advantages of low-rank adaptation while significantly enhancing MTL capabilities. MTL-LoRA augments LoRA by incorporating additional task-adaptive parameters that differentiate task-specific information and capture shared knowledge across various tasks within low-dimensional spaces. This approach enables pre-trained models to jointly adapt to different target domains with a limited number of trainable parameters. Comprehensive experimental results, including evaluations on public academic benchmarks for natural language understanding, commonsense reasoning, and image-text understanding, as well as real-world industrial text Ads relevance datasets, demonstrate that MTL-LoRA outperforms LoRA and its various variants with comparable or even fewer learnable parameters in MTL setting.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00986">Accelerating drug discovery with Artificial: a whole-lab orchestration and scheduling system for self-driving labs</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Yao Fehlis, Paul Mandel, Charles Crain, Betty Liu, David Fuller</div>
        <div class="preprint-abstract">arXiv:2504.00986v1 Announce Type: cross 
Abstract: Self-driving labs are transforming drug discovery by enabling automated, AI-guided experimentation, but they face challenges in orchestrating complex workflows, integrating diverse instruments and AI models, and managing data efficiently. Artificial addresses these issues with a comprehensive orchestration and scheduling system that unifies lab operations, automates workflows, and integrates AI-driven decision-making. By incorporating AI/ML models like NVIDIA BioNeMo - which facilitates molecular interaction prediction and biomolecular analysis - Artificial enhances drug discovery and accelerates data-driven research. Through real-time coordination of instruments, robots, and personnel, the platform streamlines experiments, enhances reproducibility, and advances drug discovery.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2406.15877">BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, Thong Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman Jain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, Binyuan Hui, Niklas Muennighoff, David Lo, Daniel Fried, Xiaoning Du, Harm de Vries, Leandro Von Werra</div>
        <div class="preprint-abstract">arXiv:2406.15877v4 Announce Type: replace-cross 
Abstract: Task automation has been greatly empowered by the recent advances in Large Language Models (LLMs) via Python code, where the tasks ranging from software engineering development to general-purpose reasoning. While current benchmarks have shown that LLMs can solve tasks using programs like human developers, the majority of their evaluations are limited to short and self-contained algorithmic tasks or standalone function calls. Solving challenging and practical tasks requires the capability of utilizing diverse function calls as tools to efficiently implement functionalities like data analysis and web development. In addition, using multiple tools to solve a task needs compositional reasoning by accurately understanding complex instructions. Fulfilling both of these characteristics can pose a great challenge for LLMs.To assess how well LLMs can solve challenging and practical tasks via programs, we introduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained tasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with an average branch coverage of 99%. In addition, we propose a natural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that automatically transforms the original docstrings into short instructions only with essential information. Our extensive evaluation of 60 LLMs shows that LLMs are not yet capable of following complex instructions to use function calls precisely, with scores up to 60%, significantly lower than the human performance of 97%. The results underscore the need for further advancements in this area.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00401">Beyond Wide-Angle Images: Unsupervised Video Portrait Correction via Spatiotemporal Diffusion Adaptation</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Wenbo Nie, Lang Nie, Chunyu Lin, Jingwen Chen, Ke Xing, Jiyuan Wang, Yao Zhao</div>
        <div class="preprint-abstract">arXiv:2504.00401v1 Announce Type: cross 
Abstract: Wide-angle cameras, despite their popularity for content creation, suffer from distortion-induced facial stretching-especially at the edge of the lens-which degrades visual appeal. To address this issue, we propose an image portrait correction framework using diffusion models named ImagePD. It integrates the long-range awareness of transformer and multi-step denoising of diffusion models into a unified framework, achieving global structural robustness and local detail refinement. Besides, considering the high cost of obtaining video labels, we then repurpose ImagePD for unlabeled wide-angle videos (termed VideoPD), by spatiotemporal diffusion adaption with spatial consistency and temporal smoothness constraints. For the former, we encourage the denoised image to approximate pseudo labels following the wide-angle distortion distribution pattern, while for the latter, we derive rectification trajectories with backward optical flows and smooth them. Compared with ImagePD, VideoPD maintains high-quality facial corrections in space and mitigates the potential temporal shakes sequentially. Finally, to establish an evaluation benchmark and train the framework, we establish a video portrait dataset with a large diversity in people number, lighting conditions, and background. Experiments demonstrate that the proposed methods outperform existing solutions quantitatively and qualitatively, contributing to high-fidelity wide-angle videos with stable and natural portraits. The codes and dataset will be available.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00328">Simple yet Effective Node Property Prediction on Edge Streams under Distribution Shifts</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jongha Lee, Taehyung Kwon, Heechan Moon, Kijung Shin</div>
        <div class="preprint-abstract">arXiv:2504.00328v1 Announce Type: new 
Abstract: The problem of predicting node properties (e.g., node classes) in graphs has received significant attention due to its broad range of applications. Graphs from real-world datasets often evolve over time, with newly emerging edges and dynamically changing node properties, posing a significant challenge for this problem. In response, temporal graph neural networks (TGNNs) have been developed to predict dynamic node properties from a stream of emerging edges. However, our analysis reveals that most TGNN-based methods are (a) far less effective without proper node features and, due to their complex model architectures, (b) vulnerable to distribution shifts. In this paper, we propose SPLASH, a simple yet powerful method for predicting node properties on edge streams under distribution shifts. Our key contributions are as follows: (1) we propose feature augmentation methods and an automatic feature selection method for edge streams, which improve the effectiveness of TGNNs, (2) we propose a lightweight MLP-based TGNN architecture that is highly efficient and robust under distribution shifts, and (3) we conduct extensive experiments to evaluate the accuracy, efficiency, generalization, and qualitative performance of the proposed method and its competitors on dynamic node classification, dynamic anomaly detection, and node affinity prediction tasks across seven real-world datasets.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00726">EMO: Edge Model Overlays to Scale Model Size in Federated Learning</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Di Wu, Weibo He, Wanglei Feng, Zhenyu Wen, Bin Qian, Blesson Varghese</div>
        <div class="preprint-abstract">arXiv:2504.00726v1 Announce Type: new 
Abstract: Federated Learning (FL) trains machine learning models on edge devices with distributed data. However, the computational and memory limitations of these devices restrict the training of large models using FL. Split Federated Learning (SFL) addresses this challenge by distributing the model across the device and server, but it introduces a tightly coupled data flow, leading to computational bottlenecks and high communication costs. We propose EMO as a solution to enable the training of large models in FL while mitigating the challenges of SFL. EMO introduces Edge Model Overlay(s) between the device and server, enabling the creation of a larger ensemble model without modifying the FL workflow. The key innovation in EMO is Augmented Federated Learning (AFL), which builds an ensemble model by connecting the original (smaller) FL model with model(s) trained in the overlay(s) to facilitate horizontal or vertical scaling. This is accomplished through three key modules: a hierarchical activation replay cache to decouple AFL from FL, a convergence-aware communication controller to optimize communication overhead, and an ensemble inference module. Evaluations on a real-world prototype show that EMO improves accuracy by up to 17.77% compared to FL, and reduces communication costs by up to 7.17x and decreases training time by up to 6.9x compared to SFL.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/1904.06866">Predicting human decisions with behavioral theories and machine learning</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Ori Plonsky, Reut Apel, Eyal Ert, Moshe Tennenholtz, David Bourgin, Joshua C. Peterson, Daniel Reichman, Thomas L. Griffiths, Stuart J. Russell, Evan C. Carter, James F. Cavanagh, Ido Erev</div>
        <div class="preprint-abstract">arXiv:1904.06866v3 Announce Type: replace 
Abstract: Predicting human decisions under risk and uncertainty remains a fundamental challenge across disciplines. Existing models often struggle even in highly stylized tasks like choice between lotteries. We introduce BEAST Gradient Boosting (BEAST-GB), a hybrid model integrating behavioral theory (BEAST) with machine learning. We first present CPC18, a competition for predicting risky choice, in which BEAST-GB won. Then, using two large datasets, we demonstrate BEAST-GB predicts more accurately than neural networks trained on extensive data and dozens of existing behavioral models. BEAST-GB also generalizes robustly across unseen experimental contexts, surpassing direct empirical generalization, and helps refine and improve the behavioral theory itself. Our analyses highlight the potential of anchoring predictions on behavioral theory even in data-rich settings and even when the theory alone falters. Our results underscore how integrating machine learning with theoretical frameworks, especially those-like BEAST-designed for prediction, can improve our ability to predict and understand human behavior.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00427">Wavenumber affects the lift of ray-inspired fins near a substrate</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Yuanhang Zhu, Leo Liu, Tianjun Han, Qimin Feng, Keith Moored, Qiang Zhong, Daniel Quinn</div>
        <div class="preprint-abstract">arXiv:2504.00427v1 Announce Type: cross 
Abstract: Rays and skates tend to have different fin kinematics depending on their proximity to a ground plane such as the sea floor. Near the ground, rays tend to be more undulatory (high wavenumber), while far from the ground, rays tend to be more oscillatory (low wavenumber). It is unknown whether these differences are driven by hydrodynamics or other biological pressures. Here we show that near the ground, the time-averaged lift on a ray-like fin is highly dependent on wavenumber. We support our claims using a ray-inspired robotic rig that can produce oscillatory and undulatory motions on the same fin. Potential flow simulations reveal that lift is always negative because quasisteady forces overcome wake-induced forces. Three-dimensional flow measurements demonstrate that oscillatory wakes are more disrupted by the ground than undulatory wakes. All these effects lead to a suction force toward the ground that is stronger and more destabilizing for oscillatory fins than undulatory fins. Our results suggest that wavenumber plays a role in the near-ground dynamics of ray-like fins, particularly in terms of dorsoventral accelerations. The fact that lower wavenumber is linked with stronger suction forces offers a new way to interpret the depth-dependent kinematics of rays and ray-inspired robots.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.23862">Learned Image Compression and Restoration for Digital Pathology</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">SeonYeong Lee, EonSeung Seong, DongEon Lee, SiYeoul Lee, Yubin Cho, Chunsu Park, Seonho Kim, MinKyung Seo, YoungSin Ko, MinWoo Kim</div>
        <div class="preprint-abstract">arXiv:2503.23862v2 Announce Type: replace-cross 
Abstract: Digital pathology images play a crucial role in medical diagnostics, but their ultra-high resolution and large file sizes pose significant challenges for storage, transmission, and real-time visualization. To address these issues, we propose CLERIC, a novel deep learning-based image compression framework designed specifically for whole slide images (WSIs). CLERIC integrates a learnable lifting scheme and advanced convolutional techniques to enhance compression efficiency while preserving critical pathological details. Our framework employs a lifting-scheme transform in the analysis stage to decompose images into low- and high-frequency components, enabling more structured latent representations. These components are processed through parallel encoders incorporating Deformable Residual Blocks (DRB) and Recurrent Residual Blocks (R2B) to improve feature extraction and spatial adaptability. The synthesis stage applies an inverse lifting transform for effective image reconstruction, ensuring high-fidelity restoration of fine-grained tissue structures. We evaluate CLERIC on a digital pathology image dataset and compare its performance against state-of-the-art learned image compression (LIC) models. Experimental results demonstrate that CLERIC achieves superior rate-distortion (RD) performance, significantly reducing storage requirements while maintaining high diagnostic image quality. Our study highlights the potential of deep learning-based compression in digital pathology, facilitating efficient data management and long-term storage while ensuring seamless integration into clinical workflows and AI-assisted diagnostic systems. Code and models are available at: https://github.com/pnu-amilab/CLERIC.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00034">Quantum Generative Models for Image Generation: Insights from MNIST and MedMNIST</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Chi-Sheng Chen, Wei An Hou, Siang-Wei Hu, Zhen-Sheng Cai</div>
        <div class="preprint-abstract">arXiv:2504.00034v1 Announce Type: cross 
Abstract: Research on quantum generative models is currently in its early exploratory stages, with very few established methodologies. In this paper, we propose a novel hybrid quantum generative model based on variational quantum circuits for image generation tasks, introducing innovative noise techniques specifically tailored for quantum computation. Our approach utilizes two distinctive noise strategies: quantum-generated noise inherent to quantum circuits, and a newly developed noise scheduling method, applying different noise levels strategically across time steps during the training process. Experiments conducted on MNIST and MedMNIST datasets demonstrate that our hybrid quantum model, combined with these specialized noise techniques, achieves promising results, suggesting improved generative performance compared to baseline quantum generative approaches. This exploratory work lays a critical foundation and opens new avenues for advancing quantum generative modeling research.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00142">Lorentzian Graph Isomorphic Network</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Srinitish Srinivasan, Omkumar CU</div>
        <div class="preprint-abstract">arXiv:2504.00142v1 Announce Type: new 
Abstract: We introduce the Lorentzian Graph Isomorphic Network (LGIN), a novel graph neural network (GNN) designed to operate in hyperbolic spaces, leveraging the Lorentzian model to enhance graph representation learning. Existing GNNs primarily operate in Euclidean spaces, which can limit their ability to capture hierarchical and multi-relational structures inherent to complex graphs. LGIN addresses this by incorporating curvature-aware aggregation functions that preserve the Lorentzian metric tensor, ensuring embeddings remain constrained within the hyperbolic space by proposing a new update rule that effectively captures both local neighborhood interactions and global structural properties, enabling LGIN to distinguish non-isomorphic graphs with expressiveness at least as powerful as the Weisfeiler-Lehman test. Through extensive evaluation across nine benchmark datasets, including molecular and protein structures, LGIN consistently outperforms or matches state-of-the-art GNNs, demonstrating its robustness and efficacy in modeling complex graph structures. To the best of our knowledge, this is the first study to extend the concept of a powerful graph neural network to Riemannian manifolds, paving the way for future advancements in hyperbolic graph learning. The code for our paper can be found at https://github.com/Deceptrax123/LGIN.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00899">Role and Use of Race in AI/ML Models Related to Health</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Martin C. Were, Ang Li, Bradley A. Malin, Zhijun Yin, Joseph R. Coco, Benjamin X. Collins, Ellen Wright Clayton, Laurie L. Novak, Rachele Hendricks-Sturrup, Abiodun Oluyomi, Shilo Anders, Chao Yan</div>
        <div class="preprint-abstract">arXiv:2504.00899v1 Announce Type: cross 
Abstract: The role and use of race within health-related artificial intelligence and machine learning (AI/ML) models has sparked increasing attention and controversy. Despite the complexity and breadth of related issues, a robust and holistic framework to guide stakeholders in their examination and resolution remains lacking. This perspective provides a broad-based, systematic, and cross-cutting landscape analysis of race-related challenges, structured around the AI/ML lifecycle and framed through "points to consider" to support inquiry and decision-making.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2412.03526">Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular Videos</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Hanxue Liang, Jiawei Ren, Ashkan Mirzaei, Antonio Torralba, Ziwei Liu, Igor Gilitschenski, Sanja Fidler, Cengiz Oztireli, Huan Ling, Zan Gojcic, Jiahui Huang</div>
        <div class="preprint-abstract">arXiv:2412.03526v2 Announce Type: replace-cross 
Abstract: Recent advancements in static feed-forward scene reconstruction have demonstrated significant progress in high-quality novel view synthesis. However, these models often struggle with generalizability across diverse environments and fail to effectively handle dynamic content. We present BTimer (short for BulletTimer), the first motion-aware feed-forward model for real-time reconstruction and novel view synthesis of dynamic scenes. Our approach reconstructs the full scene in a 3D Gaussian Splatting representation at a given target ('bullet') timestamp by aggregating information from all the context frames. Such a formulation allows BTimer to gain scalability and generalization by leveraging both static and dynamic scene datasets. Given a casual monocular dynamic video, BTimer reconstructs a bullet-time scene within 150ms while reaching state-of-the-art performance on both static and dynamic scene datasets, even compared with optimization-based approaches.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2502.01860">SE Arena: An Interactive Platform for Evaluating Foundation Models in Software Engineering</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Zhimin Zhao</div>
        <div class="preprint-abstract">arXiv:2502.01860v2 Announce Type: replace-cross 
Abstract: Foundation models (FMs), particularly large language models (LLMs), have shown significant promise in various software engineering (SE) tasks, including code generation, debugging, and requirement refinement. Despite these advances, existing evaluation frameworks are insufficient for assessing model performance in iterative, context-rich workflows characteristic of SE activities. To address this limitation, we introduce SE Arena, an interactive platform designed to evaluate SE-focused chatbots. SE Arena provides a transparent, open-source leaderboard, supports multi-round conversational workflows, and enables end-to-end model comparisons. Moreover, SE Arena incorporates a new feature called RepoChat, which automatically injects repository-related context (e.g., issues, commits, pull requests) into the conversation, further aligning evaluations with real-world development processes. This paper outlines the design and capabilities of SE Arena, emphasizing its potential to advance the evaluation and practical application of FMs in software engineering.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00858">Whispering Under the Eaves: Protecting User Privacy Against Commercial and LLM-powered Automatic Speech Recognition Systems</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Weifei Jin, Yuxin Cao, Junjie Su, Derui Wang, Yedi Zhang, Minhui Xue, Jie Hao, Jin Song Dong, Yixian Yang</div>
        <div class="preprint-abstract">arXiv:2504.00858v1 Announce Type: cross 
Abstract: The widespread application of automatic speech recognition (ASR) supports large-scale voice surveillance, raising concerns about privacy among users. In this paper, we concentrate on using adversarial examples to mitigate unauthorized disclosure of speech privacy thwarted by potential eavesdroppers in speech communications. While audio adversarial examples have demonstrated the capability to mislead ASR models or evade ASR surveillance, they are typically constructed through time-intensive offline optimization, restricting their practicality in real-time voice communication. Recent work overcame this limitation by generating universal adversarial perturbations (UAPs) and enhancing their transferability for black-box scenarios. However, they introduced excessive noise that significantly degrades audio quality and affects human perception, thereby limiting their effectiveness in practical scenarios. To address this limitation and protect live users' speech against ASR systems, we propose a novel framework, AudioShield. Central to this framework is the concept of Transferable Universal Adversarial Perturbations in the Latent Space (LS-TUAP). By transferring the perturbations to the latent space, the audio quality is preserved to a large extent. Additionally, we propose target feature adaptation to enhance the transferability of UAPs by embedding target text features into the perturbations. Comprehensive evaluation on four commercial ASR APIs (Google, Amazon, iFlytek, and Alibaba), three voice assistants, two LLM-powered ASR and one NN-based ASR demonstrates the protection superiority of AudioShield over existing competitors, and both objective and subjective evaluations indicate that AudioShield significantly improves the audio quality. Moreover, AudioShield also shows high effectiveness in real-time end-to-end scenarios, and demonstrates strong resilience against adaptive countermeasures.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00613">LLM-Guided Search for Deletion-Correcting Codes</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Franziska Weindel, Reinhard Heckel</div>
        <div class="preprint-abstract">arXiv:2504.00613v1 Announce Type: new 
Abstract: Finding deletion-correcting codes of maximum size has been an open problem for over 70 years, even for a single deletion. In this paper, we propose a novel approach for constructing deletion-correcting codes. A code is a set of sequences satisfying certain constraints, and we construct it by greedily adding the highest-priority sequence according to a priority function. To find good priority functions, we leverage FunSearch, a large language model (LLM)-guided evolutionary search proposed by Romera et al., 2024. FunSearch iteratively generates, evaluates, and refines priority functions to construct large deletion-correcting codes. For a single deletion, our evolutionary search finds functions that construct codes which match known maximum sizes, reach the size of the largest (conjectured optimal) Varshamov-Tenengolts codes where the maximum is unknown, and independently rediscover them in equivalent form. For two deletions, we find functions that construct codes with new best-known sizes for code lengths \( n = 12, 13 \), and \( 16 \), establishing improved lower bounds. These results demonstrate the potential of LLM-guided search for information theory and code design and represent the first application of such methods for constructing error-correcting codes.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00018">SandboxEval: Towards Securing Test Environment for Untrusted Code</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Rafiqul Rabin, Jesse Hostetler, Sean McGregor, Brett Weir, Nick Judd</div>
        <div class="preprint-abstract">arXiv:2504.00018v1 Announce Type: cross 
Abstract: While large language models (LLMs) are powerful assistants in programming tasks, they may also produce malicious code. Testing LLM-generated code therefore poses significant risks to assessment infrastructure tasked with executing untrusted code. To address these risks, this work focuses on evaluating the security and confidentiality properties of test environments, reducing the risk that LLM-generated code may compromise the assessment infrastructure. We introduce SandboxEval, a test suite featuring manually crafted test cases that simulate real-world safety scenarios for LLM assessment environments in the context of untrusted code execution. The suite evaluates vulnerabilities to sensitive information exposure, filesystem manipulation, external communication, and other potentially dangerous operations in the course of assessment activity. We demonstrate the utility of SandboxEval by deploying it on an open-source implementation of Dyff, an established AI assessment framework used to evaluate the safety of LLMs at scale. We show, first, that the test suite accurately describes limitations placed on an LLM operating under instructions to generate malicious code. Second, we show that the test results provide valuable insights for developers seeking to harden assessment infrastructure and identify risks associated with LLM execution activities.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00609">Bi-Grid Reconstruction for Image Anomaly Detection</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Huichuan Huang, Zhiqing Zhong, Guangyu Wei, Yonghao Wan, Wenlong Sun, Aimin Feng</div>
        <div class="preprint-abstract">arXiv:2504.00609v1 Announce Type: cross 
Abstract: In image anomaly detection, significant advancements have been made using un- and self-supervised methods with datasets containing only normal samples. However, these approaches often struggle with fine-grained anomalies. This paper introduces \textbf{GRAD}: Bi-\textbf{G}rid \textbf{R}econstruction for Image \textbf{A}nomaly \textbf{D}etection, which employs two continuous grids to enhance anomaly detection from both normal and abnormal perspectives. In this work: 1) Grids as feature repositories that improve generalization and mitigate the Identical Shortcut (IS) issue; 2) An abnormal feature grid that refines normal feature boundaries, boosting detection of fine-grained defects; 3) The Feature Block Paste (FBP) module, which synthesizes various anomalies at the feature level for quick abnormal grid deployment. GRAD's robust representation capabilities also allow it to handle multiple classes with a single model. Evaluations on datasets like MVTecAD, VisA, and GoodsAD show significant performance improvements in fine-grained anomaly detection. GRAD excels in overall accuracy and in discerning subtle differences, demonstrating its superiority over existing methods.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00029">Generating Structured Plan Representation of Procedures with LLMs</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Deepeka Garg, Sihan Zeng, Sumitra Ganesh, Leo Ardon</div>
        <div class="preprint-abstract">arXiv:2504.00029v1 Announce Type: cross 
Abstract: In this paper, we address the challenges of managing Standard Operating Procedures (SOPs), which often suffer from inconsistencies in language, format, and execution, leading to operational inefficiencies. Traditional process modeling demands significant manual effort, domain expertise, and familiarity with complex languages like Business Process Modeling Notation (BPMN), creating barriers for non-techincal users. We introduce SOP Structuring (SOPStruct), a novel approach that leverages Large Language Models (LLMs) to transform SOPs into decision-tree-based structured representations. SOPStruct produces a standardized representation of SOPs across different domains, reduces cognitive load, and improves user comprehension by effectively capturing task dependencies and ensuring sequential integrity. Our approach enables leveraging the structured information to automate workflows as well as empower the human users. By organizing procedures into logical graphs, SOPStruct facilitates backtracking and error correction, offering a scalable solution for process optimization. We employ a novel evaluation framework, combining deterministic methods with the Planning Domain Definition Language (PDDL) to verify graph soundness, and non-deterministic assessment by an LLM to ensure completeness. We empirically validate the robustness of our LLM-based structured SOP representation methodology across SOPs from different domains and varying levels of complexity. Despite the current lack of automation readiness in many organizations, our research highlights the transformative potential of LLMs to streamline process modeling, paving the way for future advancements in automated procedure optimization.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00374">When Persuasion Overrides Truth in Multi-Agent LLM Debates: Introducing a Confidence-Weighted Persuasion Override Rate (CW-POR)</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Mahak Agarwal, Divyam Khanna</div>
        <div class="preprint-abstract">arXiv:2504.00374v1 Announce Type: cross 
Abstract: In many real-world scenarios, a single Large Language Model (LLM) may encounter contradictory claims-some accurate, others forcefully incorrect-and must judge which is true. We investigate this risk in a single-turn, multi-agent debate framework: one LLM-based agent provides a factual answer from TruthfulQA, another vigorously defends a falsehood, and the same LLM architecture serves as judge. We introduce the Confidence-Weighted Persuasion Override Rate (CW-POR), which captures not only how often the judge is deceived but also how strongly it believes the incorrect choice. Our experiments on five open-source LLMs (3B-14B parameters), where we systematically vary agent verbosity (30-300 words), reveal that even smaller models can craft persuasive arguments that override truthful answers-often with high confidence. These findings underscore the importance of robust calibration and adversarial testing to prevent LLMs from confidently endorsing misinformation.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00670">Oscillation in the SIRS model</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">D. Marenduzzo, A. T. Brown, C. Miller, G. J. Ackland</div>
        <div class="preprint-abstract">arXiv:2504.00670v1 Announce Type: new 
Abstract: We study the SIRS epidemic model, both analytically and on a square lattice. The analytic model has two stable solutions, post outbreak/epidemic (no infected, $I=0$) and the endemic state (constant number of infected: $I>0$). When the model is implemented with noise, or on a lattice, a third state is possible, featuring regular oscillations. This is understood as a cycle of boom and bust, where an epidemic sweeps through, and dies out leaving a small number of isolated infecteds. As immunity wanes, herd immunity is lost throughout the population and the epidemic repeats. The key result is that the oscillation is an intrinsic feature of the system itself, not driven by external factors such as seasonality or behavioural changes. The model shows that non-seasonal oscillations, such as those observed for the omicron COVID variant, need no additional explanation such as the appearance of more infectious variants at regular intervals or coupling to behaviour. We infer that the loss of immunity to the SARS-CoV-2 virus occurs on a timescale of about ten weeks.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00820">Deep Generative Models: Complexity, Dimensionality, and Approximation</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Kevin Wang, Hongqian Niu, Yixin Wang, Didong Li</div>
        <div class="preprint-abstract">arXiv:2504.00820v1 Announce Type: new 
Abstract: Generative networks have shown remarkable success in learning complex data distributions, particularly in generating high-dimensional data from lower-dimensional inputs. While this capability is well-documented empirically, its theoretical underpinning remains unclear. One common theoretical explanation appeals to the widely accepted manifold hypothesis, which suggests that many real-world datasets, such as images and signals, often possess intrinsic low-dimensional geometric structures. Under this manifold hypothesis, it is widely believed that to approximate a distribution on a $d$-dimensional Riemannian manifold, the latent dimension needs to be at least $d$ or $d+1$. In this work, we show that this requirement on the latent dimension is not necessary by demonstrating that generative networks can approximate distributions on $d$-dimensional Riemannian manifolds from inputs of any arbitrary dimension, even lower than $d$, taking inspiration from the concept of space-filling curves. This approach, in turn, leads to a super-exponential complexity bound of the deep neural networks through expanded neurons. Our findings thus challenge the conventional belief on the relationship between input dimensionality and the ability of generative networks to model data distributions. This novel insight not only corroborates the practical effectiveness of generative networks in handling complex data structures, but also underscores a critical trade-off between approximation error, dimensionality, and model complexity.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00012">I'm Sorry Dave: How the old world of personnel security can inform the new world of AI insider risk</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Paul Martin, Sarah Mercer</div>
        <div class="preprint-abstract">arXiv:2504.00012v1 Announce Type: cross 
Abstract: Organisations are rapidly adopting artificial intelligence (AI) tools to perform tasks previously undertaken by people. The potential benefits are enormous. Separately, some organisations deploy personnel security measures to mitigate the security risks arising from trusted human insiders. Unfortunately, there is no meaningful interplay between the rapidly evolving domain of AI and the traditional world of personnel security. This is a problem. The complex risks from human insiders are hard enough to understand and manage, despite many decades of effort. The emerging security risks from AI insiders are even more opaque. Both sides need all the help they can get. Some of the concepts and approaches that have proved useful in dealing with human insiders are also applicable to the emerging risks from AI insiders. Furthermore, AI can be used defensively to protect against both human and AI insiders.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2501.18530">Optimal generalisation and learning transition in extensive-width shallow neural networks near interpolation</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jean Barbier, Francesco Camilli, Minh-Toan Nguyen, Mauro Pastore, Rudy Skerk</div>
        <div class="preprint-abstract">arXiv:2501.18530v2 Announce Type: replace-cross 
Abstract: We consider a teacher-student model of supervised learning with a fully-trained two-layer neural network whose width $k$ and input dimension $d$ are large and proportional. We provide an effective theory for approximating the Bayes-optimal generalisation error of the network for any activation function in the regime of sample size $n$ scaling quadratically with the input dimension, i.e., around the interpolation threshold where the number of trainable parameters $kd+k$ and of data $n$ are comparable. Our analysis tackles generic weight distributions. We uncover a discontinuous phase transition separating a "universal" phase from a "specialisation" phase. In the first, the generalisation error is independent of the weight distribution and decays slowly with the sampling rate $n/d^2$, with the student learning only some non-linear combinations of the teacher weights. In the latter, the error is weight distribution-dependent and decays faster due to the alignment of the student towards the teacher network. We thus unveil the existence of a highly predictive solution near interpolation, which is however potentially hard to find by practical algorithms.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00589">Efficient Annotator Reliablity Assessment with EffiARA</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Owen Cook, Jake Vasilakes, Ian Roberts, Xingyi Song</div>
        <div class="preprint-abstract">arXiv:2504.00589v1 Announce Type: cross 
Abstract: Data annotation is an essential component of the machine learning pipeline; it is also a costly and time-consuming process. With the introduction of transformer-based models, annotation at the document level is increasingly popular; however, there is no standard framework for structuring such tasks. The EffiARA annotation framework is, to our knowledge, the first project to support the whole annotation pipeline, from understanding the resources required for an annotation task to compiling the annotated dataset and gaining insights into the reliability of individual annotators as well as the dataset as a whole. The framework's efficacy is supported by two previous studies: one improving classification performance through annotator-reliability-based soft label aggregation and sample weighting, and the other increasing the overall agreement among annotators through removing identifying and replacing an unreliable annotator. This work introduces the EffiARA Python package and its accompanying webtool, which provides an accessible graphical user interface for the system. We open-source the EffiARA Python package at https://github.com/MiniEggz/EffiARA and the webtool is publicly accessible at https://effiara.gate.ac.uk.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00026">Diffusion models applied to skin and oral cancer classification</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jos\'e J. M. Uliana, Renato A. Krohling</div>
        <div class="preprint-abstract">arXiv:2504.00026v1 Announce Type: cross 
Abstract: This study investigates the application of diffusion models in medical image classification (DiffMIC), focusing on skin and oral lesions. Utilizing the datasets PAD-UFES-20 for skin cancer and P-NDB-UFES for oral cancer, the diffusion model demonstrated competitive performance compared to state-of-the-art deep learning models like Convolutional Neural Networks (CNNs) and Transformers. Specifically, for the PAD-UFES-20 dataset, the model achieved a balanced accuracy of 0.6457 for six-class classification and 0.8357 for binary classification (cancer vs. non-cancer). For the P-NDB-UFES dataset, it attained a balanced accuracy of 0.9050. These results suggest that diffusion models are viable models for classifying medical images of skin and oral lesions. In addition, we investigate the robustness of the model trained on PAD-UFES-20 for skin cancer but tested on the clinical images of the HIBA dataset.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00393">Deep learning for state estimation of commercial sodium-ion batteries using partial charging profiles: validation with a multi-temperature ageing dataset</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jiapeng Liu, Lunte Li, Jing Xiang, Laiyong Xie, Yuhao Wang, Francesco Ciucci</div>
        <div class="preprint-abstract">arXiv:2504.00393v1 Announce Type: new 
Abstract: Accurately predicting the state of health for sodium-ion batteries is crucial for managing battery modules, playing a vital role in ensuring operational safety. However, highly accurate models available thus far are rare due to a lack of aging data for sodium-ion batteries. In this study, we experimentally collected 53 single cells at four temperatures (0, 25, 35, and 45 {\deg}C), along with two battery modules in the lab. By utilizing the charging profiles, we were able to predict the SOC, capacity, and SOH simultaneously. This was achieved by designing a new framework that integrates the neural ordinary differential equation and 2D convolutional neural networks, using the partial charging profile as input. The charging profile is partitioned into segments, and each segment is fed into the network to output the SOC. For capacity and SOH prediction, we first aggregated the extracted features corresponding to segments from one cycle, after which an embedding block for temperature is concatenated for the final prediction. This novel approach eliminates the issue of multiple outputs for a single target. Our model demonstrated an $R^2$ accuracy of 0.998 for SOC and 0.997 for SOH across single cells at various temperatures. Furthermore, the trained model can be employed to predict single cells at temperatures outside the training set and battery modules with different capacity and current levels. The results presented here highlight the high accuracy of our model and its capability to predict multiple targets simultaneously using a partial charging profile.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00754">Automated Feature Labeling with Token-Space Gradient Descent</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Julian Schulz, Seamus Fallows</div>
        <div class="preprint-abstract">arXiv:2504.00754v1 Announce Type: new 
Abstract: We present a novel approach to feature labeling using gradient descent in token-space. While existing methods typically use language models to generate hypotheses about feature meanings, our method directly optimizes label representations by using a language model as a discriminator to predict feature activations. We formulate this as a multi-objective optimization problem in token-space, balancing prediction accuracy, entropy minimization, and linguistic naturalness. Our proof-of-concept experiments demonstrate successful convergence to interpretable single-token labels across diverse domains, including features for detecting animals, mammals, Chinese text, and numbers. Although our current implementation is constrained to single-token labels and relatively simple features, the results suggest that token-space gradient descent could become a valuable addition to the interpretability researcher's toolkit.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00844">PRISM-0: A Predicate-Rich Scene Graph Generation Framework for Zero-Shot Open-Vocabulary Tasks</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Abdelrahman Elskhawy, Mengze Li, Nassir Navab, Benjamin Busam</div>
        <div class="preprint-abstract">arXiv:2504.00844v1 Announce Type: cross 
Abstract: In Scene Graphs Generation (SGG) one extracts structured representation from visual inputs in the form of objects nodes and predicates connecting them. This facilitates image-based understanding and reasoning for various downstream tasks. Although fully supervised SGG approaches showed steady performance improvements, they suffer from a severe training bias. This is caused by the availability of only small subsets of curated data and exhibits long-tail predicate distribution issues with a lack of predicate diversity adversely affecting downstream tasks. To overcome this, we introduce PRISM-0, a framework for zero-shot open-vocabulary SGG that bootstraps foundation models in a bottom-up approach to capture the whole spectrum of diverse, open-vocabulary predicate prediction. Detected object pairs are filtered and passed to a Vision Language Model (VLM) that generates descriptive captions. These are used to prompt an LLM to generate fine-andcoarse-grained predicates for the pair. The predicates are then validated using a VQA model to provide a final SGG. With the modular and dataset-independent PRISM-0, we can enrich existing SG datasets such as Visual Genome (VG). Experiments illustrate that PRIMS-0 generates semantically meaningful graphs that improve downstream tasks such as Image Captioning and Sentence-to-Graph Retrieval with a performance on par to the best fully supervised methods.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.17290">Calibration Strategies for Robust Causal Estimation: Theoretical and Empirical Insights on Propensity Score Based Estimators</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jan Rabenseifner, Sven Klaassen, Jannis Kueck, Philipp Bach</div>
        <div class="preprint-abstract">arXiv:2503.17290v2 Announce Type: replace-cross 
Abstract: The partitioning of data for estimation and calibration critically impacts the performance of propensity score based estimators like inverse probability weighting (IPW) and double/debiased machine learning (DML) frameworks. We extend recent advances in calibration techniques for propensity score estimation, improving the robustness of propensity scores in challenging settings such as limited overlap, small sample sizes, or unbalanced data. Our contributions are twofold: First, we provide a theoretical analysis of the properties of calibrated estimators in the context of DML. To this end, we refine existing calibration frameworks for propensity score models, with a particular emphasis on the role of sample-splitting schemes in ensuring valid causal inference. Second, through extensive simulations, we show that calibration reduces variance of inverse-based propensity score estimators while also mitigating bias in IPW, even in small-sample regimes. Notably, calibration improves stability for flexible learners (e.g., gradient boosting) while preserving the doubly robust properties of DML. A key insight is that, even when methods perform well without calibration, incorporating a calibration step does not degrade performance, provided that an appropriate sample-splitting approach is chosen.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00041">Imbalanced malware classification: an approach based on dynamic classifier selection</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">J. V. S. Souza, C. B. Vieira, G. D. C. Cunha, R. M. O. Cruz</div>
        <div class="preprint-abstract">arXiv:2504.00041v1 Announce Type: cross 
Abstract: In recent years, the rise of cyber threats has emphasized the need for robust malware detection systems, especially on mobile devices. Malware, which targets vulnerabilities in devices and user data, represents a substantial security risk. A significant challenge in malware detection is the imbalance in datasets, where most applications are benign, with only a small fraction posing a threat. This study addresses the often-overlooked issue of class imbalance in malware detection by evaluating various machine learning strategies for detecting malware in Android applications. We assess monolithic classifiers and ensemble methods, focusing on dynamic selection algorithms, which have shown superior performance compared to traditional approaches. In contrast to balancing strategies performed on the whole dataset, we propose a balancing procedure that works individually for each classifier in the pool. Our empirical analysis demonstrates that the KNOP algorithm obtained the best results using a pool of Random Forest. Additionally, an instance hardness assessment revealed that balancing reduces the difficulty of the minority class and enhances the detection of the minority class (malware). The code used for the experiments is available at https://github.com/jvss2/Machine-Learning-Empirical-Evaluation.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00795">Explainable AI-Based Interface System for Weather Forecasting Model</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Soyeon Kim, Junho Choi, Yeji Choi, Subeen Lee, Artyom Stitsyuk, Minkyoung Park, Seongyeop Jeong, Youhyun Baek, Jaesik Choi</div>
        <div class="preprint-abstract">arXiv:2504.00795v1 Announce Type: new 
Abstract: Machine learning (ML) is becoming increasingly popular in meteorological decision-making. Although the literature on explainable artificial intelligence (XAI) is growing steadily, user-centered XAI studies have not extend to this domain yet. This study defines three requirements for explanations of black-box models in meteorology through user studies: statistical model performance for different rainfall scenarios to identify model bias, model reasoning, and the confidence of model outputs. Appropriate XAI methods are mapped to each requirement, and the generated explanations are tested quantitatively and qualitatively. An XAI interface system is designed based on user feedback. The results indicate that the explanations increase decision utility and user trust. Users prefer intuitive explanations over those based on XAI algorithms even for potentially easy-to-recognize examples. These findings can provide evidence for future research on user-centered XAI algorithms, as well as a basis to improve the usability of AI systems in practice.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00694">On Benchmarking Code LLMs for Android Malware Analysis</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Yiling He, Hongyu She, Xingzhi Qian, Xinran Zheng, Zhuo Chen, Zhan Qin, Lorenzo Cavallaro</div>
        <div class="preprint-abstract">arXiv:2504.00694v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in various code intelligence tasks. However, their effectiveness for Android malware analysis remains underexplored. Decompiled Android code poses unique challenges for analysis, primarily due to its large volume of functions and the frequent absence of meaningful function names. This paper presents Cama, a benchmarking framework designed to systematically evaluate the effectiveness of Code LLMs in Android malware analysis tasks. Cama specifies structured model outputs (comprising function summaries, refined function names, and maliciousness scores) to support key malware analysis tasks, including malicious function identification and malware purpose summarization. Built on these, it integrates three domain-specific evaluation metrics, consistency, fidelity, and semantic relevance, enabling rigorous stability and effectiveness assessment and cross-model comparison. We construct a benchmark dataset consisting of 118 Android malware samples, encompassing over 7.5 million distinct functions, and use Cama to evaluate four popular open-source models. Our experiments provide insights into how Code LLMs interpret decompiled code and quantify the sensitivity to function renaming, highlighting both the potential and current limitations of Code LLMs in malware analysis tasks.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00652">Towards Adaptive AI Governance: Comparative Insights from the U.S., EU, and Asia</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Vikram Kulothungan, Deepti Gupta</div>
        <div class="preprint-abstract">arXiv:2504.00652v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) trends vary significantly across global regions, shaping the trajectory of innovation, regulation, and societal impact. This variation influences how different regions approach AI development, balancing technological progress with ethical and regulatory considerations. This study conducts a comparative analysis of AI trends in the United States (US), the European Union (EU), and Asia, focusing on three key dimensions: generative AI, ethical oversight, and industrial applications. The US prioritizes market-driven innovation with minimal regulatory constraints, the EU enforces a precautionary risk-based framework emphasizing ethical safeguards, and Asia employs state-guided AI strategies that balance rapid deployment with regulatory oversight. Although these approaches reflect different economic models and policy priorities, their divergence poses challenges to international collaboration, regulatory harmonization, and the development of global AI standards. To address these challenges, this paper synthesizes regional strengths to propose an adaptive AI governance framework that integrates risk-tiered oversight, innovation accelerators, and strategic alignment mechanisms. By bridging governance gaps, this study offers actionable insights for fostering responsible AI development while ensuring a balance between technological progress, ethical imperatives, and regulatory coherence.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00970">SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV Caching</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Yuxuan Zhu, Ali Falahati, David H. Yang, Mohammad Mohammadi Amiri</div>
        <div class="preprint-abstract">arXiv:2504.00970v1 Announce Type: cross 
Abstract: Large language models face significant computational and memory challenges when processing long contexts. During inference, efficient management of the key-value (KV) cache, which stores intermediate activations for autoregressive generation, is critical to reducing memory overhead and improving computational efficiency. Traditional token-level efficient KV caching methods overlook semantic information, treating tokens independently without considering their semantic relationships. Meanwhile, existing semantic-preserving KV cache management approaches often suffer from substantial memory usage and high time-to-first-token. To address these limitations, we propose SentenceKV, a novel sentence-level semantic KV caching approach designed to enhance inference efficiency while preserving semantic coherence. During prefilling, SentenceKV groups tokens based on sentence-level semantic similarity, compressing sentence representations into concise semantic vectors stored directly on the GPU, while individual KV pairs are offloaded to CPU. During decoding, SentenceKV generates tokens by selectively retrieving semantically relevant sentence-level KV entries, leveraging the semantic similarity between the prefilling-stage semantic vectors and decoding-stage queries. This ensures efficient and contextually accurate predictions, minimizing the loading of redundant or irrelevant data into GPU memory and significantly reducing memory overhead while maintaining stable inference latency, even for extremely long contexts. Extensive evaluations on benchmarks including PG-19, LongBench, and Needle-In-A-Haystack demonstrate that SentenceKV significantly outperforms state-of-the-art methods in both efficiency and memory usage, without compromising model accuracy.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00040">Quantum Methods for Managing Ambiguity in Natural Language Processing</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jurek Eisinger, Ward Gauderis, Lin de Huybrecht, Geraint A. Wiggins</div>
        <div class="preprint-abstract">arXiv:2504.00040v1 Announce Type: cross 
Abstract: The Categorical Compositional Distributional (DisCoCat) framework models meaning in natural language using the mathematical framework of quantum theory, expressed as formal diagrams. DisCoCat diagrams can be associated with tensor networks and quantum circuits. DisCoCat diagrams have been connected to density matrices in various contexts in Quantum Natural Language Processing (QNLP). Previous use of density matrices in QNLP entails modelling ambiguous words as probability distributions over more basic words (the word \texttt{queen}, e.g., might mean the reigning queen or the chess piece). In this article, we investigate using probability distributions over processes to account for syntactic ambiguity in sentences. The meanings of these sentences are represented by density matrices. We show how to create probability distributions on quantum circuits that represent the meanings of sentences and explain how this approach generalises tasks from the literature. We conduct an experiment to validate the proposed theory.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.01002">Token embeddings violate the manifold hypothesis</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Michael Robinson, Sourya Dey, Tony Chiang</div>
        <div class="preprint-abstract">arXiv:2504.01002v1 Announce Type: cross 
Abstract: To fully understand the behavior of a large language model (LLM) requires our understanding of its input space. If this input space differs from our assumption, our understanding of and conclusions about the LLM is likely flawed, regardless of its architecture. Here, we elucidate the structure of the token embeddings, the input domain for LLMs, both empirically and theoretically. We present a generalized and statistically testable model where the neighborhood of each token splits into well-defined signal and noise dimensions.
  This model is based on a generalization of a manifold called a fiber bundle, so we denote our hypothesis test as the ``fiber bundle null.'' Failing to reject the null is uninformative, but rejecting it at a specific token indicates that token has a statistically significant local structure, and so is of interest to us. By running our test over several open-source LLMs, each with unique token embeddings, we find that the null is frequently rejected, and so the token subspace is provably not a fiber bundle and hence also not a manifold. As a consequence of our findings, when an LLM is presented with two semantically equivalent prompts, and if one prompt contains a token implicated by our test, that prompt will likely exhibit more output variability proportional to the local signal dimension of the token.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00011">Four Things People Should Know About Migraines</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Mohammad S. Parsa, Lukasz Golab</div>
        <div class="preprint-abstract">arXiv:2504.00011v1 Announce Type: cross 
Abstract: Migraine literacy among the public is known to be low, and this lack of understanding has a negative impact on migraineurs' quality of life. To understand this impact, we use text mining methods to study migraine discussion on the Reddit social media platform. We summarize the findings in the form of "four things people should know about chronic migraines": it is a serious disease that affects people of all ages, it can be triggered by many different factors, it affects women more than men, and it can get worse in combination with the COVID-19 virus.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00557">Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jewon Lee, Ki-Ung Song, Seungmin Yang, Donguk Lim, Jaeyeon Kim, Wooksu Shin, Bo-Kyeong Kim, Yong Jae Lee, Tae-Ho Kim</div>
        <div class="preprint-abstract">arXiv:2504.00557v1 Announce Type: cross 
Abstract: Visual token reduction lowers inference costs caused by extensive image features in large vision-language models (LVLMs). Unlike relevant studies that prune tokens in self-attention-only LVLMs, our work uniquely addresses cross-attention-based models, which achieve superior performance. We identify that the key-value (KV) cache size for image tokens in cross-attention layers significantly exceeds that of text tokens in self-attention layers, posing a major compute bottleneck. To mitigate this issue, we exploit the sparse nature in cross-attention maps to selectively prune redundant visual features. Our Trimmed Llama effectively reduces KV cache demands without requiring additional training. By benefiting from 50%-reduced visual features, our model can reduce inference latency and memory usage while achieving benchmark parity.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00904">Explorable INR: An Implicit Neural Representation for Ensemble Simulation Enabling Efficient Spatial and Parameter Exploration</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Yi-Tang Chen, Haoyu Li, Neng Shi, Xihaier Luo, Wei Xu, Han-Wei Shen</div>
        <div class="preprint-abstract">arXiv:2504.00904v1 Announce Type: cross 
Abstract: With the growing computational power available for high-resolution ensemble simulations in scientific fields such as cosmology and oceanology, storage and computational demands present significant challenges. Current surrogate models fall short in the flexibility of point- or region-based predictions as the entire field reconstruction is required for each parameter setting, hence hindering the efficiency of parameter space exploration. Limitations exist in capturing physical attribute distributions and pinpointing optimal parameter configurations. In this work, we propose Explorable INR, a novel implicit neural representation-based surrogate model, designed to facilitate exploration and allow point-based spatial queries without computing full-scale field data. In addition, to further address computational bottlenecks of spatial exploration, we utilize probabilistic affine forms (PAFs) for uncertainty propagation through Explorable INR to obtain statistical summaries, facilitating various ensemble analysis and visualization tasks that are expensive with existing models. Furthermore, we reformulate the parameter exploration problem as optimization tasks using gradient descent and KL divergence minimization that ensures scalability. We demonstrate that the Explorable INR with the proposed approach for spatial and parameter exploration can significantly reduce computation and memory costs while providing effective ensemble analysis.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2502.01684">Leveraging Joint Predictive Embedding and Bayesian Inference in Graph Self Supervised Learning</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Srinitish Srinivasan, Omkumar CU</div>
        <div class="preprint-abstract">arXiv:2502.01684v3 Announce Type: replace 
Abstract: Graph representation learning has emerged as a cornerstone for tasks like node classification and link prediction, yet prevailing self-supervised learning (SSL) methods face challenges such as computational inefficiency, reliance on contrastive objectives, and representation collapse. Existing approaches often depend on feature reconstruction, negative sampling, or complex decoders, which introduce training overhead and hinder generalization. Further, current techniques which address such limitations fail to account for the contribution of node embeddings to a certain prediction in the absence of labeled nodes. To address these limitations, we propose a novel joint embedding predictive framework for graph SSL that eliminates contrastive objectives and negative sampling while preserving semantic and structural information. Additionally, we introduce a semantic-aware objective term that incorporates pseudo-labels derived from Gaussian Mixture Models (GMMs), enhancing node discriminability by evaluating latent feature contributions. Extensive experiments demonstrate that our framework outperforms state-of-the-art graph SSL methods across benchmarks, achieving superior performance without contrastive loss or complex decoders. Key innovations include (1) a non-contrastive, view-invariant joint embedding predictive architecture, (2) Leveraging single context and multiple targets relationship between subgraphs, and (3) GMM-based pseudo-label scoring to capture semantic contributions. This work advances graph SSL by offering a computationally efficient, collapse-resistant paradigm that bridges spatial and semantic graph features for downstream tasks. The code for our paper can be found at https://github.com/Deceptrax123/JPEB-GSSL</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00921">Benchmarking Federated Machine Unlearning methods for Tabular Data</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Chenguang Xiao, Abhirup Ghosh, Han Wu, Shuo Wang, Diederick van Thiel</div>
        <div class="preprint-abstract">arXiv:2504.00921v1 Announce Type: new 
Abstract: Machine unlearning, which enables a model to forget specific data upon request, is increasingly relevant in the era of privacy-centric machine learning, particularly within federated learning (FL) environments. This paper presents a pioneering study on benchmarking machine unlearning methods within a federated setting for tabular data, addressing the unique challenges posed by cross-silo FL where data privacy and communication efficiency are paramount. We explore unlearning at the feature and instance levels, employing both machine learning, random forest and logistic regression models. Our methodology benchmarks various unlearning algorithms, including fine-tuning and gradient-based approaches, across multiple datasets, with metrics focused on fidelity, certifiability, and computational efficiency. Experiments demonstrate that while fidelity remains high across methods, tree-based models excel in certifiability, ensuring exact unlearning, whereas gradient-based methods show improved computational efficiency. This study provides critical insights into the design and selection of unlearning algorithms tailored to the FL environment, offering a foundation for further research in privacy-preserving machine learning.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00306">LOCO-EPI: Leave-one-chromosome-out (LOCO) as a benchmarking paradigm for deep learning based prediction of enhancer-promoter interactions</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Muhammad Tahir, Shehroz S. Khan, James Davie, Soichiro Yamanaka, Ahmed Ashraf</div>
        <div class="preprint-abstract">arXiv:2504.00306v1 Announce Type: new 
Abstract: In mammalian and vertebrate genomes, the promoter regions of the gene and their distal enhancers may be located millions of base-pairs from each other, while a promoter may not interact with the closest enhancer. Since base-pair proximity is not a good indicator of these interactions, there is considerable work toward developing methods for predicting Enhancer-Promoter Interactions (EPI). Several machine learning methods have reported increasingly higher accuracies for predicting EPI. Typically, these approaches randomly split the dataset of Enhancer-Promoter (EP) pairs into training and testing subsets followed by model training. However, the aforementioned random splitting causes information leakage by assigning EP pairs from the same genomic region to both testing and training sets, leading to performance overestimation. In this paper we propose to use a more thorough training and testing paradigm i.e., Leave-one-chromosome-out (LOCO) cross-validation for EPI-prediction. We demonstrate that a deep learning algorithm, which gives higher accuracies when trained and tested on random-splitting setting, drops drastically in performance under LOCO setting, confirming overestimation of performance. We further propose a novel hybrid deep neural network for EPI-prediction that fuses k-mer features of the nucleotide sequence. We show that the hybrid architecture performs significantly better in the LOCO setting, demonstrating it can learn more generalizable aspects of EP interactions. With this paper we are also releasing the LOCO splitting-based EPI dataset. Research data is available in this public repository: https://github.com/malikmtahir/EPI</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2502.17022">Class-Dependent Perturbation Effects in Evaluating Time Series Attributions</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Gregor Baer, Isel Grau, Chao Zhang, Pieter Van Gorp</div>
        <div class="preprint-abstract">arXiv:2502.17022v2 Announce Type: replace 
Abstract: As machine learning models become increasingly prevalent in time series applications, Explainable Artificial Intelligence (XAI) methods are essential for understanding their predictions. Within XAI, feature attribution methods aim to identify which input features contribute the most to a model's prediction, with their evaluation typically relying on perturbation-based metrics. Through systematic empirical analysis across multiple datasets, model architectures, and perturbation strategies, we reveal previously overlooked class-dependent effects in these metrics: they show varying effectiveness across classes, achieving strong results for some while remaining less sensitive to others. In particular, we find that the most effective perturbation strategies often demonstrate the most pronounced class differences. Our analysis suggests that these effects arise from the learned biases of classifiers, indicating that perturbation-based evaluation may reflect specific model behaviors rather than intrinsic attribution quality. We propose an evaluation framework with a class-aware penalty term to help assess and account for these effects in evaluating feature attributions, offering particular value for class-imbalanced datasets. Although our analysis focuses on time series classification, these class-dependent effects likely extend to other structured data domains where perturbation-based evaluation is common.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.24326">Self-Supervised Pretraining for Aerial Road Extraction</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Rupert Polley, Sai Vignesh Abishek Deenadayalan, J. Marius Z\"ollner</div>
        <div class="preprint-abstract">arXiv:2503.24326v2 Announce Type: replace-cross 
Abstract: Deep neural networks for aerial image segmentation require large amounts of labeled data, but high-quality aerial datasets with precise annotations are scarce and costly to produce. To address this limitation, we propose a self-supervised pretraining method that improves segmentation performance while reducing reliance on labeled data. Our approach uses inpainting-based pretraining, where the model learns to reconstruct missing regions in aerial images, capturing their inherent structure before being fine-tuned for road extraction. This method improves generalization, enhances robustness to domain shifts, and is invariant to model architecture and dataset choice. Experiments show that our pretraining significantly boosts segmentation accuracy, especially in low-data regimes, making it a scalable solution for aerial image analysis.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00289">Do Chinese models speak Chinese languages?</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Andrea W Wen-Yi, Unso Eun Seo Jo, David Mimno</div>
        <div class="preprint-abstract">arXiv:2504.00289v1 Announce Type: cross 
Abstract: The release of top-performing open-weight LLMs has cemented China's role as a leading force in AI development. Do these models support languages spoken in China? Or do they speak the same languages as Western models? Comparing multilingual capabilities is important for two reasons. First, language ability provides insights into pre-training data curation, and thus into resource allocation and development priorities. Second, China has a long history of explicit language policy, varying between inclusivity of minority languages and a Mandarin-first policy. To test whether Chinese LLMs today reflect an agenda about China's languages, we test performance of Chinese and Western open-source LLMs on Asian regional and Chinese minority languages. Our experiments on Information Parity and reading comprehension show Chinese models' performance across these languages correlates strongly (r=0.93) with Western models', with the sole exception being better Mandarin. Sometimes, Chinese models cannot identify languages spoken by Chinese minorities such as Kazakh and Uyghur, even though they are good at French and German. These results provide a window into current development priorities, suggest options for future development, and indicate guidance for end users.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2311.14395">MSCMNet: Multi-scale Semantic Correlation Mining for Visible-Infrared Person Re-Identification</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Xuecheng Hua, Ke Cheng, Hu Lu, Juanjuan Tu, Yuanquan Wang, Shitong Wang</div>
        <div class="preprint-abstract">arXiv:2311.14395v2 Announce Type: replace 
Abstract: The main challenge in the Visible-Infrared Person Re-Identification (VI-ReID) task lies in how to extract discriminative features from different modalities for matching purposes. While the existing well works primarily focus on minimizing the modal discrepancies, the modality information can not thoroughly be leveraged. To solve this problem, a Multi-scale Semantic Correlation Mining network (MSCMNet) is proposed to comprehensively exploit semantic features at multiple scales and simultaneously reduce modality information loss as small as possible in feature extraction. The proposed network contains three novel components. Firstly, after taking into account the effective utilization of modality information, the Multi-scale Information Correlation Mining Block (MIMB) is designed to explore semantic correlations across multiple scales. Secondly, in order to enrich the semantic information that MIMB can utilize, a quadruple-stream feature extractor (QFE) with non-shared parameters is specifically designed to extract information from different dimensions of the dataset. Finally, the Quadruple Center Triplet Loss (QCT) is further proposed to address the information discrepancy in the comprehensive features. Extensive experiments on the SYSU-MM01, RegDB, and LLCM datasets demonstrate that the proposed MSCMNet achieves the greatest accuracy.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2410.06074">Scalable Mechanistic Neural Networks for Differential Equations and Machine Learning</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jiale Chen, Dingling Yao, Adeel Pervez, Dan Alistarh, Francesco Locatello</div>
        <div class="preprint-abstract">arXiv:2410.06074v3 Announce Type: replace 
Abstract: We propose Scalable Mechanistic Neural Network (S-MNN), an enhanced neural network framework designed for scientific machine learning applications involving long temporal sequences. By reformulating the original Mechanistic Neural Network (MNN) (Pervez et al., 2024), we reduce the computational time and space complexities from cubic and quadratic with respect to the sequence length, respectively, to linear. This significant improvement enables efficient modeling of long-term dynamics without sacrificing accuracy or interpretability. Extensive experiments demonstrate that S-MNN matches the original MNN in precision while substantially reducing computational resources. Consequently, S-MNN can drop-in replace the original MNN in applications, providing a practical and efficient tool for integrating mechanistic bottlenecks into neural network models of complex dynamical systems. Source code is available at https://github.com/IST-DASLab/ScalableMNN.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00180">Contradiction Detection in RAG Systems: Evaluating LLMs as Context Validators for Improved Information Consistency</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Vignesh Gokul, Srikanth Tenneti, Alwarappan Nakkiran</div>
        <div class="preprint-abstract">arXiv:2504.00180v1 Announce Type: cross 
Abstract: Retrieval Augmented Generation (RAG) systems have emerged as a powerful method for enhancing large language models (LLMs) with up-to-date information. However, the retrieval step in RAG can sometimes surface documents containing contradictory information, particularly in rapidly evolving domains such as news. These contradictions can significantly impact the performance of LLMs, leading to inconsistent or erroneous outputs. This study addresses this critical challenge in two ways. First, we present a novel data generation framework to simulate different types of contradictions that may occur in the retrieval stage of a RAG system. Second, we evaluate the robustness of different LLMs in performing as context validators, assessing their ability to detect contradictory information within retrieved document sets. Our experimental results reveal that context validation remains a challenging task even for state-of-the-art LLMs, with performance varying significantly across different types of contradictions. While larger models generally perform better at contradiction detection, the effectiveness of different prompting strategies varies across tasks and model architectures. We find that chain-of-thought prompting shows notable improvements for some models but may hinder performance in others, highlighting the complexity of the task and the need for more robust approaches to context validation in RAG systems.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.19605">Lean Formalization of Generalization Error Bound by Rademacher Complexity</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Sho Sonoda, Kazumi Kasaura, Yuma Mizuno, Kei Tsukamoto, Naoto Onda</div>
        <div class="preprint-abstract">arXiv:2503.19605v2 Announce Type: replace 
Abstract: We formalize the generalization error bound using Rademacher complexity in the Lean 4 theorem prover. Generalization error quantifies the gap between a learning machine's performance on given training data versus unseen test data, and Rademacher complexity serves as an estimate of this error based on the complexity of learning machines, or hypothesis class. Unlike traditional methods such as PAC learning and VC dimension, Rademacher complexity is applicable across diverse machine learning scenarios including deep learning and kernel methods. We formalize key concepts and theorems, including the empirical and population Rademacher complexities, and establish generalization error bounds through formal proofs of McDiarmid's inequality, Hoeffding's lemma, and symmetrization arguments.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00133">Data-driven Power Loss Identification through Physics-Based Thermal Model Backpropagation</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Mattia Scarpa, Francesco Pase, Ruggero Carli, Mattia Bruschetta, Franscesco Toso</div>
        <div class="preprint-abstract">arXiv:2504.00133v1 Announce Type: cross 
Abstract: Digital twins for power electronics require accurate power losses whose direct measurements are often impractical or impossible in real-world applications. This paper presents a novel hybrid framework that combines physics-based thermal modeling with data-driven techniques to identify and correct power losses accurately using only temperature measurements. Our approach leverages a cascaded architecture where a neural network learns to correct the outputs of a nominal power loss model by backpropagating through a reduced-order thermal model. We explore two neural architectures, a bootstrapped feedforward network, and a recurrent neural network, demonstrating that the bootstrapped feedforward approach achieves superior performance while maintaining computational efficiency for real-time applications. Between the interconnection, we included normalization strategies and physics-guided training loss functions to preserve stability and ensure physical consistency. Experimental results show that our hybrid model reduces both temperature estimation errors (from 7.2+-6.8{\deg}C to 0.3+-0.3{\deg}C) and power loss prediction errors (from 5.4+-6.6W to 0.2+-0.3W) compared to traditional physics-based approaches, even in the presence of thermal model uncertainties. This methodology allows us to accurately estimate power losses without direct measurements, making it particularly helpful for real-time industrial applications where sensor placement is hindered by cost and physical limitations.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00359">Discovering universal temperature regulation dynamics in animals</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Cody E. FitzGerald, Andrew J. Engedal, Niall M. Mangan</div>
        <div class="preprint-abstract">arXiv:2504.00359v1 Announce Type: new 
Abstract: Hibernation is an adaptation to extreme environmental seasonality that has been studied for almost 200 years, but our mechanistic understanding of the underlying physiological system remains lacking due to the partially observed nature of the system. During hibernation, small mammals, such as the Arctic ground squirrel, exhibit dramatic oscillations in body temperature, typically one of the only physiological states measured, of up to 40 $^{\circ}$C. These spikes are known as interbout arousals and typically occur 10-20 times throughout hibernation. The physiological mechanism that drives interbout arousals is unknown, but two distinct mechanisms have been hypothesized. Using model selection for partially observed systems, we are able to differentiate between these two mechanistic hypotheses using only body temperature data recorded from a free-ranging Arctic ground squirrel. We then modify our discovered physiological model of Arctic ground squirrel to include environmental information and find that we can qualitatively match body temperature data recorded from a wide range of species, including a bird, a shrew, and a bear, which also dynamically modulate body temperature. Our results suggest that a universal, environmentally sensitive mechanism could regulate body temperature across a diverse range of species -- a mechanistic restructuring of our current understanding of the physiological organization across species. While the findings presented here are applicable to thermophysiology, the general modeling procedure is applicable to time series data collected from partially observed biological, chemical, physical, mechanical, and cosmic systems for which the goal is to elucidate the underlying mechanism or control structure.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00019">ObscuraCoder: Powering Efficient Code LM Pre-Training Via Obfuscation Grounding</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Indraneil Paul, Haoyi Yang, Goran Glava\v{s}, Kristian Kersting, Iryna Gurevych</div>
        <div class="preprint-abstract">arXiv:2504.00019v1 Announce Type: cross 
Abstract: Language models (LMs) have become a staple of the code-writing toolbox. Their pre-training recipe has, however, remained stagnant over recent years, barring the occasional changes in data sourcing and filtering strategies. In particular, research exploring modifications to Code-LMs' pre-training objectives, geared towards improving data efficiency and better disentangling between syntax and semantics, has been noticeably sparse, especially compared with corresponding efforts in natural language LMs. In this work, we examine grounding on obfuscated code as a means of helping Code-LMs look beyond the surface-form syntax and enhance their pre-training sample efficiency. To this end, we compile ObscuraX, a dataset of approximately 55M source and obfuscated code pairs in seven languages. Subsequently, we pre-train ObscuraCoder models, ranging in size from 255M to 2.8B parameters, on a 272B-token corpus that includes ObscuraX and demonstrate that our obfuscation-based pre-training recipe leads to consistent improvements in Code-LMs' abilities compared to both vanilla autoregressive pre-training as well as existing de-obfuscation (DOBF) objectives. ObscuraCoder demonstrates sizeable gains across multiple tests of syntactic and semantic code understanding, along with improved capabilities in multilingual code completion, multilingual code commit summarization, and multi-purpose library-oriented code generation.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00232">Opportunistic Screening for Pancreatic Cancer using Computed Tomography Imaging and Radiology Reports</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">David Le, Ramon Correa-Medero, Amara Tariq, Bhavik Patel, Motoyo Yano, Imon Banerjee</div>
        <div class="preprint-abstract">arXiv:2504.00232v1 Announce Type: new 
Abstract: Pancreatic ductal adenocarcinoma (PDAC) is a highly aggressive cancer, with most cases diagnosed at stage IV and a five-year overall survival rate below 5%. Early detection and prognosis modeling are crucial for improving patient outcomes and guiding early intervention strategies. In this study, we developed and evaluated a deep learning fusion model that integrates radiology reports and CT imaging to predict PDAC risk. The model achieved a concordance index (C-index) of 0.6750 (95% CI: 0.6429, 0.7121) and 0.6435 (95% CI: 0.6055, 0.6789) on the internal and external dataset, respectively, for 5-year survival risk estimation. Kaplan-Meier analysis demonstrated significant separation (p<0.0001) between the low and high risk groups predicted by the fusion model. These findings highlight the potential of deep learning-based survival models in leveraging clinical and imaging data for pancreatic cancer.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00414">Multimodal LLMs for OCR, OCR Post-Correction, and Named Entity Recognition in Historical Documents</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Gavin Greif, Niclas Griesshaber, Robin Greif</div>
        <div class="preprint-abstract">arXiv:2504.00414v1 Announce Type: cross 
Abstract: We explore how multimodal Large Language Models (mLLMs) can help researchers transcribe historical documents, extract relevant historical information, and construct datasets from historical sources. Specifically, we investigate the capabilities of mLLMs in performing (1) Optical Character Recognition (OCR), (2) OCR Post-Correction, and (3) Named Entity Recognition (NER) tasks on a set of city directories published in German between 1754 and 1870. First, we benchmark the off-the-shelf transcription accuracy of both mLLMs and conventional OCR models. We find that the best-performing mLLM model significantly outperforms conventional state-of-the-art OCR models and other frontier mLLMs. Second, we are the first to introduce multimodal post-correction of OCR output using mLLMs. We find that this novel approach leads to a drastic improvement in transcription accuracy and consistently produces highly accurate transcriptions (<1% CER), without any image pre-processing or model fine-tuning. Third, we demonstrate that mLLMs can efficiently recognize entities in transcriptions of historical documents and parse them into structured dataset formats. Our findings provide early evidence for the long-term potential of mLLMs to introduce a paradigm shift in the approaches to historical data collection and document transcription.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00017">Enhance Vision-based Tactile Sensors via Dynamic Illumination and Image Fusion</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Artemii Redkin, Zdravko Dugonjic, Mike Lambeta, Roberto Calandra</div>
        <div class="preprint-abstract">arXiv:2504.00017v1 Announce Type: cross 
Abstract: Vision-based tactile sensors use structured light to measure deformation in their elastomeric interface. Until now, vision-based tactile sensors such as DIGIT and GelSight have been using a single, static pattern of structured light tuned to the specific form factor of the sensor. In this work, we investigate the effectiveness of dynamic illumination patterns, in conjunction with image fusion techniques, to improve the quality of sensing of vision-based tactile sensors. Specifically, we propose to capture multiple measurements, each with a different illumination pattern, and then fuse them together to obtain a single, higher-quality measurement. Experimental results demonstrate that this type of dynamic illumination yields significant improvements in image contrast, sharpness, and background difference. This discovery opens the possibility of retroactively improving the sensing quality of existing vision-based tactile sensors with a simple software update, and for new hardware designs capable of fully exploiting dynamic illumination.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00469">Learning-Based Approximate Nonlinear Model Predictive Control Motion Cueing</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Camilo Gonzalez Arango (Institute for Intelligent Systems Research and Innovation, Deakin University, Waurn Ponds, Victoria, 3216, Australia), Houshyar Asadi (Institute for Intelligent Systems Research and Innovation, Deakin University, Waurn Ponds, Victoria, 3216, Australia), Mohammad Reza Chalak Qazani (Sohar University, Sohar, 311, Oman), Chee Peng Lim (Swinburne University, Hawthorn, Victoria, 3122, Australia)</div>
        <div class="preprint-abstract">arXiv:2504.00469v1 Announce Type: cross 
Abstract: Motion Cueing Algorithms (MCAs) encode the movement of simulated vehicles into movement that can be reproduced with a motion simulator to provide a realistic driving experience within the capabilities of the machine. This paper introduces a novel learning-based MCA for serial robot-based motion simulators. Building on the differentiable predictive control framework, the proposed method merges the advantages of Nonlinear Model Predictive Control (NMPC) - notably nonlinear constraint handling and accurate kinematic modeling - with the computational efficiency of machine learning. By shifting the computational burden to offline training, the new algorithm enables real-time operation at high control rates, thus overcoming the key challenge associated with NMPC-based motion cueing. The proposed MCA incorporates a nonlinear joint-space plant model and a policy network trained to mimic NMPC behavior while accounting for joint acceleration, velocity, and position limits. Simulation experiments across multiple motion cueing scenarios showed that the proposed algorithm performed on par with a state-of-the-art NMPC-based alternative in terms of motion cueing quality as quantified by the RMSE and correlation coefficient with respect to reference signals. However, the proposed algorithm was on average 400 times faster than the NMPC baseline. In addition, the algorithm successfully generalized to unseen operating conditions, including motion cueing scenarios on a different vehicle and real-time physics-based simulations.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2501.13023">Provably-Safe Neural Network Training Using Hybrid Zonotope Reachability Analysis</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Long Kiu Chung, Shreyas Kousik</div>
        <div class="preprint-abstract">arXiv:2501.13023v2 Announce Type: replace 
Abstract: Even though neural networks are being increasingly deployed in safety-critical control applications, it remains difficult to enforce constraints on their output, meaning that it is hard to guarantee safety in such settings. While many existing methods seek to verify a neural network's satisfaction of safety constraints, few address how to correct an unsafe network. The handful of works that extract a training signal from verification cannot handle non-convex sets, and are either conservative or slow. To begin addressing these challenges, this work proposes a neural network training method that can encourage the exact image of a non-convex input set for a neural network with rectified linear unit (ReLU) nonlinearities to avoid a non-convex unsafe region. This is accomplished by reachability analysis with scaled hybrid zonotopes, a modification of the existing hybrid zonotope set representation that enables parameterized scaling of non-convex polytopic sets with a differentiable collision check via mixed-integer linear programs (MILPs). The proposed method was shown to be effective and fast for networks with up to 240 neurons, with the computational complexity dominated by inverse operations on matrices that scale linearly in size with the number of neurons and complexity of input and unsafe sets. We demonstrate the practicality of our method by training a forward-invariant neural network controller for a non-convex input set to an affine system, as well as generating safe reach-avoid plans for a black-box dynamical system.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00944">Diffusion-model approach to flavor models: A case study for $S_4^\prime$ modular flavor model</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Satsuki Nishimura, Hajime Otsuka, Haruki Uchiyama</div>
        <div class="preprint-abstract">arXiv:2504.00944v1 Announce Type: cross 
Abstract: We propose a numerical method of searching for parameters with experimental constraints in generic flavor models by utilizing diffusion models, which are classified as a type of generative artificial intelligence (generative AI). As a specific example, we consider the $S_4^\prime$ modular flavor model and construct a neural network that reproduces quark masses, the CKM matrix, and the Jarlskog invariant by treating free parameters in the flavor model as generating targets. By generating new parameters with the trained network, we find various phenomenologically interesting parameter regions where an analytical evaluation of the $S_4^\prime$ model is challenging. Additionally, we confirm that the spontaneous CP violation occurs in the $S_4^\prime$ model. The diffusion model enables an inverse problem approach, allowing the machine to provide a series of plausible model parameters from given experimental data. Moreover, it can serve as a versatile analytical tool for extracting new physical predictions from flavor models.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2411.12972">UniFlow: A Foundation Model for Unified Urban Spatio-Temporal Flow Prediction</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Yuan Yuan, Jingtao Ding, Chonghua Han, Zhi Sheng, Depeng Jin, Yong Li</div>
        <div class="preprint-abstract">arXiv:2411.12972v3 Announce Type: replace 
Abstract: Urban spatio-temporal flow prediction, encompassing traffic flows and crowd flows, is crucial for optimizing city infrastructure and managing traffic and emergency responses. Traditional approaches have relied on separate models tailored to either grid-based data, representing cities as uniform cells, or graph-based data, modeling cities as networks of nodes and edges. In this paper, we build UniFlow, a foundational model for general urban flow prediction that unifies both grid-based and graphbased data. We first design a multi-view spatio-temporal patching mechanism to standardize different data into a consistent sequential format and then introduce a spatio-temporal transformer architecture to capture complex correlations and dynamics. To leverage shared spatio-temporal patterns across different data types and facilitate effective cross-learning, we propose SpatioTemporal Memory Retrieval Augmentation (ST-MRA). By creating structured memory modules to store shared spatio-temporal patterns, ST-MRA enhances predictions through adaptive memory retrieval. Extensive experiments demonstrate that UniFlow outperforms existing models in both grid-based and graph-based flow prediction, excelling particularly in scenarios with limited data availability, showcasing its superior performance and broad applicability. The datasets and code implementation have been released on https://github.com/YuanYuan98/UniFlow.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2203.03906">Designing Heterogeneous GNNs with Desired Permutation Properties for Wireless Resource Allocation</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jianyu Zhao, Chenyang Yang, Tingting Liu</div>
        <div class="preprint-abstract">arXiv:2203.03906v3 Announce Type: replace 
Abstract: Graph neural networks (GNNs) have been designed for learning a variety of wireless policies, i.e., the mappings from environment parameters to decision variables, thanks to their superior performance, and the potential in enabling scalability and size generalizability. These merits are rooted in leveraging permutation prior, i.e., satisfying the permutation property of the policy to be learned (referred to as desired permutation property). Many wireless policies are with complicated permutation properties. To satisfy these properties, heterogeneous GNNs (HetGNNs) should be used to learn such policies. There are two critical factors that enable a HetGNN to satisfy a desired permutation property: constructing an appropriate heterogeneous graph and judiciously designing the architecture of the HetGNN. However, both the graph and the HetGNN are designed heuristically so far. In this paper, we strive to provide a systematic approach for the design to satisfy the desired permutation property. We first propose a method for constructing a graph for a policy, where the edges and their types are defined for the sake of satisfying complicated permutation properties. Then, we provide and prove three sufficient conditions to design a HetGNN such that it can satisfy the desired permutation property when learning over an appropriate graph. These conditions suggest a method of designing the HetGNN with desired permutation property by sharing the processing, combining, and pooling functions according to the types of vertices and edges of the graph. We take power allocation and hybrid precoding policies as examples for demonstrating how to apply the proposed methods and validating the impact of the permutation prior by simulations.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00707">Energy Weighted Learning Progress Guided Interleaved Multi-Task Learning</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Hanne Say (Graduate School of Science and Engineering, Ozyegin University, Istanbul, Turkey), Suzan Ece Ada (Department of Computer Engineering, Bogazici University, Istanbul, Turkey), Emre Ugur (Department of Computer Engineering, Bogazici University, Istanbul, Turkey), Erhan Oztop (Graduate School of Science and Engineering, Ozyegin University, Istanbul, Turkey, OTRI, SISREC, Osaka University, Osaka, Japan)</div>
        <div class="preprint-abstract">arXiv:2504.00707v1 Announce Type: cross 
Abstract: Humans can continuously acquire new skills and knowledge by exploiting existing ones for improved learning, without forgetting them. Similarly, 'continual learning' in machine learning aims to learn new information while preserving the previously acquired knowledge. Existing research often overlooks the nature of human learning, where tasks are interleaved due to human choice or environmental constraints. So, almost never do humans master one task before switching to the next. To investigate to what extent human-like learning can benefit the learner, we propose a method that interleaves tasks based on their 'learning progress' and energy consumption. From a machine learning perspective, our approach can be seen as a multi-task learning system that balances learning performance with energy constraints while mimicking ecologically realistic human task learning. To assess the validity of our approach, we consider a robot learning setting in simulation, where the robot learns the effect of its actions in different contexts. The conducted experiments show that our proposed method achieves better performance than sequential task learning and reduces energy consumption for learning the tasks.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.23001">Buyer-Initiated Auction Mechanism for Data Redemption in Machine Unlearning</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Bin Han, Di Feng, Jie Wang, Hans D. Schotten</div>
        <div class="preprint-abstract">arXiv:2503.23001v2 Announce Type: replace 
Abstract: The rapid growth of artificial intelligence (AI) has raised privacy concerns over user data, leading to regulations like the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA). With the essential toolbox provided by machine unlearning, AI service providers are now able to remove user data from their trained models as well as the training datasets, so as to comply with such regulations. However, extensive data redemption can be costly and degrade model accuracy. To balance the cost of unlearning and the privacy protection, we propose a buyer-initiated auction mechanism for data redemption, enabling the service provider to purchase data from willing users with appropriate compensation. This approach does not require the server to have any a priori knowledge about the users' privacy preference, and provides an efficient solution for maximizing the social welfare in the investigated problem.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2407.02994">MedPix 2.0: A Comprehensive Multimodal Biomedical Data set for Advanced AI Applications with Retrieval Augmented Generation and Knowledge Graphs</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Irene Siragusa, Salvatore Contino, Massimo La Ciura, Rosario Alicata, Roberto Pirrone</div>
        <div class="preprint-abstract">arXiv:2407.02994v2 Announce Type: cross 
Abstract: The increasing interest in developing Artificial Intelligence applications in the medical domain, suffers from the lack of high-quality data set, mainly due to privacy-related issues. In addition, the recent increase in large multimodal models (LMM) leads to the need for multimodal medical data sets, where clinical reports and findings are attached to the corresponding CT or MRI scans. This paper illustrates the entire workflow for building the MedPix 2.0 data set. Starting with the well-known multimodal data set MedPix\textsuperscript{\textregistered}, mainly used by physicians, nurses, and healthcare students for Continuing Medical Education purposes, a semi-automatic pipeline was developed to extract visual and textual data followed by a manual curing procedure in which noisy samples were removed, thus creating a MongoDB database. Along with the data set, we developed a GUI aimed at navigating efficiently the MongoDB instance and obtaining the raw data that can be easily used for training and/or fine-tuning LMMs. To enforce this point, in this work, we first recall DR-Minerva, a RAG-based LMM trained using MedPix 2.0. DR-Minerva predicts the body part and the modality used to scan its input image. We also propose the extension of DR-Minerva with a Knowledge Graph that uses Llama 3.1 Instruct 8B, and leverages MedPix 2.0. The resulting architecture can be queried in a end-to-end manner, as a medical decision support system. MedPix 2.0 is available on GitHub. \url{https://github.com/CHILab1/MedPix-2.0}</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00282">Federated Learning for Cross-Domain Data Privacy: A Distributed Approach to Secure Collaboration</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Yiwei Zhang, Jie Liu, Jiawei Wang, Lu Dai, Fan Guo, Guohui Cai</div>
        <div class="preprint-abstract">arXiv:2504.00282v1 Announce Type: new 
Abstract: This paper proposes a data privacy protection framework based on federated learning, which aims to realize effective cross-domain data collaboration under the premise of ensuring data privacy through distributed learning. Federated learning greatly reduces the risk of privacy breaches by training the model locally on each client and sharing only model parameters rather than raw data. The experiment verifies the high efficiency and privacy protection ability of federated learning under different data sources through the simulation of medical, financial, and user data. The results show that federated learning can not only maintain high model performance in a multi-domain data environment but also ensure effective protection of data privacy. The research in this paper provides a new technical path for cross-domain data collaboration and promotes the application of large-scale data analysis and machine learning while protecting privacy.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00957">Enabling Efficient Processing of Spiking Neural Networks with On-Chip Learning on Commodity Neuromorphic Processors for Edge AI Systems</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Rachmad Vidya Wicaksana Putra, Pasindu Wickramasinghe, Muhammad Shafique</div>
        <div class="preprint-abstract">arXiv:2504.00957v1 Announce Type: cross 
Abstract: The rising demand for energy-efficient edge AI systems (e.g., mobile agents/robots) has increased the interest in neuromorphic computing, since it offers ultra-low power/energy AI computation through spiking neural network (SNN) algorithms on neuromorphic processors. However, their efficient implementation strategy has not been comprehensively studied, hence limiting SNN deployments for edge AI systems. Toward this, we propose a design methodology to enable efficient SNN processing on commodity neuromorphic processors. To do this, we first study the key characteristics of targeted neuromorphic hardware (e.g., memory and compute budgets), and leverage this information to perform compatibility analysis for network selection. Afterward, we employ a mapping strategy for efficient SNN implementation on the targeted processor. Furthermore, we incorporate an efficient on-chip learning mechanism to update the systems' knowledge for adapting to new input classes and dynamic environments. The experimental results show that the proposed methodology leads the system to achieve low latency of inference (i.e., less than 50ms for image classification, less than 200ms for real-time object detection in video streaming, and less than 1ms in keyword recognition) and low latency of on-chip learning (i.e., less than 2ms for keyword recognition), while incurring less than 250mW of processing power and less than 15mJ of energy consumption across the respective different applications and scenarios. These results show the potential of the proposed methodology in enabling efficient edge AI systems for diverse application use-cases.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00125">LLMs for Explainable AI: A Comprehensive Survey</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Ahsan Bilal, David Ebert, Beiyu Lin</div>
        <div class="preprint-abstract">arXiv:2504.00125v1 Announce Type: new 
Abstract: Large Language Models (LLMs) offer a promising approach to enhancing Explainable AI (XAI) by transforming complex machine learning outputs into easy-to-understand narratives, making model predictions more accessible to users, and helping bridge the gap between sophisticated model behavior and human interpretability. AI models, such as state-of-the-art neural networks and deep learning models, are often seen as "black boxes" due to a lack of transparency. As users cannot fully understand how the models reach conclusions, users have difficulty trusting decisions from AI models, which leads to less effective decision-making processes, reduced accountabilities, and unclear potential biases. A challenge arises in developing explainable AI (XAI) models to gain users' trust and provide insights into how models generate their outputs. With the development of Large Language Models, we want to explore the possibilities of using human language-based models, LLMs, for model explainabilities. This survey provides a comprehensive overview of existing approaches regarding LLMs for XAI, and evaluation techniques for LLM-generated explanation, discusses the corresponding challenges and limitations, and examines real-world applications. Finally, we discuss future directions by emphasizing the need for more interpretable, automated, user-centric, and multidisciplinary approaches for XAI via LLMs.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00730">Detection of Disease on Nasal Breath Sound by New Lightweight Architecture: Using COVID-19 as An Example</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jiayuan She, Lin Shi, Peiqi Li, Ziling Dong, Renxing Li, Shengkai Li, Liping Gu, Tong Zhao, Zhuochang Yang, Yajie Ji, Liang Feng, Jiangang Chen</div>
        <div class="preprint-abstract">arXiv:2504.00730v1 Announce Type: new 
Abstract: Background. Infectious diseases, particularly COVID-19, continue to be a significant global health issue. Although many countries have reduced or stopped large-scale testing measures, the detection of such diseases remains a propriety. Objective. This study aims to develop a novel, lightweight deep neural network for efficient, accurate, and cost-effective detection of COVID-19 using a nasal breathing audio data collected via smartphones. Methodology. Nasal breathing audio from 128 patients diagnosed with the Omicron variant was collected. Mel-Frequency Cepstral Coefficients (MFCCs), a widely used feature in speech and sound analysis, were employed for extracting important characteristics from the audio signals. Additional feature selection was performed using Random Forest (RF) and Principal Component Analysis (PCA) for dimensionality reduction. A Dense-ReLU-Dropout model was trained with K-fold cross-validation (K=3), and performance metrics like accuracy, precision, recall, and F1-score were used to evaluate the model. Results. The proposed model achieved 97% accuracy in detecting COVID-19 from nasal breathing sounds, outperforming state-of-the-art methods such as those by [23] and [13]. Our Dense-ReLU-Dropout model, using RF and PCA for feature selection, achieves high accuracy with greater computational efficiency compared to existing methods that require more complex models or larger datasets. Conclusion. The findings suggest that the proposed method holds significant potential for clinical implementation, advancing smartphone-based diagnostics in infectious diseases. The Dense-ReLU-Dropout model, combined with innovative feature processing techniques, offers a promising approach for efficient and accurate COVID-19 detection, showcasing the capabilities of mobile device-based diagnostics</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00024">A multi-locus predictiveness curve and its summary assessment for genetic risk prediction</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Changshuai Wei, Ming Li, Yalu Wen, Chengyin Ye, Qing Lu</div>
        <div class="preprint-abstract">arXiv:2504.00024v1 Announce Type: cross 
Abstract: With the advance of high-throughput genotyping and sequencing technologies, it becomes feasible to comprehensive evaluate the role of massive genetic predictors in disease prediction. There exists, therefore, a critical need for developing appropriate statistical measurements to access the combined effects of these genetic variants in disease prediction. Predictiveness curve is commonly used as a graphical tool to measure the predictive ability of a risk prediction model on a single continuous biomarker. Yet, for most complex diseases, risk prediciton models are formed on multiple genetic variants. We therefore propose a multi-marker predictiveness curve and provide a non-parametric method to construct the curve for case-control studies. We further introduce a global predictiveness U and a partial predictiveness U to summarize prediction curve across the whole population and sub-population of clinical interest, respectively. We also demonstrate the connections of predictiveness curve with ROC curve and Lorenz curve. Through simulation, we compared the performance of the predictiveness U to other three summary indices: R square, Total Gain, and Average Entropy, and showed that Predictiveness U outperformed the other three indexes in terms of unbiasedness and robustness. Moreover, we simulated a series of rare-variants disease model, found partial predictiveness U performed better than global predictiveness U. Finally, we conducted a real data analysis, using predictiveness curve and predictiveness U to evaluate a risk prediction model for Nicotine Dependence.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2412.09078">Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Zhenni Bi, Kai Han, Chuanjian Liu, Yehui Tang, Yunhe Wang</div>
        <div class="preprint-abstract">arXiv:2412.09078v5 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable abilities across various language tasks, but solving complex reasoning problems remains a significant challenge. While existing methods, such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT), enhance reasoning by decomposing problems or structuring prompts, they typically perform a single pass of reasoning and may fail to revisit flawed paths, compromising accuracy. To address this limitation, we propose a novel reasoning framework called Forest-of-Thought (FoT), which integrates multiple reasoning trees to leverage collective decision-making for solving complex logical problems. FoT employs sparse activation strategies to select the most relevant reasoning paths, improving both efficiency and accuracy. Additionally, we introduce a dynamic self-correction strategy that enables real-time error correction, along with consensus-guided decision-making strategies to optimize both correctness and computational resources. Experimental results demonstrate that the FoT framework, combined with these strategies, significantly enhances the reasoning capabilities of LLMs, enabling them to solve complex tasks with greater precision and efficiency. Code will be available at https://github.com/iamhankai/Forest-of-Thought.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2406.17807">Enhancing Commentary Strategies for Imperfect Information Card Games: A Study of Large Language Models in Guandan Commentary</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Meiling Tao, Xuechen Liang, Ziyi Wang, Yiling Tao, Tianyu Shi</div>
        <div class="preprint-abstract">arXiv:2406.17807v4 Announce Type: replace-cross 
Abstract: Recent advancements in large language models (LLMs) have unlocked the potential for generating high-quality game commentary. However, producing insightful and engaging commentary for complex games with incomplete information remains a significant challenge. In this paper, we introduce a novel commentary method that combine Reinforcement Learning (RL) and LLMs, tailored specifically for the Chinese card game \textit{Guandan}. Our system leverages RL to generate intricate card-playing scenarios and employs LLMs to generate corresponding commentary text, effectively emulating the strategic analysis and narrative prowess of professional commentators. The framework comprises a state commentary guide, a Theory of Mind (ToM)-based strategy analyzer, and a style retrieval module, which seamlessly collaborate to deliver detailed and context-relevant game commentary in the Chinese language environment. We empower LLMs with ToM capabilities and refine both retrieval and information filtering mechanisms. This facilitates the generation of personalized commentary content. Our experimental results showcase the substantial enhancement in performance achieved by the proposed commentary framework when applied to open-source LLMs, surpassing the performance of GPT-4 across multiple evaluation metrics.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00511">Unifying Interpretations of Phase Transitions in the Vicsek Model: Correlation Length as a Diagnostic Tool</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Wenhao Yu, Zinuo Li, Liufang Xu</div>
        <div class="preprint-abstract">arXiv:2504.00511v1 Announce Type: cross 
Abstract: Vicsek Model is widely used in simulations of dry active matter. We re-examined two typical phase transitions in the original Vicsek model by using the velocity correlation length. One is the noise-driven disordered-to-ordered phase transition driven by noise, which was initially considered as a second-order transition (continuous transition), but was later demonstrated by Chate's detailed study to be a first-order transition. The other one is the disordered-to-ordered phase transition driven by average distance between particles, which is a second-order transition and satisfies the hyper-scaling relation of continuous transitions. We have discovered the change of correlation length during transition indicates a critical point in continuous transition while not in the discontinuous situation. We have also provided a method to classify phase transitions in active matter systems by using the correlation length and summarized previous work within the same framework. Finally, we end up with a potential application in experiments of bactirial swarms and robotic swarms. We hope our work paves the way for both theory and experiment development of active matter.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00472">Memorizing is Not Enough: Deep Knowledge Injection Through Reasoning</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Ruoxi Xu, Yunjie Ji, Boxi Cao, Yaojie Lu, Hongyu Lin, Xianpei Han, Ben He, Yingfei Sun, Xiangang Li, Le Sun</div>
        <div class="preprint-abstract">arXiv:2504.00472v1 Announce Type: cross 
Abstract: Although large language models (LLMs) excel in knowledge recall and reasoning, their static nature leads to outdated information as the real world evolves or when adapting to domain-specific knowledge, highlighting the need for effective knowledge injection. However, current research on knowledge injection remains superficial, mainly focusing on knowledge memorization and retrieval. This paper proposes a four-tier knowledge injection framework that systematically defines the levels of knowledge injection: memorization, retrieval, reasoning, and association. Based on this framework, we introduce DeepKnowledge, a synthetic experimental testbed designed for fine-grained evaluation of the depth of knowledge injection across three knowledge types (novel, incremental, and updated). We then explore various knowledge injection scenarios and evaluate the depth of knowledge injection for each scenario on the benchmark. Experimental results reveal key factors to reach each level of knowledge injection for LLMs and establish a mapping between the levels of knowledge injection and the corresponding suitable injection methods, aiming to provide a comprehensive approach for efficient knowledge injection across various levels.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2502.12226">On Creating a Causally Grounded Usable Rating Method for Assessing the Robustness of Foundation Models Supporting Time Series</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Kausik Lakkaraju, Rachneet Kaur, Parisa Zehtabi, Sunandita Patra, Siva Likitha Valluru, Zhen Zeng, Biplav Srivastava, Marco Valtorta</div>
        <div class="preprint-abstract">arXiv:2502.12226v2 Announce Type: replace 
Abstract: Foundation Models (FMs) have improved time series forecasting in various sectors, such as finance, but their vulnerability to input disturbances can hinder their adoption by stakeholders, such as investors and analysts. To address this, we propose a causally grounded rating framework to study the robustness of Foundational Models for Time Series (FMTS) with respect to input perturbations. We evaluate our approach to the stock price prediction problem, a well-studied problem with easily accessible public data, evaluating six state-of-the-art (some multi-modal) FMTS across six prominent stocks spanning three industries. The ratings proposed by our framework effectively assess the robustness of FMTS and also offer actionable insights for model selection and deployment. Within the scope of our study, we find that (1) multi-modal FMTS exhibit better robustness and accuracy compared to their uni-modal versions and, (2) FMTS pre-trained on time series forecasting task exhibit better robustness and forecasting accuracy compared to general-purpose FMTS pre-trained across diverse settings. Further, to validate our framework's usability, we conduct a user study showcasing FMTS prediction errors along with our computed ratings. The study confirmed that our ratings reduced the difficulty for users in comparing the robustness of different systems.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00608">PLM4NDV: Minimizing Data Access for Number of Distinct Values Estimation with Pre-trained Language Models</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Xianghong Xu, Xiao He, Tieying Zhang, Lei Zhang, Rui Shi, Jianjun Chen</div>
        <div class="preprint-abstract">arXiv:2504.00608v1 Announce Type: cross 
Abstract: Number of Distinct Values (NDV) estimation of a multiset/column is a basis for many data management tasks, especially within databases. Despite decades of research, most existing methods require either a significant amount of samples through uniform random sampling or access to the entire column to produce estimates, leading to substantial data access costs and potentially ineffective estimations in scenarios with limited data access. In this paper, we propose leveraging semantic information, i.e., schema, to address these challenges. The schema contains rich semantic information that can benefit the NDV estimation. To this end, we propose PLM4NDV, a learned method incorporating Pre-trained Language Models (PLMs) to extract semantic schema information for NDV estimation. Specifically, PLM4NDV leverages the semantics of the target column and the corresponding table to gain a comprehensive understanding of the column's meaning. By using the semantics, PLM4NDV reduces data access costs, provides accurate NDV estimation, and can even operate effectively without any data access. Extensive experiments on a large-scale real-world dataset demonstrate the superiority of PLM4NDV over baseline methods. Our code is available at https://github.com/bytedance/plm4ndv.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00174">MetaCLBench: Meta Continual Learning Benchmark on Resource-Constrained Edge Devices</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Sijia Li, Young D. Kwon, Lik-Hang Lee, Pan Hui</div>
        <div class="preprint-abstract">arXiv:2504.00174v1 Announce Type: new 
Abstract: Meta-Continual Learning (Meta-CL) has emerged as a promising approach to minimize manual labeling efforts and system resource requirements by enabling Continual Learning (CL) with limited labeled samples. However, while existing methods have shown success in image-based tasks, their effectiveness remains unexplored for sequential time-series data from sensor systems, particularly audio inputs. To address this gap, we conduct a comprehensive benchmark study evaluating six representative Meta-CL approaches using three network architectures on five datasets from both image and audio modalities. We develop MetaCLBench, an end-to-end Meta-CL benchmark framework for edge devices to evaluate system overheads and investigate trade-offs among performance, computational costs, and memory requirements across various Meta-CL methods. Our results reveal that while many Meta-CL methods enable to learn new classes for both image and audio modalities, they impose significant computational and memory costs on edge devices. Also, we find that pre-training and meta-training procedures based on source data before deployment improve Meta-CL performance. Finally, to facilitate further research, we provide practical guidelines for researchers and machine learning practitioners implementing Meta-CL on resource-constrained environments and make our benchmark framework and tools publicly available, enabling fair evaluation across both accuracy and system-level metrics.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00660">Learning to Normalize on the SPD Manifold under Bures-Wasserstein Geometry</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Rui Wang, Shaocheng Jin, Ziheng Chen, Xiaoqing Luo, Xiao-Jun Wu</div>
        <div class="preprint-abstract">arXiv:2504.00660v1 Announce Type: new 
Abstract: Covariance matrices have proven highly effective across many scientific fields. Since these matrices lie within the Symmetric Positive Definite (SPD) manifold - a Riemannian space with intrinsic non-Euclidean geometry, the primary challenge in representation learning is to respect this underlying geometric structure. Drawing inspiration from the success of Euclidean deep learning, researchers have developed neural networks on the SPD manifolds for more faithful covariance embedding learning. A notable advancement in this area is the implementation of Riemannian batch normalization (RBN), which has been shown to improve the performance of SPD network models. Nonetheless, the Riemannian metric beneath the existing RBN might fail to effectively deal with the ill-conditioned SPD matrices (ICSM), undermining the effectiveness of RBN. In contrast, the Bures-Wasserstein metric (BWM) demonstrates superior performance for ill-conditioning. In addition, the recently introduced Generalized BWM (GBWM) parameterizes the vanilla BWM via an SPD matrix, allowing for a more nuanced representation of vibrant geometries of the SPD manifold. Therefore, we propose a novel RBN algorithm based on the GBW geometry, incorporating a learnable metric parameter. Moreover, the deformation of GBWM by matrix power is also introduced to further enhance the representational capacity of GBWM-based RBN. Experimental results on different datasets validate the effectiveness of our proposed method.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00226">Large Language Models in Numberland: A Quick Test of Their Numerical Reasoning Abilities</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Roussel Rahman</div>
        <div class="preprint-abstract">arXiv:2504.00226v1 Announce Type: new 
Abstract: An essential element of human mathematical reasoning is our number sense -- an abstract understanding of numbers and their relationships -- which allows us to solve problems involving vast number spaces using limited computational resources. Mathematical reasoning of Large Language Models (LLMs) is often tested on high-level problems (such as Olympiad challenges, geometry, word problems, and puzzles), but their low-level number sense remains less explored. We introduce "Numberland," a 100-problem test to evaluate the numerical reasoning abilities of LLM-based agents. The tasks -- basic operations, advanced calculations (e.g., exponentiation, complex numbers), prime number checks, and the 24 game -- aim to test elementary skills and their integration in solving complex and uncertain problems. We evaluated five LLM-based agents: OpenAI's o1 and o1-mini, Google Gemini, Microsoft Copilot, and Anthropic Claude. They scored 74-95% on the first three tasks that allow deterministic steps to solutions. In the 24 game, which needs trial-and-error search, performance dropped to 10-73%. We tested the top 24 solver (o1 with 73% accuracy) on 25 harder problems, and its score fell to 27%, confirming search as a bottleneck. These results, along with the types of mistakes, suggest a fragile number of LLMs, which is a bit surprising given their prowess in challenging benchmarks. The limits of LLM numerical reasoning highlight the scope of simple, targeted tests to evaluate and explain LLM math skills to ensure safe use.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00059">ModelRadar: Aspect-based Forecast Evaluation</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Vitor Cerqueira, Luis Roque, Carlos Soares</div>
        <div class="preprint-abstract">arXiv:2504.00059v1 Announce Type: new 
Abstract: Accurate evaluation of forecasting models is essential for ensuring reliable predictions. Current practices for evaluating and comparing forecasting models focus on summarising performance into a single score, using metrics such as SMAPE. While convenient, averaging performance over all samples dilutes relevant information about model behavior under varying conditions. This limitation is especially problematic for time series forecasting, where multiple layers of averaging--across time steps, horizons, and multiple time series in a dataset--can mask relevant performance variations. We address this limitation by proposing ModelRadar, a framework for evaluating univariate time series forecasting models across multiple aspects, such as stationarity, presence of anomalies, or forecasting horizons. We demonstrate the advantages of this framework by comparing 24 forecasting methods, including classical approaches and different machine learning algorithms. NHITS, a state-of-the-art neural network architecture, performs best overall but its superiority varies with forecasting conditions. For instance, concerning the forecasting horizon, we found that NHITS (and also other neural networks) only outperforms classical approaches for multi-step ahead forecasting. Another relevant insight is that classical approaches such as ETS or Theta are notably more robust in the presence of anomalies. These and other findings highlight the importance of aspect-based model evaluation for both practitioners and researchers. ModelRadar is available as a Python package.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.01008">IntrinsiX: High-Quality PBR Generation using Image Priors</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Peter Kocsis (Technical University of Munich), Lukas H\"ollein (Technical University of Munich), Matthias Nie{\ss}ner (Technical University of Munich)</div>
        <div class="preprint-abstract">arXiv:2504.01008v1 Announce Type: cross 
Abstract: We introduce IntrinsiX, a novel method that generates high-quality intrinsic images from text description. In contrast to existing text-to-image models whose outputs contain baked-in scene lighting, our approach predicts physically-based rendering (PBR) maps. This enables the generated outputs to be used for content creation scenarios in core graphics applications that facilitate re-lighting, editing, and texture generation tasks. In order to train our generator, we exploit strong image priors, and pre-train separate models for each PBR material component (albedo, roughness, metallic, normals). We then align these models with a new cross-intrinsic attention formulation that concatenates key and value features in a consistent fashion. This allows us to exchange information between each output modality and to obtain semantically coherent PBR predictions. To ground each intrinsic component, we propose a rendering loss which provides image-space signals to constrain the model, thus facilitating sharp details also in the output BRDF properties. Our results demonstrate detailed intrinsic generation with strong generalization capabilities that outperforms existing intrinsic image decomposition methods used with generated images by a significant margin. Finally, we show a series of applications, including re-lighting, editing, and text-conditioned room-scale PBR texture generation.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.20964">Active Hydrodynamic Theory of Euchromatin and Heterochromatin</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">S. Alex Rautu, Alexandra Zidovska, David Saintillan, Michael J. Shelley</div>
        <div class="preprint-abstract">arXiv:2503.20964v2 Announce Type: replace-cross 
Abstract: The genome contains genetic information essential for cell's life. The genome's spatial organization inside the cell nucleus is critical for its proper function including gene regulation. The two major genomic compartments -- euchromatin and heterochromatin -- contain largely transcriptionally active and silenced genes, respectively, and exhibit distinct dynamics. In this work, we present a hydrodynamic framework that describes the large-scale behavior of euchromatin and heterochromatin, and accounts for the interplay of mechanical forces, active processes, and nuclear confinement. Our model shows contractile stresses from cross-linking proteins lead to the formation of heterochromatin droplets via mechanically driven phase separation. These droplets grow, coalesce, and in nuclear confinement, wet the boundary. Active processes, such as gene transcription in euchromatin, introduce non-equilibrium fluctuations that drive long-range, coherent motions of chromatin as well as the nucleoplasm, and thus alter the genome's spatial organization. These fluctuations also indirectly deform heterochromatin droplets, by continuously changing their shape. Taken together, our findings reveal how active forces, mechanical stresses and hydrodynamic flows contribute to the genome's organization at large scales and provide a physical framework for understanding chromatin organization and dynamics in live cells.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00930">CFIRE: A General Method for Combining Local Explanations</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Sebastian M\"uller, Vanessa Toborek, Tam\'as Horv\'ath, Christian Bauckhage</div>
        <div class="preprint-abstract">arXiv:2504.00930v1 Announce Type: new 
Abstract: We propose a novel eXplainable AI algorithm to compute faithful, easy-to-understand, and complete global decision rules from local explanations for tabular data by combining XAI methods with closed frequent itemset mining. Our method can be used with any local explainer that indicates which dimensions are important for a given sample for a given black-box decision. This property allows our algorithm to choose among different local explainers, addressing the disagreement problem, \ie the observation that no single explanation method consistently outperforms others across models and datasets. Unlike usual experimental methodology, our evaluation also accounts for the Rashomon effect in model explainability. To this end, we demonstrate the robustness of our approach in finding suitable rules for nearly all of the 700 black-box models we considered across 14 benchmark datasets. The results also show that our method exhibits improved runtime, high precision and F1-score while generating compact and complete rules.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00952">Personalized Federated Training of Diffusion Models with Privacy Guarantees</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Kumar Kshitij Patel, Weitong Zhang, Lingxiao Wang</div>
        <div class="preprint-abstract">arXiv:2504.00952v1 Announce Type: new 
Abstract: The scarcity of accessible, compliant, and ethically sourced data presents a considerable challenge to the adoption of artificial intelligence (AI) in sensitive fields like healthcare, finance, and biomedical research. Furthermore, access to unrestricted public datasets is increasingly constrained due to rising concerns over privacy, copyright, and competition. Synthetic data has emerged as a promising alternative, and diffusion models -- a cutting-edge generative AI technology -- provide an effective solution for generating high-quality and diverse synthetic data. In this paper, we introduce a novel federated learning framework for training diffusion models on decentralized private datasets. Our framework leverages personalization and the inherent noise in the forward diffusion process to produce high-quality samples while ensuring robust differential privacy guarantees. Our experiments show that our framework outperforms non-collaborative training methods, particularly in settings with high data heterogeneity, and effectively reduces biases and imbalances in synthetic data, resulting in fairer downstream models.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.02881">Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Han Xue, Jieji Ren, Wendi Chen, Gu Zhang, Yuan Fang, Guoying Gu, Huazhe Xu, Cewu Lu</div>
        <div class="preprint-abstract">arXiv:2503.02881v2 Announce Type: replace-cross 
Abstract: Humans can accomplish complex contact-rich tasks using vision and touch, with highly reactive capabilities such as quick adjustments to environmental changes and adaptive control of contact forces; however, this remains challenging for robots. Existing visual imitation learning (IL) approaches rely on action chunking to model complex behaviors, which lacks the ability to respond instantly to real-time tactile feedback during the chunk execution. Furthermore, most teleoperation systems struggle to provide fine-grained tactile / force feedback, which limits the range of tasks that can be performed. To address these challenges, we introduce TactAR, a low-cost teleoperation system that provides real-time tactile feedback through Augmented Reality (AR), along with Reactive Diffusion Policy (RDP), a novel slow-fast visual-tactile imitation learning algorithm for learning contact-rich manipulation skills. RDP employs a two-level hierarchy: (1) a slow latent diffusion policy for predicting high-level action chunks in latent space at low frequency, (2) a fast asymmetric tokenizer for closed-loop tactile feedback control at high frequency. This design enables both complex trajectory modeling and quick reactive behavior within a unified framework. Through extensive evaluation across three challenging contact-rich tasks, RDP significantly improves performance compared to state-of-the-art visual IL baselines through rapid response to tactile / force feedback. Furthermore, experiments show that RDP is applicable across different tactile / force sensors. Code and videos are available on https://reactive-diffusion-policy.github.io.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2412.11923">PICLe: Pseudo-Annotations for In-Context Learning in Low-Resource Named Entity Detection</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Sepideh Mamooler, Syrielle Montariol, Alexander Mathis, Antoine Bosselut</div>
        <div class="preprint-abstract">arXiv:2412.11923v2 Announce Type: replace-cross 
Abstract: In-context learning (ICL) enables Large Language Models (LLMs) to perform tasks using few demonstrations, facilitating task adaptation when labeled examples are hard to obtain. However, ICL is sensitive to the choice of demonstrations, and it remains unclear which demonstration attributes enable in-context generalization. In this work, we conduct a perturbation study of in-context demonstrations for low-resource Named Entity Detection (NED). Our surprising finding is that in-context demonstrations with partially correct annotated entity mentions can be as effective for task transfer as fully correct demonstrations. Based off our findings, we propose Pseudo-annotated In-Context Learning (PICLe), a framework for in-context learning with noisy, pseudo-annotated demonstrations. PICLe leverages LLMs to annotate many demonstrations in a zero-shot first pass. We then cluster these synthetic demonstrations, sample specific sets of in-context demonstrations from each cluster, and predict entity mentions using each set independently. Finally, we use self-verification to select the final set of entity mentions. We evaluate PICLe on five biomedical NED datasets and show that, with zero human annotation, PICLe outperforms ICL in low-resource settings where limited gold examples can be used as in-context demonstrations.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.01001">Zero-shot Benchmarking: A Framework for Flexible and Scalable Automatic Evaluation of Language Models</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jos\'e Pombal, Nuno M. Guerreiro, Ricardo Rei, Andr\'e F. T. Martins</div>
        <div class="preprint-abstract">arXiv:2504.01001v1 Announce Type: cross 
Abstract: As language models improve and become capable of performing more complex tasks across modalities, evaluating them automatically becomes increasingly challenging. Developing strong and robust task-specific automatic metrics gets harder, and human-annotated test sets -- which are expensive to create -- saturate more quickly. A compelling alternative is to design reliable strategies to automate the creation of test data and evaluation, but previous attempts either rely on pre-existing data, or focus solely on individual tasks. We present Zero-shot Benchmarking (ZSB), a framework for creating high-quality benchmarks for any task by leveraging language models for both synthetic test data creation and evaluation. ZSB is simple and flexible: it requires only the creation of a prompt for data generation and one for evaluation; it is scalable to tasks and languages where collecting real-world data is costly or impractical; it is model-agnostic, allowing the creation of increasingly challenging benchmarks as models improve. To assess the effectiveness of our framework, we create benchmarks for five text-only tasks and a multi-modal one: general capabilities in four languages (English, Chinese, French, and Korean), translation, and general vision-language capabilities in English. We then rank a broad range of open and closed systems on our benchmarks. ZSB rankings consistently correlate strongly with human rankings, outperforming widely-adopted standard benchmarks. Through ablations, we find that strong benchmarks can be created with open models, and that judge model size and dataset variety are crucial drivers of performance. We release all our benchmarks, and code to reproduce our experiments and to produce new benchmarks.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00204">RailGoerl24: G\"orlitz Rail Test Center CV Dataset 2024</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Rustam Tagiew (German Centre for Rail Traffic Research at the Federal Railway Authority), Ilkay Wunderlich (EYYES GmbH), Mark Sastuba (German Centre for Rail Traffic Research at the Federal Railway Authority), Steffen Seitz (Conrad Zuse School of Embedded Composite AI and the Chair of Fundamentals of Electrical Engineering of Dresden University of Technology)</div>
        <div class="preprint-abstract">arXiv:2504.00204v1 Announce Type: cross 
Abstract: Driverless train operation for open tracks on urban guided transport and mainline railways requires, among other things automatic detection of actual and potential obstacles, especially humans, in the danger zone of the train's path. Machine learning algorithms have proven to be powerful state-of-the-art tools for this task. However, these algorithms require large amounts of high-quality annotated data containing human beings in railway-specific environments as training data. Unfortunately, the amount of publicly available datasets is not yet sufficient and is significantly inferior to the datasets in the road domain. Therefore, this paper presents RailGoerl24, an on-board visual light Full HD camera dataset of 12205 frames recorded in a railway test center of T\"UV S\"UD Rail, in G\"orlitz, Germany. Its main purpose is to support the development of driverless train operation for guided transport. RailGoerl24 also includes a terrestrial LiDAR scan covering parts of the area used to acquire the RGB data. In addition to the raw data, the dataset contains 33556 boxwise annotations in total for the object class 'person'. The faces of recorded actors are not blurred or altered in any other way. RailGoerl24, soon available at data.fid-move.de/dataset/railgoerl24, can also be used for tasks beyond collision prediction.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2402.05928">Sharp Rates in Dependent Learning Theory: Avoiding Sample Size Deflation for the Square Loss</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Ingvar Ziemann, Stephen Tu, George J. Pappas, Nikolai Matni</div>
        <div class="preprint-abstract">arXiv:2402.05928v4 Announce Type: replace 
Abstract: In this work, we study statistical learning with dependent ($\beta$-mixing) data and square loss in a hypothesis class $\mathscr{F}\subset L_{\Psi_p}$ where $\Psi_p$ is the norm $\|f\|_{\Psi_p} \triangleq \sup_{m\geq 1} m^{-1/p} \|f\|_{L^m} $ for some $p\in [2,\infty]$. Our inquiry is motivated by the search for a sharp noise interaction term, or variance proxy, in learning with dependent data. Absent any realizability assumption, typical non-asymptotic results exhibit variance proxies that are deflated multiplicatively by the mixing time of the underlying covariates process. We show that whenever the topologies of $L^2$ and $\Psi_p$ are comparable on our hypothesis class $\mathscr{F}$ -- that is, $\mathscr{F}$ is a weakly sub-Gaussian class: $\|f\|_{\Psi_p} \lesssim \|f\|_{L^2}^\eta$ for some $\eta\in (0,1]$ -- the empirical risk minimizer achieves a rate that only depends on the complexity of the class and second order statistics in its leading term. Our result holds whether the problem is realizable or not and we refer to this as a \emph{near mixing-free rate}, since direct dependence on mixing is relegated to an additive higher order term. We arrive at our result by combining the above notion of a weakly sub-Gaussian class with mixed tail generic chaining. This combination allows us to compute sharp, instance-optimal rates for a wide range of problems. Examples that satisfy our framework include sub-Gaussian linear regression, more general smoothly parameterized function classes, finite hypothesis classes, and bounded smoothness classes.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00010">LayerCraft: Enhancing Text-to-Image Generation with CoT Reasoning and Layered Object Integration</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Yuyao Zhang, Jinghao Li, Yu-Wing Tai</div>
        <div class="preprint-abstract">arXiv:2504.00010v1 Announce Type: new 
Abstract: Text-to-image generation (T2I) has become a key area of research with broad applications. However, existing methods often struggle with complex spatial relationships and fine-grained control over multiple concepts. Many existing approaches require significant architectural modifications, extensive training, or expert-level prompt engineering. To address these challenges, we introduce \textbf{LayerCraft}, an automated framework that leverages large language models (LLMs) as autonomous agents for structured procedural generation. LayerCraft enables users to customize objects within an image and supports narrative-driven creation with minimal effort. At its core, the system includes a coordinator agent that directs the process, along with two specialized agents: \textbf{ChainArchitect}, which employs chain-of-thought (CoT) reasoning to generate a dependency-aware 3D layout for precise instance-level control, and the \textbf{Object-Integration Network (OIN)}, which utilizes LoRA fine-tuning on pre-trained T2I models to seamlessly blend objects into specified regions of an image based on textual prompts without requiring architectural changes. Extensive evaluations demonstrate LayerCraft's versatility in applications ranging from multi-concept customization to storytelling. By providing non-experts with intuitive, precise control over T2I generation, our framework democratizes creative image creation. Our code will be released upon acceptance at github.com/PeterYYZhang/LayerCraft</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00638">Impact of Data Duplication on Deep Neural Network-Based Image Classifiers: Robust vs. Standard Models</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Alireza Aghabagherloo, Aydin Abadi, Sumanta Sarkar, Vishnu Asutosh Dasu, Bart Preneel</div>
        <div class="preprint-abstract">arXiv:2504.00638v1 Announce Type: new 
Abstract: The accuracy and robustness of machine learning models against adversarial attacks are significantly influenced by factors such as training data quality, model architecture, the training process, and the deployment environment. In recent years, duplicated data in training sets, especially in language models, has attracted considerable attention. It has been shown that deduplication enhances both training performance and model accuracy in language models. While the importance of data quality in training image classifier Deep Neural Networks (DNNs) is widely recognized, the impact of duplicated images in the training set on model generalization and performance has received little attention.
  In this paper, we address this gap and provide a comprehensive study on the effect of duplicates in image classification. Our analysis indicates that the presence of duplicated images in the training set not only negatively affects the efficiency of model training but also may result in lower accuracy of the image classifier. This negative impact of duplication on accuracy is particularly evident when duplicated data is non-uniform across classes or when duplication, whether uniform or non-uniform, occurs in the training set of an adversarially trained model. Even when duplicated samples are selected in a uniform way, increasing the amount of duplication does not lead to a significant improvement in accuracy.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00009">Deep Learning-Based Hypoglycemia Classification Across Multiple Prediction Horizons</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Beyza Cinar, Jennifer Daniel Onwuchekwa, Maria Maleshkova</div>
        <div class="preprint-abstract">arXiv:2504.00009v1 Announce Type: cross 
Abstract: Type 1 diabetes (T1D) management can be significantly enhanced through the use of predictive machine learning (ML) algorithms, which can mitigate the risk of adverse events like hypoglycemia. Hypoglycemia, characterized by blood glucose levels below 70 mg/dL, is a life-threatening condition typically caused by excessive insulin administration, missed meals, or physical activity. Its asymptomatic nature impedes timely intervention, making ML models crucial for early detection. This study integrates short- (up to 2h) and long-term (up to 24h) prediction horizons (PHs) within a single classification model to enhance decision support. The predicted times are 5-15 min, 15-30 min, 30 min-1h, 1-2h, 2-4h, 4-8h, 8-12h, and 12-24h before hypoglycemia. In addition, a simplified model classifying up to 4h before hypoglycemia is compared. We trained ResNet and LSTM models on glucose levels, insulin doses, and acceleration data. The results demonstrate the superiority of the LSTM models when classifying nine classes. In particular, subject-specific models yielded better performance but achieved high recall only for classes 0, 1, and 2 with 98%, 72%, and 50%, respectively. A population-based six-class model improved the results with at least 60% of events detected. In contrast, longer PHs remain challenging with the current approach and may be considered with different models.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00836">Spingarn's Method and Progressive Decoupling Beyond Elicitable Monotonicity</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Brecht Evens, Puya Latafat, Panagiotis Patrinos</div>
        <div class="preprint-abstract">arXiv:2504.00836v1 Announce Type: cross 
Abstract: Spingarn's method of partial inverses and the progressive decoupling algorithm address inclusion problems involving the sum of an operator and the normal cone of a linear subspace, known as linkage problems. Despite their success, existing convergence results are limited to the so-called elicitable monotone setting, where nonmonotonicity is allowed only on the orthogonal complement of the linkage subspace. In this paper, we introduce progressive decoupling+, a generalized version of standard progressive decoupling that incorporates separate relaxation parameters for the linkage subspace and its orthogonal complement. We prove convergence under conditions that link the relaxation parameters to the nonmonotonicity of their respective subspaces and show that the special cases of Spingarn's method and standard progressive decoupling also extend beyond the elicitable monotone setting. Our analysis hinges upon an equivalence between progressive decoupling+ and the preconditioned proximal point algorithm, for which we develop a general local convergence analysis in a certain nonmonotone setting.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00027">Opioid Named Entity Recognition (ONER-2025) from Reddit</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Muhammad Ahmad, Humaira Farid, Iqra Ameer, Muhammad Muzamil, Ameer Hamza Muhammad Jalal, Ildar Batyrshin, Grigori Sidorov</div>
        <div class="preprint-abstract">arXiv:2504.00027v1 Announce Type: cross 
Abstract: The opioid overdose epidemic remains a critical public health crisis, particularly in the United States, leading to significant mortality and societal costs. Social media platforms like Reddit provide vast amounts of unstructured data that offer insights into public perceptions, discussions, and experiences related to opioid use. This study leverages Natural Language Processing (NLP), specifically Opioid Named Entity Recognition (ONER-2025), to extract actionable information from these platforms. Our research makes four key contributions. First, we created a unique, manually annotated dataset sourced from Reddit, where users share self-reported experiences of opioid use via different administration routes. This dataset contains 331,285 tokens and includes eight major opioid entity categories. Second, we detail our annotation process and guidelines while discussing the challenges of labeling the ONER-2025 dataset. Third, we analyze key linguistic challenges, including slang, ambiguity, fragmented sentences, and emotionally charged language, in opioid discussions. Fourth, we propose a real-time monitoring system to process streaming data from social media, healthcare records, and emergency services to identify overdose events. Using 5-fold cross-validation in 11 experiments, our system integrates machine learning, deep learning, and transformer-based language models with advanced contextual embeddings to enhance understanding. Our transformer-based models (bert-base-NER and roberta-base) achieved 97% accuracy and F1-score, outperforming baselines by 10.23% (RF=0.88).</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00411">Forward Learning with Differential Privacy</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Mingqian Feng, Zeliang Zhang, Jinyang Jiang, Yijie Peng, Chenliang Xu</div>
        <div class="preprint-abstract">arXiv:2504.00411v1 Announce Type: new 
Abstract: Differential privacy (DP) in deep learning is a critical concern as it ensures the confidentiality of training data while maintaining model utility. Existing DP training algorithms provide privacy guarantees by clipping and then injecting external noise into sample gradients computed by the backpropagation algorithm. Different from backpropagation, forward-learning algorithms based on perturbation inherently add noise during the forward pass and utilize randomness to estimate the gradients. Although these algorithms are non-privatized, the introduction of noise during the forward pass indirectly provides internal randomness protection to the model parameters and their gradients, suggesting the potential for naturally providing differential privacy. In this paper, we propose a \blue{privatized} forward-learning algorithm, Differential Private Unified Likelihood Ratio (DP-ULR), and demonstrate its differential privacy guarantees. DP-ULR features a novel batch sampling operation with rejection, of which we provide theoretical analysis in conjunction with classic differential privacy mechanisms. DP-ULR is also underpinned by a theoretically guided privacy controller that dynamically adjusts noise levels to manage privacy costs in each training step. Our experiments indicate that DP-ULR achieves competitive performance compared to traditional differential privacy training algorithms based on backpropagation, maintaining nearly the same privacy loss limits.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.09194">Addressing pitfalls in implicit unobserved confounding synthesis using explicit block hierarchical ancestral sampling</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Xudong Sun, Alex Markham, Pratik Misra, Carsten Marr</div>
        <div class="preprint-abstract">arXiv:2503.09194v2 Announce Type: replace-cross 
Abstract: Unbiased data synthesis is crucial for evaluating causal discovery algorithms in the presence of unobserved confounding, given the scarcity of real-world datasets. A common approach, implicit parameterization, encodes unobserved confounding by modifying the off-diagonal entries of the idiosyncratic covariance matrix while preserving positive definiteness. Within this approach, we identify that state-of-the-art protocols have two distinct issues that hinder unbiased sampling from the complete space of causal models: first, we give a detailed analysis of use of diagonally dominant constructions restricts the spectrum of partial correlation matrices; and second, the restriction of possible graphical structures when sampling bidirected edges, unnecessarily ruling out valid causal models. To address these limitations, we propose an improved explicit modeling approach for unobserved confounding, leveraging block-hierarchical ancestral generation of ground truth causal graphs. Algorithms for converting the ground truth DAG into ancestral graph is provided so that the output of causal discovery algorithms could be compared with. We draw connections between implicit and explicit parameterization, prove that our approach fully covers the space of causal models, including those generated by the implicit parameterization, thus enabling more robust evaluation of methods for causal discovery and inference.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.11937">Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Wonwoong Cho, Yan-Ying Chen, Matthew Klenk, David I. Inouye, Yanxia Zhang</div>
        <div class="preprint-abstract">arXiv:2503.11937v2 Announce Type: replace-cross 
Abstract: Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in generating high quality images. However, enabling precise control of continuous attributes, especially multiple attributes simultaneously, in a new domain (e.g., numeric values like eye openness or car width) with text-only guidance remains a significant challenge. To address this, we introduce the Attribute (Att) Adapter, a novel plug-and-play module designed to enable fine-grained, multi-attributes control in pretrained diffusion models. Our approach learns a single control adapter from a set of sample images that can be unpaired and contain multiple visual attributes. The Att-Adapter leverages the decoupled cross attention module to naturally harmonize the multiple domain attributes with text conditioning. We further introduce Conditional Variational Autoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the diverse nature of the visual world. Evaluations on two public datasets show that Att-Adapter outperforms all LoRA-based baselines in controlling continuous attributes. Additionally, our method enables a broader control range and also improves disentanglement across multiple attributes, surpassing StyleGAN-based techniques. Notably, Att-Adapter is flexible, requiring no paired synthetic data for training, and is easily scalable to multiple attributes within a single model.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00132">Contextualize-then-Aggregate: Circuits for In-Context Learning in Gemma-2 2B</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Aleksandra Bakalova, Yana Veitsman, Xinting Huang, Michael Hahn</div>
        <div class="preprint-abstract">arXiv:2504.00132v1 Announce Type: cross 
Abstract: In-Context Learning (ICL) is an intriguing ability of large language models (LLMs). Despite a substantial amount of work on its behavioral aspects and how it emerges in miniature setups, it remains unclear which mechanism assembles task information from the individual examples in a fewshot prompt. We use causal interventions to identify information flow in Gemma-2 2B for five naturalistic ICL tasks. We find that the model infers task information using a two-step strategy we call contextualize-then-aggregate: In the lower layers, the model builds up representations of individual fewshot examples, which are contextualized by preceding examples through connections between fewshot input and output tokens across the sequence. In the higher layers, these representations are aggregated to identify the task and prepare prediction of the next output. The importance of the contextualization step differs between tasks, and it may become more important in the presence of ambiguous examples. Overall, by providing rigorous causal analysis, our results shed light on the mechanisms through which ICL happens in language models.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00492">ParallelFlow: Parallelizing Linear Transformers via Flow Discretization</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Nicola Muca Cirone, Cristopher Salvi</div>
        <div class="preprint-abstract">arXiv:2504.00492v1 Announce Type: new 
Abstract: We present a theoretical framework for analyzing linear attention models through matrix-valued state space models (SSMs). Our approach, Parallel Flows, provides a perspective that systematically decouples temporal dynamics from implementation constraints, enabling independent analysis of critical algorithmic components: chunking, parallelization, and information aggregation. Central to this framework is the reinterpretation of chunking procedures as computations of the flows governing system dynamics. This connection establishes a bridge to mathematical tools from rough path theory, opening the door to new insights into sequence modeling architectures. As a concrete application, we analyze DeltaNet in a generalized low-rank setting motivated by recent theoretical advances. Our methods allow us to design simple, streamlined generalizations of hardware-efficient algorithms present in the literature, and to provide completely different ones, inspired by rough paths techniques, with provably lower complexity. This dual contribution demonstrates how principled theoretical analysis can both explain existing practical methods and inspire fundamentally new computational approaches.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00752">LLMs4SchemaDiscovery: A Human-in-the-Loop Workflow for Scientific Schema Mining with Large Language Models</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Sameer Sadruddin, Jennifer D'Souza, Eleni Poupaki, Alex Watkins, Hamed Babaei Giglou, Anisa Rula, Bora Karasulu, S\"oren Auer, Adrie Mackus, Erwin Kessels</div>
        <div class="preprint-abstract">arXiv:2504.00752v1 Announce Type: cross 
Abstract: Extracting structured information from unstructured text is crucial for modeling real-world processes, but traditional schema mining relies on semi-structured data, limiting scalability. This paper introduces schema-miner, a novel tool that combines large language models with human feedback to automate and refine schema extraction. Through an iterative workflow, it organizes properties from text, incorporates expert input, and integrates domain-specific ontologies for semantic depth. Applied to materials science--specifically atomic layer deposition--schema-miner demonstrates that expert-guided LLMs generate semantically rich schemas suitable for diverse real-world applications.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00254">ElaLoRA: Elastic & Learnable Low-Rank Adaptation for Efficient Model Fine-Tuning</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Huandong Chang, Zicheng Ma, Mingyuan Ma, Zhenting Qi, Andrew Sabot, Hong Jiang, H. T. Kung</div>
        <div class="preprint-abstract">arXiv:2504.00254v1 Announce Type: new 
Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted technique for fine-tuning large-scale pre-trained models with minimal parameter updates. However, existing methods rely on fixed ranks or focus solely on either rank pruning or expansion, failing to adapt ranks dynamically to match the importance of different layers during training. In this work, we propose ElaLoRA, an adaptive low-rank adaptation framework that dynamically prunes and expands ranks based on gradient-derived importance scores. To the best of our knowledge, ElaLoRA is the first method that enables both rank pruning and expansion during fine-tuning. Experiments across multiple benchmarks demonstrate that ElaLoRA consistently outperforms existing PEFT methods across different parameter budgets. Furthermore, our studies validate that layers receiving higher rank allocations contribute more significantly to model performance, providing theoretical justification for our adaptive strategy. By introducing a principled and adaptive rank allocation mechanism, ElaLoRA offers a scalable and efficient fine-tuning solution, particularly suited for resource-constrained environments.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2410.10114">Mixture of Experts Made Personalized: Federated Prompt Learning for Vision-Language Models</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jun Luo, Chen Chen, Shandong Wu</div>
        <div class="preprint-abstract">arXiv:2410.10114v4 Announce Type: replace 
Abstract: Federated prompt learning benefits federated learning with CLIP-like Vision-Language Model's (VLM's) robust representation learning ability through prompt learning. However, current federated prompt learning methods are habitually restricted to the traditional FL paradigm, where the participating clients are generally only allowed to download a single globally aggregated model from the server. While justifiable for training full-sized models under federated settings, in this work, we argue that this paradigm is ill-suited for lightweight prompts. By facilitating the clients to download multiple pre-aggregated prompts as fixed non-local experts, we propose Personalized Federated Mixture of Adaptive Prompts (pFedMoAP), a novel FL framework that personalizes the prompt learning process through the lens of Mixture of Experts (MoE). pFedMoAP implements a local attention-based gating network that learns to generate enhanced text features for better alignment with local image data, benefiting from both local and downloaded non-local adaptive prompt experts. Extensive experiments on 9 datasets under various federated settings demonstrate the efficacy of the proposed pFedMoAP algorithm. The code is available at https://github.com/ljaiverson/pFedMoAP.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00339">VNJPTranslate: A comprehensive pipeline for Vietnamese-Japanese translation</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Hoang Hai Phan, Nguyen Duc Minh Vu, Nam Dang Phuong</div>
        <div class="preprint-abstract">arXiv:2504.00339v1 Announce Type: cross 
Abstract: Neural Machine Translation (NMT) driven by Transformer architectures has advanced significantly, yet faces challenges with low-resource language pairs like Vietnamese-Japanese (Vi-Ja). Issues include sparse parallel data and handling linguistic/cultural nuances. Recent progress in Large Language Models (LLMs) with strong reasoning, often refined via Reinforcement Learning (RL), enables high-quality synthetic data generation. We introduce VNJPTranslate, a pipeline designed to systematically address the Vi-Ja translation task. It features a targeted data augmentation strategy using advanced LLMs with Chain-of-Thought prompting for challenging segments identified via corpus analysis. Subsequently, we employ efficient fine-tuning techniques (Unsloth with QLoRA) on a capable, low-parameter autoregressive model (specifically, a fine-tuned version of the 1.8B parameter Sailor model, which is based on the Qwen architecture) to create a practical and high-performing translation system. This integrated approach aims to improve Vi-Ja translation quality significantly over existing baselines.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00035">MiZero: The Shadowy Defender Against Text Style Infringements</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Ziwei Zhang, Juan Wen, Wanli Peng, Zhengxian Wu, Yinghan Zhou, Yiming Xue</div>
        <div class="preprint-abstract">arXiv:2504.00035v1 Announce Type: cross 
Abstract: In-Context Learning (ICL) and efficient fine-tuning methods significantly enhanced the efficiency of applying Large Language Models (LLMs) to downstream tasks. However, they also raise concerns about the imitation and infringement of personal creative data. Current methods for data copyright protection primarily focuses on content security but lacks effectiveness in protecting the copyrights of text styles. In this paper, we introduce a novel implicit zero-watermarking scheme, namely MiZero. This scheme establishes a precise watermark domain to protect the copyrighted style, surpassing traditional watermarking methods that distort the style characteristics. Specifically, we employ LLMs to extract condensed-lists utilizing the designed instance delimitation mechanism. These lists guide MiZero in generating the watermark. Extensive experiments demonstrate that MiZero effectively verifies text style copyright ownership against AI imitation.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00634">CNOT-Optimal Clifford Synthesis as SAT</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Irfansha Shaik, Jaco van de Pol</div>
        <div class="preprint-abstract">arXiv:2504.00634v1 Announce Type: cross 
Abstract: Clifford circuit optimization is an important step in the quantum compilation pipeline. Major compilers employ heuristic approaches. While they are fast, their results are often suboptimal. Minimization of noisy gates, like 2-qubit CNOT gates, is crucial for practical computing. Exact approaches have been proposed to fill the gap left by heuristic approaches. Among these are SAT based approaches that optimize gate count or depth, but they suffer from scalability issues. Further, they do not guarantee optimality on more important metrics like CNOT count or CNOT depth. A recent work proposed an exhaustive search only on Clifford circuits in a certain normal form to guarantee CNOT count optimality. But an exhaustive approach cannot scale beyond 6 qubits.
  In this paper, we incorporate search restricted to Clifford normal forms in a SAT encoding to guarantee CNOT count optimality. By allowing parallel plans, we propose a second SAT encoding that optimizes CNOT depth. By taking advantage of flexibility in SAT based approaches, we also handle connectivity restrictions in hardware platforms, and allow for qubit relabeling. We have implemented the above encodings and variations in our open source tool Q-Synth.
  In experiments, our encodings significantly outperform existing SAT approaches on random Clifford circuits. We consider practical VQE and Feynman benchmarks to compare with TKET and Qiskit compilers. In all-to-all connectivity, we observe reductions up to 32.1% in CNOT count and 48.1% in CNOT depth. Overall, we observe better results than TKET in the CNOT count and depth. We also experiment with connectivity restrictions of major quantum platforms. Compared to Qiskit, we observe up to 30.3% CNOT count and 35.9% CNOT depth further reduction.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00692">The HCI GenAI CO2ST Calculator: A Tool for Calculating the Carbon Footprint of Generative AI Use in Human-Computer Interaction Research</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Nanna Inie, Jeanette Falk, Raghavendra Selvan</div>
        <div class="preprint-abstract">arXiv:2504.00692v1 Announce Type: cross 
Abstract: Increased usage of generative AI (GenAI) in Human-Computer Interaction (HCI) research induces a climate impact from carbon emissions due to energy consumption of the hardware used to develop and run GenAI models and systems. The exact energy usage and and subsequent carbon emissions are difficult to estimate in HCI research because HCI researchers most often use cloud-based services where the hardware and its energy consumption are hidden from plain view. The HCI GenAI CO2ST Calculator is a tool designed specifically for the HCI research pipeline, to help researchers estimate the energy consumption and carbon footprint of using generative AI in their research, either a priori (allowing for mitigation strategies or experimental redesign) or post hoc (allowing for transparent documentation of carbon footprint in written reports of the research).</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.14538">Vision-Language Models for Acute Tuberculosis Diagnosis: A Multimodal Approach Combining Imaging and Clinical Data</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Ananya Ganapthy, Praveen Shastry, Naveen Kumarasami, Anandakumar D, Keerthana R, Mounigasri M, Varshinipriya M, Kishore Prasath Venkatesh, Bargava Subramanian, Kalyan Sivasailam</div>
        <div class="preprint-abstract">arXiv:2503.14538v3 Announce Type: replace-cross 
Abstract: Background: This study introduces a Vision-Language Model (VLM) leveraging SIGLIP and Gemma-3b architectures for automated acute tuberculosis (TB) screening. By integrating chest X-ray images and clinical notes, the model aims to enhance diagnostic accuracy and efficiency, particularly in resource-limited settings.
  Methods: The VLM combines visual data from chest X-rays with clinical context to generate detailed, context-aware diagnostic reports. The architecture employs SIGLIP for visual encoding and Gemma-3b for decoding, ensuring effective representation of acute TB-specific pathologies and clinical insights.
  Results: Key acute TB pathologies, including consolidation, cavities, and nodules, were detected with high precision (97percent) and recall (96percent). The model demonstrated strong spatial localization capabilities and robustness in distinguishing TB-positive cases, making it a reliable tool for acute TB diagnosis.
  Conclusion: The multimodal capability of the VLM reduces reliance on radiologists, providing a scalable solution for acute TB screening. Future work will focus on improving the detection of subtle pathologies and addressing dataset biases to enhance its generalizability and application in diverse global healthcare settings.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00063">The Axiom-Based Atlas: A Structural Mapping of Theorems via Foundational Proof Vectors</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Harim Yoo</div>
        <div class="preprint-abstract">arXiv:2504.00063v1 Announce Type: new 
Abstract: The Axiom-Based Atlas is a novel framework that structurally represents mathematical theorems as proof vectors over foundational axiom systems. By mapping the logical dependencies of theorems onto vectors indexed by axioms - such as those from Hilbert geometry, Peano arithmetic, or ZFC - we offer a new way to visualize, compare, and analyze mathematical knowledge. This vector-based formalism not only captures the logical foundation of theorems but also enables quantitative similarity metrics - such as cosine distance - between mathematical results, offering a new analytic layer for structural comparison. Using heatmaps, vector clustering, and AI-assisted modeling, this atlas enables the grouping of theorems by logical structure, not just by mathematical domain. We also introduce a prototype assistant (Atlas-GPT) that interprets natural language theorems and suggests likely proof vectors, supporting future applications in automated reasoning, mathematical education, and formal verification.
  This direction is partially inspired by Terence Tao's recent reflections on the convergence of symbolic and structural mathematics. The Axiom-Based Atlas aims to provide a scalable, interpretable model of mathematical reasoning that is both human-readable and AI-compatible, contributing to the future landscape of formal mathematical systems.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00721">Alleviating Performance Disparity in Adversarial Spatiotemporal Graph Learning Under Zero-Inflated Distribution</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Songran Bai, Yuheng Ji, Yue Liu, Xingwei Zhang, Xiaolong Zheng, Daniel Dajun Zeng</div>
        <div class="preprint-abstract">arXiv:2504.00721v1 Announce Type: new 
Abstract: Spatiotemporal Graph Learning (SGL) under Zero-Inflated Distribution (ZID) is crucial for urban risk management tasks, including crime prediction and traffic accident profiling. However, SGL models are vulnerable to adversarial attacks, compromising their practical utility. While adversarial training (AT) has been widely used to bolster model robustness, our study finds that traditional AT exacerbates performance disparities between majority and minority classes under ZID, potentially leading to irreparable losses due to underreporting critical risk events. In this paper, we first demonstrate the smaller top-k gradients and lower separability of minority class are key factors contributing to this disparity. To address these issues, we propose MinGRE, a framework for Minority Class Gradients and Representations Enhancement. MinGRE employs a multi-dimensional attention mechanism to reweight spatiotemporal gradients, minimizing the gradient distribution discrepancies across classes. Additionally, we introduce an uncertainty-guided contrastive loss to improve the inter-class separability and intra-class compactness of minority representations with higher uncertainty. Extensive experiments demonstrate that the MinGRE framework not only significantly reduces the performance disparity across classes but also achieves enhanced robustness compared to existing baselines. These findings underscore the potential of our method in fostering the development of more equitable and robust models.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00872">Bioelectrical Interfaces Beyond Cellular Excitability: Cancer, Aging, and Gene Expression Reprogramming</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Paolo Cadinu, Matthew Burgess, Catarina Franco Jones, Titouan Luciani, Marzia Iarossi, Manuel Schr\"oter, Nako Nakatsuka, Mustafa B. A. Djamgoz, Gil Gon\c{c}alves, Paola Sanju\'an-Alberte, Paula M. Mendes, Frankie J. Rawson, Malavika Nair, Michael Levin, Rosalia Moreddu</div>
        <div class="preprint-abstract">arXiv:2504.00872v1 Announce Type: cross 
Abstract: Bioelectrical interfaces represent a significant evolution in the intersection of nanotechnology and biophysics, offering new strategies for probing and influencing cellular processes. These systems capitalize on the subtle but powerful electric fields within living matter, potentially enabling applications beyond cellular excitability, ranging from targeted cancer therapies to interventions in genetic mechanisms and aging. This perspective article envisions the translation, development and application of next-generation solid-state bioelectrical interfaces and their transformative impact across several critical areas of medical research.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00062">Intermittent flow paths in biofilms grown in a microfluidic channel</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Kerem Bozkurt, Christoph Lohrmann, Felix Weinhardt, Daniel Hanke, Raphael Hopp, Robin Gerlach, Christian Holm, Holger Class</div>
        <div class="preprint-abstract">arXiv:2504.00062v1 Announce Type: cross 
Abstract: Biofilms exposed to flow experience shear stress, which leads to a competitive interaction between the growth and development of a biofilm and shearing. In this study, Pseudonomas fluorescene biofilm was grown in a microfluidic channel and exposed to forced flow of an aqueous solution of variable velocity. It can be observed that under certain conditions preferential flow paths form with a dynamic, but quasi-steady state interaction of growth, detachment, and re-attachment. We find that the regimes for preferential flow path development are determined by nutrient availability and the ratio of shear stress versus the biofilm's ability to resist shear forces. The intermittent regime of flow paths is mainly driven by the supply with nutrients, which we confirm by comparison with a numerical model based on coarse-grained molecular dynamics and Lattice Boltzmann hydrodynamics.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2312.01020">ResNLS: An Improved Model for Stock Price Forecasting</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Yuanzhe Jia, Ali Anaissi, Basem Suleiman</div>
        <div class="preprint-abstract">arXiv:2312.01020v2 Announce Type: replace 
Abstract: Stock prices forecasting has always been a challenging task. Although many research projects try to address the problem, few of them pay attention to the varying degrees of dependencies between stock prices. In this paper, we introduce a hybrid model that improves the prediction of stock prices by emphasizing the dependencies between adjacent stock prices. The proposed model, ResNLS, is mainly composed of two neural architectures, ResNet and LSTM. ResNet serves as a feature extractor to identify dependencies between stock prices, while LSTM analyzes the initial time series data with the combination of dependencies, which are considered as residuals. Our experiment reveals that when the closing price data for the previous 5 consecutive trading days is used as input, the performance of the model (ResNLS-5) is optimal compared to those with other inputs. Furthermore, ResNLS-5 demonstrates at least a 20% improvement over current state-of-the-art baselines. To verify whether ResNLS-5 can help clients effectively avoid risks and earn profits in the stock market, we construct a quantitative trading framework for back testing. The result shows that the trading strategy based on ResNLS-5 predictions can successfully mitigate losses during declining stock prices and generate profits in periods of rising stock prices. The relevant code is publicly available on GitHub.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00649">Universal Structure of Computing Moments for Exact Quantum Dynamics: Application to Arbitrary System-Bath Couplings</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Rui-Hao Bi, Wei Liu, Wenjie Dou</div>
        <div class="preprint-abstract">arXiv:2504.00649v1 Announce Type: cross 
Abstract: We introduce a general procedure for computing higher-order moments of correlation functions in open quantum systems, extending the scope of our recent work on Memory Kernel Coupling Theory (MKCT) [W. Liu, Y. Su, Y. Wang, and W. Dou, arXiv:2407.01923 (2024)]. This approach is demonstrated for arbitrary system-bath coupling that can be expressed as polynomial, $H_{SB} = \hat{V} (\alpha_0 + \alpha_1 \hat{q} + \alpha_2 \hat{q}^2+ \dots)$, where we show that the recursive commutators of a system operator obey a universal hierarchy. Exploiting this structure, the higher-order moments are obtained by evaluating the expectation values of the system and bath operators separately, with bath expectation values derived from the derivatives of a generating function. We further apply MKCT to compute the dipole autocorrelation function for the spin-boson model with both linear and quadratic coupling, achieving agreement with the hierarchical equations of motion approach. Our findings suggest a promising path toward accurate dynamics for complex open quantum systems.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00983">WorldScore: A Unified Evaluation Benchmark for World Generation</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Haoyi Duan, Hong-Xing Yu, Sirui Chen, Li Fei-Fei, Jiajun Wu</div>
        <div class="preprint-abstract">arXiv:2504.00983v1 Announce Type: cross 
Abstract: We introduce the WorldScore benchmark, the first unified benchmark for world generation. We decompose world generation into a sequence of next-scene generation tasks with explicit camera trajectory-based layout specifications, enabling unified evaluation of diverse approaches from 3D and 4D scene generation to video generation models. The WorldScore benchmark encompasses a curated dataset of 3,000 test examples that span diverse worlds: static and dynamic, indoor and outdoor, photorealistic and stylized. The WorldScore metrics evaluate generated worlds through three key aspects: controllability, quality, and dynamics. Through extensive evaluation of 19 representative models, including both open-source and closed-source ones, we reveal key insights and challenges for each category of models. Our dataset, evaluation code, and leaderboard can be found at https://haoyi-duan.github.io/WorldScore/</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.19823">GyralNet Subnetwork Partitioning via Differentiable Spectral Modularity Optimization</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Yan Zhuang, Minheng Chen, Chao Cao, Tong Chen, Jing Zhang, Xiaowei Yu, Yanjun Lyu, Lu Zhang, Tianming Liu, Dajiang Zhu</div>
        <div class="preprint-abstract">arXiv:2503.19823v2 Announce Type: replace-cross 
Abstract: Understanding the structural and functional organization of the human brain requires a detailed examination of cortical folding patterns, among which the three-hinge gyrus (3HG) has been identified as a key structural landmark. GyralNet, a network representation of cortical folding, models 3HGs as nodes and gyral crests as edges, highlighting their role as critical hubs in cortico-cortical connectivity. However, existing methods for analyzing 3HGs face significant challenges, including the sub-voxel scale of 3HGs at typical neuroimaging resolutions, the computational complexity of establishing cross-subject correspondences, and the oversimplification of treating 3HGs as independent nodes without considering their community-level relationships. To address these limitations, we propose a fully differentiable subnetwork partitioning framework that employs a spectral modularity maximization optimization strategy to modularize the organization of 3HGs within GyralNet. By incorporating topological structural similarity and DTI-derived connectivity patterns as attribute features, our approach provides a biologically meaningful representation of cortical organization. Extensive experiments on the Human Connectome Project (HCP) dataset demonstrate that our method effectively partitions GyralNet at the individual level while preserving the community-level consistency of 3HGs across subjects, offering a robust foundation for understanding brain connectivity.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00712">Spectral Normalization and Voigt-Reuss net: A universal approach to microstructure-property forecasting with physical guarantees</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Sanath Keshav, Julius Herb, Felix Fritzen</div>
        <div class="preprint-abstract">arXiv:2504.00712v1 Announce Type: new 
Abstract: Heterogeneous materials are crucial to producing lightweight components, functional components, and structures composed of them. A crucial step in the design process is the rapid evaluation of their effective mechanical, thermal, or, in general, constitutive properties. The established procedure is to use forward models that accept microstructure geometry and local constitutive properties as inputs. The classical simulation-based approach, which uses, e.g., finite elements and FFT-based solvers, can require substantial computational resources. At the same time, simulation-based models struggle to provide gradients with respect to the microstructure and the constitutive parameters. Such gradients are, however, of paramount importance for microstructure design and for inverting the microstructure-property mapping. Machine learning surrogates can excel in these situations. However, they can lead to unphysical predictions that violate essential bounds on the constitutive response, such as the upper (Voigt-like) or the lower (Reuss-like) bound in linear elasticity. Therefore, we propose a novel spectral normalization scheme that a priori enforces these bounds. The approach is fully agnostic with respect to the chosen microstructural features and the utilized surrogate model. All of these will automatically and strictly predict outputs that obey the upper and lower bounds by construction. The technique can be used for any constitutive tensor that is symmetric and where upper and lower bounds (in the L\"owner sense) exist, i.e., for permeability, thermal conductivity, linear elasticity, and many more. We demonstrate the use of spectral normalization in the Voigt-Reuss net using a simple neural network. Numerical examples on truly extensive datasets illustrate the improved accuracy, robustness, and independence of the type of input features in comparison to much-used neural networks.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00948">QSViT: A Methodology for Quantizing Spiking Vision Transformers</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Rachmad Vidya Wicaksana Putra, Saad Iftikhar, Muhammad Shafique</div>
        <div class="preprint-abstract">arXiv:2504.00948v1 Announce Type: cross 
Abstract: Vision Transformer (ViT)-based models have shown state-of-the-art performance (e.g., accuracy) in vision-based AI tasks. However, realizing their capability in resource-constrained embedded AI systems is challenging due to their inherent large memory footprints and complex computations, thereby incurring high power/energy consumption. Recently, Spiking Vision Transformer (SViT)-based models have emerged as alternate low-power ViT networks. However, their large memory footprints still hinder their applicability for resource-constrained embedded AI systems. Therefore, there is a need for a methodology to compress SViT models without degrading the accuracy significantly. To address this, we propose QSViT, a novel design methodology to compress the SViT models through a systematic quantization strategy across different network layers. To do this, our QSViT employs several key steps: (1) investigating the impact of different precision levels in different network layers, (2) identifying the appropriate base quantization settings for guiding bit precision reduction, (3) performing a guided quantization strategy based on the base settings to select the appropriate quantization setting, and (4) developing an efficient quantized network based on the selected quantization setting. The experimental results demonstrate that, our QSViT methodology achieves 22.75% memory saving and 21.33% power saving, while also maintaining high accuracy within 2.1% from that of the original non-quantized SViT model on the ImageNet dataset. These results highlight the potential of QSViT methodology to pave the way toward the efficient SViT deployments on resource-constrained embedded AI systems.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00890">Privacy-Preserving Transfer Learning for Community Detection using Locally Distributed Multiple Networks</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Xiao Guo, Xuming He, Xiangyu Chang, Shujie Ma</div>
        <div class="preprint-abstract">arXiv:2504.00890v1 Announce Type: cross 
Abstract: This paper develops a new spectral clustering-based method called TransNet for transfer learning in community detection of network data. Our goal is to improve the clustering performance of the target network using auxiliary source networks, which are heterogeneous, privacy-preserved, and locally stored across various sources. The edges of each locally stored network are perturbed using the randomized response mechanism to achieve differential privacy. Notably, we allow the source networks to have distinct privacy-preserving and heterogeneity levels as often desired in practice. To better utilize the information from the source networks, we propose a novel adaptive weighting method to aggregate the eigenspaces of the source networks multiplied by adaptive weights chosen to incorporate the effects of privacy and heterogeneity. We propose a regularization method that combines the weighted average eigenspace of the source networks with the eigenspace of the target network to achieve an optimal balance between them. Theoretically, we show that the adaptive weighting method enjoys the error-bound-oracle property in the sense that the error bound of the estimated eigenspace only depends on informative source networks. We also demonstrate that TransNet performs better than the estimator using only the target network and the estimator using only the weighted source networks.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2410.12049">Sabi\'a-3 Technical Report</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Hugo Abonizio, Thales Sales Almeida, Thiago Laitz, Roseval Malaquias Junior, Giovana Kerche Bon\'as, Rodrigo Nogueira, Ramon Pires</div>
        <div class="preprint-abstract">arXiv:2410.12049v4 Announce Type: replace-cross 
Abstract: This report presents Sabi\'a-3, our new flagship language model, and Sabiazinho-3, a more cost-effective sibling. The models were trained on a large brazilian-centric corpus. Evaluations across diverse professional and academic benchmarks show a strong performance on Portuguese and Brazil-related tasks. Sabi\'a-3 shows large improvements in comparison to our previous best of model, Sabia-2 Medium, especially in reasoning-intensive tasks. Notably, Sabi\'a-3's average performance matches frontier LLMs, while it is offered at a three to four times lower cost per token, reinforcing the benefits of domain specialization.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00709">Science Autonomy using Machine Learning for Astrobiology</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Victoria Da Poian, Bethany Theiling, Eric Lyness, David Burtt, Abigail R. Azari, Joey Pasterski, Luoth Chou, Melissa Trainer, Ryan Danell, Desmond Kaplan, Xiang Li, Lily Clough, Brett McKinney, Lukas Mandrake, Bill Diamond, Caroline Freissinet</div>
        <div class="preprint-abstract">arXiv:2504.00709v1 Announce Type: cross 
Abstract: In recent decades, artificial intelligence (AI) including machine learning (ML) have become vital for space missions enabling rapid data processing, advanced pattern recognition, and enhanced insight extraction. These tools are especially valuable in astrobiology applications, where models must distinguish biotic patterns from complex abiotic backgrounds. Advancing the integration of autonomy through AI and ML into space missions is a complex challenge, and we believe that by focusing on key areas, we can make significant progress and offer practical recommendations for tackling these obstacles.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00857">Exploring Personalized Federated Learning Architectures for Violence Detection in Surveillance Videos</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Mohammad Kassir, Siba Haidar, Antoun Yaacoub</div>
        <div class="preprint-abstract">arXiv:2504.00857v1 Announce Type: cross 
Abstract: The challenge of detecting violent incidents in urban surveillance systems is compounded by the voluminous and diverse nature of video data. This paper presents a targeted approach using Personalized Federated Learning (PFL) to address these issues, specifically employing the Federated Learning with Personalization Layers method within the Flower framework. Our methodology adapts learning models to the unique data characteristics of each surveillance node, effectively managing the heterogeneous and non-IID nature of surveillance video data. Through rigorous experiments conducted on balanced and imbalanced datasets, our PFL models demonstrated enhanced accuracy and efficiency, achieving up to 99.3% accuracy. This study underscores the potential of PFL to significantly improve the scalability and effectiveness of surveillance systems, offering a robust, privacy-preserving solution for violence detection in complex urban environments.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2411.13543">BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Davide Paglieri, Bart{\l}omiej Cupia{\l}, Samuel Coward, Ulyana Piterbarg, Maciej Wolczyk, Akbir Khan, Eduardo Pignatelli, {\L}ukasz Kuci\'nski, Lerrel Pinto, Rob Fergus, Jakob Nicolaus Foerster, Jack Parker-Holder, Tim Rockt\"aschel</div>
        <div class="preprint-abstract">arXiv:2411.13543v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) and Vision Language Models (VLMs) possess extensive knowledge and exhibit promising reasoning abilities, however, they still struggle to perform well in complex, dynamic environments. Real-world tasks require handling intricate interactions, advanced spatial reasoning, long-term planning, and continuous exploration of new strategies-areas in which we lack effective methodologies for comprehensively evaluating these capabilities. To address this gap, we introduce BALROG, a novel benchmark designed to assess the agentic capabilities of LLMs and VLMs through a diverse set of challenging games. Our benchmark incorporates a range of existing reinforcement learning environments with varying levels of difficulty, including tasks that are solvable by non-expert humans in seconds to extremely challenging ones that may take years to master (e.g., the NetHack Learning Environment). We devise fine-grained metrics to measure performance and conduct an extensive evaluation of several popular open-source and closed-source LLMs and VLMs. Our findings indicate that while current models achieve partial success in the easier games, they struggle significantly with more challenging tasks. Notably, we observe severe deficiencies in vision-based decision-making, as several models perform worse when visual representations of the environments are provided. We release BALROG as an open and user-friendly benchmark to facilitate future research and development in the agentic community. Code and Leaderboard at balrogai.com.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.18258">Severing Spurious Correlations with Data Pruning</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Varun Mulchandani, Jung-Eun Kim</div>
        <div class="preprint-abstract">arXiv:2503.18258v2 Announce Type: replace 
Abstract: Deep neural networks have been shown to learn and rely on spurious correlations present in the data that they are trained on. Reliance on such correlations can cause these networks to malfunction when deployed in the real world, where these correlations may no longer hold. To overcome the learning of and reliance on such correlations, recent studies propose approaches that yield promising results. These works, however, study settings where the strength of the spurious signal is significantly greater than that of the core, invariant signal, making it easier to detect the presence of spurious features in individual training samples and allow for further processing. In this paper, we identify new settings where the strength of the spurious signal is relatively weaker, making it difficult to detect any spurious information while continuing to have catastrophic consequences. We also discover that spurious correlations are learned primarily due to only a handful of all the samples containing the spurious feature and develop a novel data pruning technique that identifies and prunes small subsets of the training data that contain these samples. Our proposed technique does not require inferred domain knowledge, information regarding the sample-wise presence or nature of spurious information, or human intervention. Finally, we show that such data pruning attains state-of-the-art performance on previously studied settings where spurious information is identifiable.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.23668">MolGround: A Benchmark for Molecular Grounding</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jiaxin Wu, Ting Zhang, Rubing Chen, Wengyu Zhang, Chen Jason Zhang, Xiaoyong Wei, Li Qing</div>
        <div class="preprint-abstract">arXiv:2503.23668v2 Announce Type: replace 
Abstract: Current molecular understanding approaches predominantly focus on the descriptive aspect of human perception, providing broad, topic-level insights. However, the referential aspect -- linking molecular concepts to specific structural components -- remains largely unexplored. To address this gap, we propose a molecular grounding benchmark designed to evaluate a model's referential abilities. We align molecular grounding with established conventions in NLP, cheminformatics, and molecular science, showcasing the potential of NLP techniques to advance molecular understanding within the AI for Science movement. Furthermore, we constructed the largest molecular understanding benchmark to date, comprising 79k QA pairs, and developed a multi-agent grounding prototype as proof of concept. This system outperforms existing models, including GPT-4o, and its grounding outputs have been integrated to enhance traditional tasks such as molecular captioning and ATC (Anatomical, Therapeutic, Chemical) classification.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.13444">VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Ye Liu, Kevin Qinghong Lin, Chang Wen Chen, Mike Zheng Shou</div>
        <div class="preprint-abstract">arXiv:2503.13444v2 Announce Type: replace-cross 
Abstract: Videos, with their unique temporal dimension, demand precise grounded understanding, where answers are directly linked to visual, interpretable evidence. Despite significant breakthroughs in reasoning capabilities within Large Language Models, multi-modal reasoning - especially for videos - remains unexplored. In this work, we introduce VideoMind, a novel video-language agent designed for temporal-grounded video understanding. VideoMind incorporates two key innovations: (i) We identify essential capabilities for video temporal reasoning and develop a role-based agentic workflow, including a planner for coordinating different roles, a grounder for temporal localization, a verifier to assess temporal interval accuracy, and an answerer for question-answering. (ii) To efficiently integrate these diverse roles, we propose a novel Chain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA adaptors while avoiding the overhead of multiple models, thus balancing efficiency and flexibility. Extensive experiments on 14 public benchmarks, including 3 on grounded video question-answering (Grounded VideoQA), 6 on video temporal grounding (VTG), and 5 on general video question-answering (VideoQA), verify that our agent achieves state-of-the-art performance on diverse video understanding tasks, underscoring its effectiveness in advancing video agent and long-form temporal reasoning.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2408.16393">Illuminating the Diversity-Fitness Trade-Off in Black-Box Optimization</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Maria Laura Santoni, Elena Raponi, Aneta Neumann, Frank Neumann, Mike Preuss, Carola Doerr</div>
        <div class="preprint-abstract">arXiv:2408.16393v2 Announce Type: replace 
Abstract: In real-world applications, users often favor structurally diverse design choices over one high-quality solution. It is hence important to consider more solutions that decision makers can compare and further explore based on additional criteria. Alongside the existing approaches of evolutionary diversity optimization, quality diversity, and multimodal optimization, this paper presents a fresh perspective on this challenge by considering the problem of identifying a fixed number of solutions with a pairwise distance above a specified threshold while maximizing their average quality.
  We obtain first insight into these objectives by performing a subset selection on the search trajectories of different well-established search heuristics, whether they have been specifically designed with diversity in mind or not. We emphasize that the main goal of our work is not to present a new algorithm but to understand the capability of off-the-shelf algorithms to quantify the trade-off between the minimum pairwise distance within batches of solutions and their average quality. We also analyze how this trade-off depends on the properties of the underlying optimization problem.
  A possibly surprising outcome of our empirical study is the observation that naive uniform random sampling establishes a very strong baseline for our problem, hardly ever outperformed by the search trajectories of the considered heuristics. We interpret these results as a motivation to develop algorithms tailored to produce diverse solutions of high average quality.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00241">Synthesizing Public Opinions with LLMs: Role Creation, Impacts, and the Future to eDemorcacy</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Rabimba Karanjai, Boris Shor, Amanda Austin, Ryan Kennedy, Yang Lu, Lei Xu, Weidong Shi</div>
        <div class="preprint-abstract">arXiv:2504.00241v1 Announce Type: cross 
Abstract: This paper investigates the use of Large Language Models (LLMs) to synthesize public opinion data, addressing challenges in traditional survey methods like declining response rates and non-response bias. We introduce a novel technique: role creation based on knowledge injection, a form of in-context learning that leverages RAG and specified personality profiles from the HEXACO model and demographic information, and uses that for dynamically generated prompts. This method allows LLMs to simulate diverse opinions more accurately than existing prompt engineering approaches. We compare our results with pre-trained models with standard few-shot prompts. Experiments using questions from the Cooperative Election Study (CES) demonstrate that our role-creation approach significantly improves the alignment of LLM-generated opinions with real-world human survey responses, increasing answer adherence. In addition, we discuss challenges, limitations and future research directions.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00060">CF-CAM: Gradient Perturbation Mitigation and Feature Stabilization for Reliable Interpretability</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Hongjie He, Xu Pan, Yudong Yao</div>
        <div class="preprint-abstract">arXiv:2504.00060v1 Announce Type: new 
Abstract: As deep learning continues to advance, the opacity of neural network decision-making remains a critical challenge, limiting trust and applicability in high-stakes domains. Class Activation Mapping (CAM) techniques have emerged as a key approach to visualizing model decisions, yet existing methods face inherent trade-offs. Gradient-based CAM variants suffer from sensitivity to gradient perturbations, leading to unstable and unreliable explanations. Conversely, gradient-free approaches mitigate gradient instability but incur significant computational overhead and inference latency. To address these limitations, we propose Cluster Filter Class Activation Map (CF-CAM), a novel framework that reintroduces gradient-based weighting while enhancing robustness against gradient noise. CF-CAM employs a hierarchical importance weighting strategy to balance discriminative feature preservation and noise elimination. A density-aware channel clustering via Density-Based Spatial Clustering of Applications with Noise (DBSCAN) groups semantically relevant feature channels and discard noise-prone activations. Additionally, cluster-conditioned gradient filtering leverages bilateral filters to refine gradient signals, preserving edge-aware localization while suppressing noise impact. Experiment results demonstrate that CF-CAM achieves superior interpretability performance while maintaining resilience to gradient perturbations, outperforming state-of-the-art CAM methods in faithfulness and robustness. By effectively mitigating gradient instability without excessive computational cost, CF-CAM provides a reliable solution for enhancing the interpretability of deep neural networks in critical applications such as medical diagnosis and autonomous driving.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2401.13334">Explainable Bayesian Optimization</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Tanmay Chakraborty, Christian Wirth, Christin Seifert</div>
        <div class="preprint-abstract">arXiv:2401.13334v2 Announce Type: replace 
Abstract: Manual parameter tuning of cyber-physical systems is a common practice, but it is labor-intensive. Bayesian Optimization (BO) offers an automated alternative, yet its black-box nature reduces trust and limits human-BO collaborative system tuning. Experts struggle to interpret BO recommendations due to the lack of explanations. This paper addresses the post-hoc BO explainability problem for cyber-physical systems. We introduce TNTRules (Tune-No-Tune Rules), a novel algorithm that provides both global and local explanations for BO recommendations. TNTRules generates actionable rules and visual graphs, identifying optimal solution bounds and ranges, as well as potential alternative solutions. Unlike existing explainable AI (XAI) methods, TNTRules is tailored specifically for BO, by encoding uncertainty via a variance pruning technique and hierarchical agglomerative clustering. A multi-objective optimization approach allows maximizing explanation quality. We evaluate TNTRules using established XAI metrics (Correctness, Completeness, and Compactness) and compare it against adapted baseline methods. The results demonstrate that TNTRules generates high-fidelity, compact, and complete explanations, significantly outperforming three baselines on 5 multi-objective testing functions and 2 hyperparameter tuning problems.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00943">Graph Classification and Radiomics Signature for Identification of Tuberculous Meningitis</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Snigdha Agarwal, Ganaraja V H, Neelam Sinha, Abhilasha Indoria, Netravathi M, Jitender Saini</div>
        <div class="preprint-abstract">arXiv:2504.00943v1 Announce Type: cross 
Abstract: Introduction: Tuberculous meningitis (TBM) is a serious brain infection caused by Mycobacterium tuberculosis, characterized by inflammation of the meninges covering the brain and spinal cord. Diagnosis often requires invasive lumbar puncture (LP) and cerebrospinal fluid (CSF) analysis. Objectives: This study aims to classify TBM patients using T1-weighted (T1w) non-contrast Magnetic Resonance Imaging (MRI) scans. We hypothesize that specific brain regions, such as the interpeduncular cisterns, bone, and corpus callosum, contain visual markers that can non-invasively distinguish TBM patients from healthy controls. We propose a novel Pixel-array Graphs Classifier (PAG-Classifier) that leverages spatial relationships between neighbouring 3D pixels in a graph-based framework to extract significant features through eigen decomposition. These features are then used to train machine learning classifiers for effective patient classification. We validate our approach using a radiomics-based methodology, classifying TBM patients based on relevant radiomics features. Results: We utilized an internal dataset consisting of 52 scans, 32 from confirmed TBM patients based on mycobacteria detection in CSF, and 20 from healthy individuals. We achieved a 5-fold cross-validated average F1 score of 85.71% for cistern regions with our PAG-Classifier and 92.85% with the radiomics features classifier, surpassing current state-of-the-art benchmarks by 15% and 22%, respectively. However, bone and corpus callosum regions showed poor classification effectiveness, with average F1 scores below 50%. Conclusion: Our study suggests that algorithms like the PAG-Classifier serve as effective tools for non-invasive TBM analysis, particularly by targeting the interpeduncular cistern. Findings indicate that the bone and corpus callosum regions lack distinctive patterns for differentiation.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00065">Assessing Code Understanding in LLMs</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Cosimo Laneve, Alvise Span\`o, Dalila Ressi, Sabina Rossi, Michele Bugliesi</div>
        <div class="preprint-abstract">arXiv:2504.00065v1 Announce Type: cross 
Abstract: We present an empirical evaluation of Large Language Models in code understanding associated with non-trivial, semantic-preserving program transformations such as copy propagation or constant folding. Our findings show that LLMs fail to judge semantic equivalence in approximately 41\% of cases when no context is provided and in 29\% when given a simple generic context. To improve accuracy, we advocate integrating LLMs with code-optimization tools to enhance training and facilitate more robust program understanding.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00185">Self-Evolving Visual Concept Library using Vision-Language Critics</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Atharva Sehgal, Patrick Yuan, Ziniu Hu, Yisong Yue, Jennifer J. Sun, Swarat Chaudhuri</div>
        <div class="preprint-abstract">arXiv:2504.00185v1 Announce Type: cross 
Abstract: We study the problem of building a visual concept library for visual recognition. Building effective visual concept libraries is challenging, as manual definition is labor-intensive, while relying solely on LLMs for concept generation can result in concepts that lack discriminative power or fail to account for the complex interactions between them. Our approach, ESCHER, takes a library learning perspective to iteratively discover and improve visual concepts. ESCHER uses a vision-language model (VLM) as a critic to iteratively refine the concept library, including accounting for interactions between concepts and how they affect downstream classifiers. By leveraging the in-context learning abilities of LLMs and the history of performance using various concepts, ESCHER dynamically improves its concept generation strategy based on the VLM critic's feedback. Finally, ESCHER does not require any human annotations, and is thus an automated plug-and-play framework. We empirically demonstrate the ability of ESCHER to learn a concept library for zero-shot, few-shot, and fine-tuning visual classification tasks. This work represents, to our knowledge, the first application of concept library learning to real-world visual tasks.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00599">Near Field Localization via AI-Aided Subspace Methods</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Arad Gast, Luc Le Magoarou, Nir Shlezinger</div>
        <div class="preprint-abstract">arXiv:2504.00599v1 Announce Type: cross 
Abstract: The increasing demands for high-throughput and energy-efficient wireless communications are driving the adoption of extremely large antennas operating at high-frequency bands. In these regimes, multiple users will reside in the radiative near-field, and accurate localization becomes essential. Unlike conventional far-field systems that rely solely on DOA estimation, near-field localization exploits spherical wavefront propagation to recover both DOA and range information. While subspace-based methods, such as MUSIC and its extensions, offer high resolution and interpretability for near-field localization, their performance is significantly impacted by model assumptions, including non-coherent sources, well-calibrated arrays, and a sufficient number of snapshots. To address these limitations, this work proposes AI-aided subspace methods for near-field localization that enhance robustness to real-world challenges. Specifically, we introduce NF-SubspaceNet, a deep learning-augmented 2D MUSIC algorithm that learns a surrogate covariance matrix to improve localization under challenging conditions, and DCD-MUSIC, a cascaded AI-aided approach that decouples angle and range estimation to reduce computational complexity. We further develop a novel model-order-aware training method to accurately estimate the number of sources, that is combined with casting of near field subspace methods as AI models for learning. Extensive simulations demonstrate that the proposed methods outperform classical and existing deep-learning-based localization techniques, providing robust near-field localization even under coherent sources, miscalibrations, and few snapshots.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2407.06501">STORYSUMM: Evaluating Faithfulness in Story Summarization</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Melanie Subbiah, Faisal Ladhak, Akankshya Mishra, Griffin Adams, Lydia B. Chilton, Kathleen McKeown</div>
        <div class="preprint-abstract">arXiv:2407.06501v3 Announce Type: replace 
Abstract: Human evaluation has been the gold standard for checking faithfulness in abstractive summarization. However, with a challenging source domain like narrative, multiple annotators can agree a summary is faithful, while missing details that are obvious errors only once pointed out. We therefore introduce a new dataset, STORYSUMM, comprising LLM summaries of short stories with localized faithfulness labels and error explanations. This benchmark is for evaluation methods, testing whether a given method can detect challenging inconsistencies. Using this dataset, we first show that any one human annotation protocol is likely to miss inconsistencies, and we advocate for pursuing a range of methods when establishing ground truth for a summarization dataset. We finally test recent automatic metrics and find that none of them achieve more than 70% balanced accuracy on this task, demonstrating that it is a challenging benchmark for future work in faithfulness evaluation.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2311.16137">A Graph-to-Text Approach to Knowledge-Grounded Response Generation in Human-Robot Interaction</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Nicholas Thomas Walker, Stefan Ultes, Pierre Lison</div>
        <div class="preprint-abstract">arXiv:2311.16137v2 Announce Type: replace-cross 
Abstract: Knowledge graphs are often used to represent structured information in a flexible and efficient manner, but their use in situated dialogue remains under-explored. This paper presents a novel conversational model for human--robot interaction that rests upon a graph-based representation of the dialogue state. The knowledge graph representing the dialogue state is continuously updated with new observations from the robot sensors, including linguistic, situated and multimodal inputs, and is further enriched by other modules, in particular for spatial understanding. The neural conversational model employed to respond to user utterances relies on a simple but effective graph-to-text mechanism that traverses the dialogue state graph and converts the traversals into a natural language form. This conversion of the state graph into text is performed using a set of parameterized functions, and the values for those parameters are optimized based on a small set of Wizard-of-Oz interactions. After this conversion, the text representation of the dialogue state graph is included as part of the prompt of a large language model used to decode the agent response. The proposed approach is empirically evaluated through a user study with a humanoid robot that acts as conversation partner to evaluate the impact of the graph-to-text mechanism on the response generation. After moving a robot along a tour of an indoor environment, participants interacted with the robot using spoken dialogue and evaluated how well the robot was able to answer questions about what the robot observed during the tour. User scores show a statistically significant improvement in the perceived factuality of the robot responses when the graph-to-text approach is employed, compared to a baseline using inputs structured as semantic triples.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00189">Detecting Glioma, Meningioma, and Pituitary Tumors, and Normal Brain Tissues based on Yolov11 and Yolov8 Deep Learning Models</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Ahmed M. Taha, Salah A. Aly, Mohamed F. Darwish</div>
        <div class="preprint-abstract">arXiv:2504.00189v1 Announce Type: cross 
Abstract: Accurate and quick diagnosis of normal brain tissue Glioma, Meningioma, and Pituitary Tumors is crucial for optimal treatment planning and improved medical results. Magnetic Resonance Imaging (MRI) is widely used as a non-invasive diagnostic tool for detecting brain abnormalities, including tumors. However, manual interpretation of MRI scans is often time-consuming, prone to human error, and dependent on highly specialized expertise. This paper proposes an advanced AI-driven technique to detecting glioma, meningioma, and pituitary brain tumors using YoloV11 and YoloV8 deep learning models.
  Methods: Using a transfer learning-based fine-tuning approach, we integrate cutting-edge deep learning techniques with medical imaging to classify brain tumors into four categories: No-Tumor, Glioma, Meningioma, and Pituitary Tumors.
  Results: The study utilizes the publicly accessible CE-MRI Figshare dataset and involves fine-tuning pre-trained models YoloV8 and YoloV11 of 99.49% and 99.56% accuracies; and customized CNN accuracy of 96.98%. The results validate the potential of CNNs in achieving high precision in brain tumor detection and classification, highlighting their transformative role in medical imaging and diagnostics.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00356">Hybrid Global-Local Representation with Augmented Spatial Guidance for Zero-Shot Referring Image Segmentation</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Ting Liu, Siyuan Li</div>
        <div class="preprint-abstract">arXiv:2504.00356v1 Announce Type: cross 
Abstract: Recent advances in zero-shot referring image segmentation (RIS), driven by models such as the Segment Anything Model (SAM) and CLIP, have made substantial progress in aligning visual and textual information. Despite these successes, the extraction of precise and high-quality mask region representations remains a critical challenge, limiting the full potential of RIS tasks. In this paper, we introduce a training-free, hybrid global-local feature extraction approach that integrates detailed mask-specific features with contextual information from the surrounding area, enhancing mask region representation. To further strengthen alignment between mask regions and referring expressions, we propose a spatial guidance augmentation strategy that improves spatial coherence, which is essential for accurately localizing described areas. By incorporating multiple spatial cues, this approach facilitates more robust and precise referring segmentation. Extensive experiments on standard RIS benchmarks demonstrate that our method significantly outperforms existing zero-shot RIS models, achieving substantial performance gains. We believe our approach advances RIS tasks and establishes a versatile framework for region-text alignment, offering broader implications for cross-modal understanding and interaction. Code is available at https://github.com/fhgyuanshen/HybridGL .</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2501.18098">Disentangling Safe and Unsafe Corruptions via Anisotropy and Locality</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Ramchandran Muthukumar, Ambar Pal, Jeremias Sulam, Rene Vidal</div>
        <div class="preprint-abstract">arXiv:2501.18098v3 Announce Type: replace-cross 
Abstract: State-of-the-art machine learning systems are vulnerable to small perturbations to their input, where ``small'' is defined according to a threat model that assigns a positive threat to each perturbation. Most prior works define a task-agnostic, isotropic, and global threat, like the $\ell_p$ norm, where the magnitude of the perturbation fully determines the degree of the threat and neither the direction of the attack nor its position in space matter. However, common corruptions in computer vision, such as blur, compression, or occlusions, are not well captured by such threat models. This paper proposes a novel threat model called \texttt{Projected Displacement} (PD) to study robustness beyond existing isotropic and global threat models. The proposed threat model measures the threat of a perturbation via its alignment with \textit{unsafe directions}, defined as directions in the input space along which a perturbation of sufficient magnitude changes the ground truth class label. Unsafe directions are identified locally for each input based on observed training data. In this way, the PD threat model exhibits anisotropy and locality. Experiments on Imagenet-1k data indicate that, for any input, the set of perturbations with small PD threat includes \textit{safe} perturbations of large $\ell_p$ norm that preserve the true label, such as noise, blur and compression, while simultaneously excluding \textit{unsafe} perturbations that alter the true label. Unlike perceptual threat models based on embeddings of large-vision models, the PD threat model can be readily computed for arbitrary classification tasks without pre-training or finetuning. Further additional task annotation such as sensitivity to image regions or concept hierarchies can be easily integrated into the assessment of threat and thus the PD threat model presents practitioners with a flexible, task-driven threat specification.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00547">"Ensemblization" of density functional theory</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Tim Gould, Leeor Kronik, Stefano Pittalis</div>
        <div class="preprint-abstract">arXiv:2504.00547v1 Announce Type: new 
Abstract: Density functional theory (DFT) has transformed our ability to investigate and understand electronic ground states. In its original formulation, however, DFT is not suited to addressing (e.g.) degenerate ground states, mixed states with different particle numbers, or excited states. All these issues can be handled, in principle exactly, via ensemble DFT (EDFT). This Perspective provides a detailed introduction to and analysis of EDFT, in an in-principle exact framework that is constructed to avoid uncontrolled errors and inconsistencies that may be associated with {\it ad hoc} extensions of conventional DFT. In particular, it focuses on the "ensemblization" of both exact and approximate density functionals, a term we coin to describe a rigorous approach that lends itself to the construction of novel approximations consistent with the general ensemble framework, yet applicable to practical problems where traditional DFT tends to fail or does not apply at all. Specifically, symmetry considerations and ensemble properties are shown to enable each other in shaping a practical DFT-based methodology that extends beyond the ground state and, in doing so, highlights the need to look outside the standard ground state Kohn-Sham treatment.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00757">Integrating Fourier Neural Operators with Diffusion Models to improve Spectral Representation of Synthetic Earthquake Ground Motion Response</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Niccol\`o Perrone, Fanny Lehmann, Hugo Gabrielidis, Stefania Fresca, Filippo Gatti</div>
        <div class="preprint-abstract">arXiv:2504.00757v1 Announce Type: new 
Abstract: Nuclear reactor buildings must be designed to withstand the dynamic load induced by strong ground motion earthquakes. For this reason, their structural behavior must be assessed in multiple realistic ground shaking scenarios (e.g., the Maximum Credible Earthquake). However, earthquake catalogs and recorded seismograms may not always be available in the region of interest. Therefore, synthetic earthquake ground motion is progressively being employed, although with some due precautions: earthquake physics is sometimes not well enough understood to be accurately reproduced with numerical tools, and the underlying epistemic uncertainties lead to prohibitive computational costs related to model calibration. In this study, we propose an AI physics-based approach to generate synthetic ground motion, based on the combination of a neural operator that approximates the elastodynamics Green's operator in arbitrary source-geology setups, enhanced by a denoising diffusion probabilistic model. The diffusion model is trained to correct the ground motion time series generated by the neural operator. Our results show that such an approach promisingly enhances the realism of the generated synthetic seismograms, with frequency biases and Goodness-Of-Fit (GOF) scores being improved by the diffusion model. This indicates that the latter is capable to mitigate the mid-frequency spectral falloff observed in the time series generated by the neural operator. Our method showcases fast and cheap inference in different site and source conditions.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2406.16947">Generative Data Assimilation of Sparse Weather Station Observations at Kilometer Scales</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Peter Manshausen, Yair Cohen, Peter Harrington, Jaideep Pathak, Mike Pritchard, Piyush Garg, Morteza Mardani, Karthik Kashinath, Simon Byrne, Noah Brenowitz</div>
        <div class="preprint-abstract">arXiv:2406.16947v3 Announce Type: replace 
Abstract: Data assimilation of observational data into full atmospheric states is essential for weather forecast model initialization. Recently, methods for deep generative data assimilation have been proposed which allow for using new input data without retraining the model. They could also dramatically accelerate the costly data assimilation process used in operational regional weather models. Here, in a central US testbed, we demonstrate the viability of score-based data assimilation in the context of realistically complex km-scale weather. We train an unconditional diffusion model to generate snapshots of a state-of-the-art km-scale analysis product, the High Resolution Rapid Refresh. Then, using score-based data assimilation to incorporate sparse weather station data, the model produces maps of precipitation and surface winds. The generated fields display physically plausible structures, such as gust fronts, and sensitivity tests confirm learnt physics through multivariate relationships. Preliminary skill analysis shows the approach already outperforms a naive baseline of the High-Resolution Rapid Refresh system itself. By incorporating observations from 40 weather stations, 10% lower RMSEs on left-out stations are attained. Despite some lingering imperfections such as insufficiently disperse ensemble DA estimates, we find the results overall an encouraging proof of concept, and the first at km-scale. It is a ripe time to explore extensions that combine increasingly ambitious regional state generators with an increasing set of in situ, ground-based, and satellite remote sensing data streams.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00717">Advancements in Multimodal Differential Evolution: A Comprehensive Review and Future Perspectives</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Dikshit Chauhan,  Shivani, Donghwi Jung, Anupam Yadav</div>
        <div class="preprint-abstract">arXiv:2504.00717v1 Announce Type: cross 
Abstract: Multi-modal optimization involves identifying multiple global and local optima of a function, offering valuable insights into diverse optimal solutions within the search space. Evolutionary algorithms (EAs) excel at finding multiple solutions in a single run, providing a distinct advantage over classical optimization techniques that often require multiple restarts without guarantee of obtaining diverse solutions. Among these EAs, differential evolution (DE) stands out as a powerful and versatile optimizer for continuous parameter spaces. DE has shown significant success in multi-modal optimization by utilizing its population-based search to promote the formation of multiple stable subpopulations, each targeting different optima. Recent advancements in DE for multi-modal optimization have focused on niching methods, parameter adaptation, hybridization with other algorithms including machine learning, and applications across various domains. Given these developments, it is an opportune moment to present a critical review of the latest literature and identify key future research directions. This paper offers a comprehensive overview of recent DE advancements in multimodal optimization, including methods for handling multiple optima, hybridization with EAs, and machine learning, and highlights a range of real-world applications. Additionally, the paper outlines a set of compelling open problems and future research issues from multiple perspectives</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00839">Context-Aware Human Behavior Prediction Using Multimodal Large Language Models: Challenges and Insights</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Yuchen Liu, Lino Lerch, Luigi Palmieri, Andrey Rudenko, Sebastian Koch, Timo Ropinski, Marco Aiello</div>
        <div class="preprint-abstract">arXiv:2504.00839v1 Announce Type: cross 
Abstract: Predicting human behavior in shared environments is crucial for safe and efficient human-robot interaction. Traditional data-driven methods to that end are pre-trained on domain-specific datasets, activity types, and prediction horizons. In contrast, the recent breakthroughs in Large Language Models (LLMs) promise open-ended cross-domain generalization to describe various human activities and make predictions in any context. In particular, Multimodal LLMs (MLLMs) are able to integrate information from various sources, achieving more contextual awareness and improved scene understanding. The difficulty in applying general-purpose MLLMs directly for prediction stems from their limited capacity for processing large input sequences, sensitivity to prompt design, and expensive fine-tuning. In this paper, we present a systematic analysis of applying pre-trained MLLMs for context-aware human behavior prediction. To this end, we introduce a modular multimodal human activity prediction framework that allows us to benchmark various MLLMs, input variations, In-Context Learning (ICL), and autoregressive techniques. Our evaluation indicates that the best-performing framework configuration is able to reach 92.8% semantic similarity and 66.1% exact label accuracy in predicting human behaviors in the target frame.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2412.05330">Patient-specific prediction of glioblastoma growth via reduced order modeling and neural networks</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">D. Cerrone, D. Riccobelli, S. Gazzoni, P. Vitullo, F. Ballarin, J. Falco, F. Acerbi, A. Manzoni, P. Zunino, P. Ciarletta</div>
        <div class="preprint-abstract">arXiv:2412.05330v2 Announce Type: replace-cross 
Abstract: Glioblastoma is among the most aggressive brain tumors in adults, characterized by patient-specific invasion patterns driven by the underlying brain microstructure. In this work, we present a proof-of-concept for a mathematical model of GBL growth, enabling real-time prediction and patient-specific parameter identification from longitudinal neuroimaging data.
  The framework exploits a diffuse-interface mathematical model to describe the tumor evolution and a reduced-order modeling strategy, relying on proper orthogonal decomposition, trained on synthetic data derived from patient-specific brain anatomies reconstructed from magnetic resonance imaging and diffusion tensor imaging. A neural network surrogate learns the inverse mapping from tumor evolution to model parameters, achieving significant computational speed-up while preserving high accuracy.
  To ensure robustness and interpretability, we perform both global and local sensitivity analyses, identifying the key biophysical parameters governing tumor dynamics and assessing the stability of the inverse problem solution. These results establish a methodological foundation for future clinical deployment of patient-specific digital twins in neuro-oncology.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2310.12183">An Optimistic-Robust Approach for Dynamic Positioning of Omnichannel Inventories</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Pavithra Harsha, Shivaram Subramanian, Ali Koc, Mahesh Ramakrishna, Brian Quanz, Dhruv Shah, Chandra Narayanaswami</div>
        <div class="preprint-abstract">arXiv:2310.12183v2 Announce Type: replace-cross 
Abstract: We introduce a new class of data-driven and distribution-free optimistic-robust bimodal inventory optimization (BIO) strategy to effectively allocate inventory across a retail chain to meet time-varying, uncertain omnichannel demand. The bimodal nature of BIO stems from its ability to balance downside risk, as in traditional Robust Optimization (RO), which focuses on worst-case adversarial demand, with upside potential to enhance average-case performance. This enables BIO to remain as resilient as RO while capturing benefits that would otherwise be lost due to endogenous outliers. Omnichannel inventory planning provides a suitable problem setting for analyzing the effectiveness of BIO's bimodal strategy in managing the tradeoff between lost sales at stores and cross-channel e-commerce fulfillment costs, factors that are inherently asymmetric due to channel-specific behaviors. We provide structural insights about the BIO solution and how it can be tuned to achieve a preferred tradeoff between robustness and the average-case performance. Using a real-world dataset from a large American omnichannel retail chain, a business value assessment during a peak period indicates that BIO outperforms pure RO by 27% in terms of realized average profitability and surpasses other competitive baselines under imperfect distributional information by over 10%. This demonstrates that BIO provides a novel, data-driven, and distribution-free alternative to traditional RO that achieves strong average performance while carefully balancing robustness.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.19942">A stochastic gradient descent algorithm with random search directions</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Em\'eric Gbaguidi</div>
        <div class="preprint-abstract">arXiv:2503.19942v2 Announce Type: replace-cross 
Abstract: Stochastic coordinate descent algorithms are efficient methods in which each iterate is obtained by fixing most coordinates at their values from the current iteration, and approximately minimizing the objective with respect to the remaining coordinates. However, this approach is usually restricted to canonical basis vectors of $\mathbb{R}^d$. In this paper, we develop a new class of stochastic gradient descent algorithms with random search directions which uses the directional derivative of the gradient estimate following more general random vectors. We establish the almost sure convergence of these algorithms with decreasing step. We further investigate their central limit theorem and pay particular attention to analyze the impact of the search distributions on the asymptotic covariance matrix. We also provide non-asymptotic $\mathbb{L}^p$ rates of convergence.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00008">Tensor Generalized Approximate Message Passing</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Yinchuan Li, Guangchen Lan, Xiaodong Wang</div>
        <div class="preprint-abstract">arXiv:2504.00008v1 Announce Type: new 
Abstract: We propose a tensor generalized approximate message passing (TeG-AMP) algorithm for low-rank tensor inference, which can be used to solve tensor completion and decomposition problems. We derive TeG-AMP algorithm as an approximation of the sum-product belief propagation algorithm in high dimensions where the central limit theorem and Taylor series approximations are applicable. As TeG-AMP is developed based on a general TR decomposition model, it can be directly applied to many low-rank tensor types. Moreover, our TeG-AMP can be simplified based on the CP decomposition model and a tensor simplified AMP is proposed for low CP-rank tensor inference problems. Experimental results demonstrate that the proposed methods significantly improve recovery performances since it takes full advantage of tensor structures.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00850">Global Intervention and Distillation for Federated Out-of-Distribution Generalization</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Zhuang Qi, Runhui Zhang, Lei Meng, Wei Wu, Yachong Zhang, Xiangxu Meng</div>
        <div class="preprint-abstract">arXiv:2504.00850v1 Announce Type: cross 
Abstract: Attribute skew in federated learning leads local models to focus on learning non-causal associations, guiding them towards inconsistent optimization directions, which inevitably results in performance degradation and unstable convergence. Existing methods typically leverage data augmentation to enhance sample diversity or employ knowledge distillation to learn invariant representations. However, the instability in the quality of generated data and the lack of domain information limit their performance on unseen samples. To address these issues, this paper presents a global intervention and distillation method, termed FedGID, which utilizes diverse attribute features for backdoor adjustment to break the spurious association between background and label. It includes two main modules, where the global intervention module adaptively decouples objects and backgrounds in images, injects background information into random samples to intervene in the sample distribution, which links backgrounds to all categories to prevent the model from treating background-label associations as causal. The global distillation module leverages a unified knowledge base to guide the representation learning of client models, preventing local models from overfitting to client-specific attributes. Experimental results on three datasets demonstrate that FedGID enhances the model's ability to focus on the main subjects in unseen data and outperforms existing methods in collaborative modeling.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00362">The Impact of Hydration Shell Inclusion and Chain Exclusion in the Efficacy of Reaction Coordinates for Homogeneous and Heterogeneous Ice Nucleation</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Kimia Sinaeian, Amir Haji-Akbari</div>
        <div class="preprint-abstract">arXiv:2504.00362v1 Announce Type: new 
Abstract: Ice nucleation plays a pivotal role in many natural and industrial processes, and molecular simulations play have proven vital in uncovering its kinetics and mechanisms. A fundamental component of such simulations is the choice of an order parameter (OP) that quantifies the progress of nucleation, with the efficacy of an OP typically measured by its ability to predict the committor probabilities. Here, we leverage a machine learning framework introduced in our earlier work (Domingues,~\emph{et al.}, \emph{J. Phys. Chem. Lett.}, 15, 1279, {\bf 2024}) to systematically investigate how key implementation details influence the efficacy of standard Steinhardt OPs in capturing the progress of both homogeneous and heterogeneous ice nucleation. Our analysis identify distance and $q_6$ cutoffs, as the primary determinants of OP performance, regardless of the mode of nucleation. We also examine the impact of two popular refinement strategies, namely chain exclusion and hydration shell inclusion, on OP efficacy. We find neither strategy to exhibit a universally consistent impact. Instead, their efficacy depends strongly on the chosen distance and $q_6$ cutoffs. Chain exclusion enhances OP efficacy when the underlying OP lacks sufficient selectivity, whereas hydration shell inclusion is beneficial for overly selective OPs. Consequently, we demonstrate that selecting optimal combinations of such cutoffs can eliminate the need for these refinement strategies altogether. These findings provide a systematic understanding of how to design and optimize OPs for accurately describing complex nucleation phenomena, offering valuable guidance for improving the predictive power of molecular simulations.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00758">TAMIS: Tailored Membership Inference Attacks on Synthetic Data</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Paul Andrey, Batiste Le Bars, Marc Tommasi</div>
        <div class="preprint-abstract">arXiv:2504.00758v1 Announce Type: new 
Abstract: Membership Inference Attacks (MIA) enable to empirically assess the privacy of a machine learning algorithm. In this paper, we propose TAMIS, a novel MIA against differentially-private synthetic data generation methods that rely on graphical models. This attack builds upon MAMA-MIA, a recently-published state-of-the-art method. It lowers its computational cost and requires less attacker knowledge. Our attack is the product of a two-fold improvement. First, we recover the graphical model having generated a synthetic dataset by using solely that dataset, rather than shadow-modeling over an auxiliary one. This proves less costly and more performant. Second, we introduce a more mathematically-grounded attack score, that provides a natural threshold for binary predictions. In our experiments, TAMIS achieves better or similar performance as MAMA-MIA on replicas of the SNAKE challenge.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00597">On the Consistency of Multilingual Context Utilization in Retrieval-Augmented Generation</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jirui Qi, Raquel Fern\'andez, Arianna Bisazza</div>
        <div class="preprint-abstract">arXiv:2504.00597v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) with large language models (LLMs) has demonstrated strong performance in multilingual question-answering (QA) tasks by leveraging relevant passages retrieved from corpora. In multilingual RAG (mRAG), the retrieved passages can be written in languages other than that of the query entered by the user, making it challenging for LLMs to effectively utilize the provided information. Recent research suggests that retrieving passages from multilingual corpora can improve RAG performance, particularly for low-resource languages. However, the extent to which LLMs can leverage different kinds of multilingual contexts to generate accurate answers, *independently from retrieval quality*, remains understudied. In this paper, we conduct an extensive assessment of LLMs' ability to (i) make consistent use of a relevant passage regardless of its language, (ii) respond in the expected language, and (iii) focus on the relevant passage even when multiple `distracting' passages in different languages are provided in the context. Our experiments with four LLMs across three QA datasets covering a total of 48 languages reveal a surprising ability of LLMs to extract the relevant information from out-language passages, but a much weaker ability to formulate a full answer in the correct language. Our analysis, based on both accuracy and feature attribution techniques, further shows that distracting passages negatively impact answer quality regardless of their language. However, distractors in the query language exert a slightly stronger influence. Taken together, our findings deepen the understanding of how LLMs utilize context in mRAG systems, providing directions for future improvements.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.02130">Forgetting Transformer: Softmax Attention with a Forget Gate</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Zhixuan Lin, Evgenii Nikishin, Xu Owen He, Aaron Courville</div>
        <div class="preprint-abstract">arXiv:2503.02130v2 Announce Type: replace 
Abstract: An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name this attention mechanism Forgetting Attention and the resulting model the Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings. Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformer's superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet. We also introduce a "Pro" block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer. Our code is available at https://github.com/zhixuan-lin/forgetting-transformer.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00603">Data Cleansing for GANs</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Naoyuki Terashita, Hiroki Ohashi, Satoshi Hara</div>
        <div class="preprint-abstract">arXiv:2504.00603v1 Announce Type: new 
Abstract: As the application of generative adversarial networks (GANs) expands, it becomes increasingly critical to develop a unified approach that improves performance across various generative tasks. One effective strategy that applies to any machine learning task is identifying harmful instances, whose removal improves the performance. While previous studies have successfully estimated these harmful training instances in supervised settings, their approaches are not easily applicable to GANs. The challenge lies in two requirements of the previous approaches that do not apply to GANs. First, previous approaches require that the absence of a training instance directly affects the parameters. However, in the training for GANs, the instances do not directly affect the generator's parameters since they are only fed into the discriminator. Second, previous approaches assume that the change in loss directly quantifies the harmfulness of the instance to a model's performance, while common types of GAN losses do not always reflect the generative performance. To overcome the first challenge, we propose influence estimation methods that use the Jacobian of the generator's gradient with respect to the discriminator's parameters (and vice versa). Such a Jacobian represents the indirect effect between two models: how removing an instance from the discriminator's training changes the generator's parameters. Second, we propose an instance evaluation scheme that measures the harmfulness of each training instance based on how a GAN evaluation metric (e.g., Inception score) is expected to change by the instance's removal. Furthermore, we demonstrate that removing the identified harmful instances significantly improves the generative performance on various GAN evaluation metrics.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00906">Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Saaket Agashe, Kyle Wong, Vincent Tu, Jiachen Yang, Ang Li, Xin Eric Wang</div>
        <div class="preprint-abstract">arXiv:2504.00906v1 Announce Type: new 
Abstract: Computer use agents automate digital tasks by directly interacting with graphical user interfaces (GUIs) on computers and mobile devices, offering significant potential to enhance human productivity by completing an open-ended space of user queries. However, current agents face significant challenges: imprecise grounding of GUI elements, difficulties with long-horizon task planning, and performance bottlenecks from relying on single generalist models for diverse cognitive tasks. To this end, we introduce Agent S2, a novel compositional framework that delegates cognitive responsibilities across various generalist and specialist models. We propose a novel Mixture-of-Grounding technique to achieve precise GUI localization and introduce Proactive Hierarchical Planning, dynamically refining action plans at multiple temporal scales in response to evolving observations. Evaluations demonstrate that Agent S2 establishes new state-of-the-art (SOTA) performance on three prominent computer use benchmarks. Specifically, Agent S2 achieves 18.9% and 32.7% relative improvements over leading baseline agents such as Claude Computer Use and UI-TARS on the OSWorld 15-step and 50-step evaluation. Moreover, Agent S2 generalizes effectively to other operating systems and applications, surpassing previous best methods by 52.8% on WindowsAgentArena and by 16.52% on AndroidWorld relatively. Code available at https://github.com/simular-ai/Agent-S.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00883">Improved Visual-Spatial Reasoning via R1-Zero-Like Training</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Zhenyi Liao, Qingsong Xie, Yanhao Zhang, Zijian Kong, Haonan Lu, Zhenyu Yang, Zhijie Deng</div>
        <div class="preprint-abstract">arXiv:2504.00883v1 Announce Type: cross 
Abstract: Increasing attention has been placed on improving the reasoning capacities of multi-modal large language models (MLLMs). As the cornerstone for AI agents that function in the physical realm, video-based visual-spatial intelligence (VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This work conducts a first, in-depth study on improving the visual-spatial reasoning of MLLMs via R1-Zero-like training. Technically, we first identify that the visual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models cannot be activated via Chain of Thought (CoT) prompts. We then incorporate GRPO training for improved visual-spatial reasoning, using the carefully curated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation, we identify the necessity to keep the KL penalty (even with a small value) in GRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o. Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves performance comparable to that of the best open-source model LLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning and direct preference optimization baselines and observe strong performance superiority. The code and dataset will be available soon.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00307">Diffusion models for probabilistic precipitation generation from atmospheric variables</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Michael Aich, Sebastian Bathiany, Philipp Hess, Yu Huang, Niklas Boers</div>
        <div class="preprint-abstract">arXiv:2504.00307v1 Announce Type: new 
Abstract: Improving the representation of precipitation in Earth system models (ESMs) is critical for assessing the impacts of climate change and especially of extreme events like floods and droughts. In existing ESMs, precipitation is not resolved explicitly, but represented by parameterizations. These typically rely on resolving approximated but computationally expensive column-based physics, not accounting for interactions between locations. They struggle to capture fine-scale precipitation processes and introduce significant biases. We present a novel approach, based on generative machine learning, which integrates a conditional diffusion model with a UNet architecture to generate accurate, high-resolution (0.25{\deg}) global daily precipitation fields from a small set of prognostic atmospheric variables. Unlike traditional parameterizations, our framework efficiently produces ensemble predictions, capturing uncertainties in precipitation, and does not require fine-tuning by hand. We train our model on the ERA5 reanalysis and present a method that allows us to apply it to arbitrary ESM data, enabling fast generation of probabilistic forecasts and climate scenarios. By leveraging interactions between global prognostic variables, our approach provides an alternative parameterization scheme that mitigates biases present in the ESM precipitation while maintaining consistency with its large-scale (annual) trends. This work demonstrates that complex precipitation patterns can be learned directly from large-scale atmospheric variables, offering a computationally efficient alternative to conventional schemes.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00395">Minimum Description Length of a Spectrum Variational Autoencoder: A Theory</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Canlin Zhang, Xiuwen Liu</div>
        <div class="preprint-abstract">arXiv:2504.00395v1 Announce Type: new 
Abstract: Deep neural networks (DNNs) trained through end-to-end learning have achieved remarkable success across diverse machine learning tasks, yet they are not explicitly designed to adhere to the Minimum Description Length (MDL) principle, which posits that the best model provides the shortest description of the data. In this paper, we argue that MDL is essential to deep learning and propose a further generalized principle: Understanding is the use of a small amount of information to represent a large amount of information. To this end, we introduce a novel theoretical framework for designing and evaluating deep Variational Autoencoders (VAEs) based on MDL. In our theory, we designed the Spectrum VAE, a specific VAE architecture whose MDL can be rigorously evaluated under given conditions. Additionally, we introduce the concept of latent dimension combination, or pattern of spectrum, and provide the first theoretical analysis of their role in achieving MDL. We claim that a Spectrum VAE understands the data distribution in the most appropriate way when the MDL is achieved. This work is entirely theoretical and lays the foundation for future research on designing deep learning systems that explicitly adhere to information-theoretic principles.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00338">Agentic Multimodal AI for Hyperpersonalized B2B and B2C Advertising in Competitive Markets: An AI-Driven Competitive Advertising Framework</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Sakhinana Sagar Srinivas, Akash Das, Shivam Gupta, Venkataramana Runkana</div>
        <div class="preprint-abstract">arXiv:2504.00338v1 Announce Type: new 
Abstract: The growing use of foundation models (FMs) in real-world applications demands adaptive, reliable, and efficient strategies for dynamic markets. In the chemical industry, AI-discovered materials drive innovation, but commercial success hinges on market adoption, requiring FM-driven advertising frameworks that operate in-the-wild. We present a multilingual, multimodal AI framework for autonomous, hyper-personalized advertising in B2B and B2C markets. By integrating retrieval-augmented generation (RAG), multimodal reasoning, and adaptive persona-based targeting, our system generates culturally relevant, market-aware ads tailored to shifting consumer behaviors and competition. Validation combines real-world product experiments with a Simulated Humanistic Colony of Agents to model consumer personas, optimize strategies at scale, and ensure privacy compliance. Synthetic experiments mirror real-world scenarios, enabling cost-effective testing of ad strategies without risky A/B tests. Combining structured retrieval-augmented reasoning with in-context learning (ICL), the framework boosts engagement, prevents market cannibalization, and maximizes ROAS. This work bridges AI-driven innovation and market adoption, advancing multimodal FM deployment for high-stakes decision-making in commercial marketing.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00457">Distilling Multi-view Diffusion Models into 3D Generators</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Hao Qin, Luyuan Chen, Ming Kong, Mengxu Lu, Qiang Zhu</div>
        <div class="preprint-abstract">arXiv:2504.00457v1 Announce Type: cross 
Abstract: We introduce DD3G, a formulation that Distills a multi-view Diffusion model (MV-DM) into a 3D Generator using gaussian splatting. DD3G compresses and integrates extensive visual and spatial geometric knowledge from the MV-DM by simulating its ordinary differential equation (ODE) trajectory, ensuring the distilled generator generalizes better than those trained solely on 3D data. Unlike previous amortized optimization approaches, we align the MV-DM and 3D generator representation spaces to transfer the teacher's probabilistic flow to the student, thus avoiding inconsistencies in optimization objectives caused by probabilistic sampling. The introduction of probabilistic flow and the coupling of various attributes in 3D Gaussians introduce challenges in the generation process. To tackle this, we propose PEPD, a generator consisting of Pattern Extraction and Progressive Decoding phases, which enables efficient fusion of probabilistic flow and converts a single image into 3D Gaussians within 0.06 seconds. Furthermore, to reduce knowledge loss and overcome sparse-view supervision, we design a joint optimization objective that ensures the quality of generated samples through explicit supervision and implicit verification. Leveraging existing 2D generation models, we compile 120k high-quality RGBA images for distillation. Experiments on synthetic and public datasets demonstrate the effectiveness of our method. Our project is available at: https://qinbaigao.github.io/DD3G_project/</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2501.06143">Multilingual Performance of a Multimodal Artificial Intelligence System on Multisubject Physics Concept Inventories</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Gerd Kortemeyer, Marina Babayeva, Giulia Polverini, Ralf Widenhorn, Bor Gregorcic</div>
        <div class="preprint-abstract">arXiv:2501.06143v2 Announce Type: replace-cross 
Abstract: We investigate the multilingual and multimodal performance of a large language model-based artificial intelligence (AI) system, GPT-4o, using a diverse set of physics concept inventories spanning multiple languages and subject categories. The inventories, sourced from the PhysPort website, cover classical physics topics such as mechanics, electromagnetism, optics, and thermodynamics, as well as relativity, quantum mechanics, astronomy, mathematics, and laboratory skills. Unlike previous text-only studies, we uploaded the inventories as images to reflect what a student would see on paper, thereby assessing the system's multimodal functionality. Our results indicate variation in performance across subjects, with laboratory skills standing out as the weakest. We also observe differences across languages, with English and European languages showing the strongest performance. Notably, the relative difficulty of an inventory item is largely independent of the language of the survey. When comparing AI results to existing literature on student performance, we find that the AI system outperforms average post-instruction undergraduate students in all subject categories except laboratory skills. Furthermore, the AI performs worse on items requiring visual interpretation of images than on those that are purely text-based.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2502.08426">Semantic Learning for Molecular Communication in Internet of Bio-Nano Things</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Hanlin Cai, Ozgur B. Akan</div>
        <div class="preprint-abstract">arXiv:2502.08426v2 Announce Type: replace-cross 
Abstract: Molecular communication (MC) provides a foundational framework for information transmission in the Internet of Bio-Nano Things (IoBNT), where efficiency and reliability are crucial. However, the inherent limitations of molecular channels, such as low transmission rates, noise, and intersymbol interference (ISI), limit their ability to support complex data transmission. This paper proposes an end-to-end semantic learning framework designed to optimize task-oriented molecular communication, with a focus on biomedical diagnostic tasks under resource-constrained conditions. The proposed framework employs a deep encoder-decoder architecture to efficiently extract, quantize, and decode semantic features, prioritizing taskrelevant semantic information to enhance diagnostic classification performance. Additionally, a probabilistic channel network is introduced to approximate molecular propagation dynamics, enabling gradient-based optimization for end-to-end learning. Experimental results demonstrate that the proposed semantic framework improves diagnostic accuracy by at least 25% compared to conventional JPEG compression with LDPC coding methods under resource-constrained communication scenarios.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00170">Backdoor Detection through Replicated Execution of Outsourced Training</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Hengrui Jia, Sierra Wyllie, Akram Bin Sediq, Ahmed Ibrahim, Nicolas Papernot</div>
        <div class="preprint-abstract">arXiv:2504.00170v1 Announce Type: cross 
Abstract: It is common practice to outsource the training of machine learning models to cloud providers. Clients who do so gain from the cloud's economies of scale, but implicitly assume trust: the server should not deviate from the client's training procedure. A malicious server may, for instance, seek to insert backdoors in the model. Detecting a backdoored model without prior knowledge of both the backdoor attack and its accompanying trigger remains a challenging problem. In this paper, we show that a client with access to multiple cloud providers can replicate a subset of training steps across multiple servers to detect deviation from the training procedure in a similar manner to differential testing. Assuming some cloud-provided servers are benign, we identify malicious servers by the substantial difference between model updates required for backdooring and those resulting from clean training. Perhaps the strongest advantage of our approach is its suitability to clients that have limited-to-no local compute capability to perform training; we leverage the existence of multiple cloud providers to identify malicious updates without expensive human labeling or heavy computation. We demonstrate the capabilities of our approach on an outsourced supervised learning task where $50\%$ of the cloud providers insert their own backdoor; our approach is able to correctly identify $99.6\%$ of them. In essence, our approach is successful because it replaces the signature-based paradigm taken by existing approaches with an anomaly-based detection paradigm. Furthermore, our approach is robust to several attacks from adaptive adversaries utilizing knowledge of our detection scheme.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2403.17238">Temporal and Semantic Evaluation Metrics for Foundation Models in Post-Hoc Analysis of Robotic Sub-tasks</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jonathan Salfity, Selma Wanna, Minkyu Choi, Mitch Pryor</div>
        <div class="preprint-abstract">arXiv:2403.17238v2 Announce Type: replace-cross 
Abstract: Recent works in Task and Motion Planning (TAMP) show that training control policies on language-supervised robot trajectories with quality labeled data markedly improves agent task success rates. However, the scarcity of such data presents a significant hurdle to extending these methods to general use cases. To address this concern, we present an automated framework to decompose trajectory data into temporally bounded and natural language-based descriptive sub-tasks by leveraging recent prompting strategies for Foundation Models (FMs) including both Large Language Models (LLMs) and Vision Language Models (VLMs). Our framework provides both time-based and language-based descriptions for lower-level sub-tasks that comprise full trajectories. To rigorously evaluate the quality of our automatic labeling framework, we contribute an algorithm SIMILARITY to produce two novel metrics, temporal similarity and semantic similarity. The metrics measure the temporal alignment and semantic fidelity of language descriptions between two sub-task decompositions, namely an FM sub-task decomposition prediction and a ground-truth sub-task decomposition. We present scores for temporal similarity and semantic similarity above 90%, compared to 30% of a randomized baseline, for multiple robotic environments, demonstrating the effectiveness of our proposed framework. Our results enable building diverse, large-scale, language-supervised datasets for improved robotic TAMP.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00540">Adversarial Curriculum Graph-Free Knowledge Distillation for Graph Neural Networks</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Yuang Jia, Xiaojuan Shan, Jun Xia, Guancheng Wan, Yuchen Zhang, Wenke Huang, Mang Ye, Stan Z. Li</div>
        <div class="preprint-abstract">arXiv:2504.00540v1 Announce Type: new 
Abstract: Data-free Knowledge Distillation (DFKD) is a method that constructs pseudo-samples using a generator without real data, and transfers knowledge from a teacher model to a student by enforcing the student to overcome dimensional differences and learn to mimic the teacher's outputs on these pseudo-samples. In recent years, various studies in the vision domain have made notable advancements in this area. However, the varying topological structures and non-grid nature of graph data render the methods from the vision domain ineffective. Building upon prior research into differentiable methods for graph neural networks, we propose a fast and high-quality data-free knowledge distillation approach in this paper. Without compromising distillation quality, the proposed graph-free KD method (ACGKD) significantly reduces the spatial complexity of pseudo-graphs by leveraging the Binary Concrete distribution to model the graph structure and introducing a spatial complexity tuning parameter. This approach enables efficient gradient computation for the graph structure, thereby accelerating the overall distillation process. Additionally, ACGKD eliminates the dimensional ambiguity between the student and teacher models by increasing the student's dimensions and reusing the teacher's classifier. Moreover, it equips graph knowledge distillation with a CL-based strategy to ensure the student learns graph structures progressively. Extensive experiments demonstrate that ACGKD achieves state-of-the-art performance in distilling knowledge from GNNs without training data.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00955">Unfair Learning: GenAI Exceptionalism and Copyright Law</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">David Atkinson</div>
        <div class="preprint-abstract">arXiv:2504.00955v1 Announce Type: cross 
Abstract: This paper challenges the argument that generative artificial intelligence (GenAI) is entitled to broad immunity from copyright law for reproducing copyrighted works without authorization due to a fair use defense. It examines fair use legal arguments and eight distinct substantive arguments, contending that every legal and substantive argument favoring fair use for GenAI applies equally, if not more so, to humans. Therefore, granting GenAI exceptional privileges in this domain is legally and logically inconsistent with withholding broad fair use exemptions from individual humans. It would mean no human would need to pay for virtually any copyright work again. The solution is to take a circumspect view of any fair use claim for mass copyright reproduction by any entity and focus on the first principles of whether permitting such exceptionalism for GenAI promotes science and the arts.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2410.06717">Exact full-RSB SAT/UNSAT transition in infinitely wide two-layer neural networks</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Brandon L. Annesi, Enrico M. Malatesta, Francesco Zamponi</div>
        <div class="preprint-abstract">arXiv:2410.06717v4 Announce Type: replace-cross 
Abstract: We analyze the problem of storing random pattern-label associations using two classes of continuous non-convex weights models, namely the perceptron with negative margin and an infinite-width two-layer neural network with non-overlapping receptive fields and generic activation function. Using a full-RSB ansatz we compute the exact value of the SAT/UNSAT transition. Furthermore, in the case of the negative perceptron we show that the overlap distribution of typical states displays an overlap gap (a disconnected support) in certain regions of the phase diagram defined by the value of the margin and the density of patterns to be stored. This implies that some recent theorems that ensure convergence of Approximate Message Passing (AMP) based algorithms to capacity are not applicable. Finally, we show that Gradient Descent is not able to reach the maximal capacity, irrespectively of the presence of an overlap gap for typical states. This finding, similarly to what occurs in binary weight models, suggests that gradient-based algorithms are biased towards highly atypical states, whose inaccessibility determines the algorithmic threshold.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00299">Collaborative LLM Numerical Reasoning with Local Data Protection</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Min Zhang, Yuzhe Lu, Yun Zhou, Panpan Xu, Lin Lee Cheong, Chang-Tien Lu, Haozhu Wang</div>
        <div class="preprint-abstract">arXiv:2504.00299v1 Announce Type: new 
Abstract: Numerical reasoning over documents, which demands both contextual understanding and logical inference, is challenging for low-capacity local models deployed on computation-constrained devices. Although such complex reasoning queries could be routed to powerful remote models like GPT-4, exposing local data raises significant data leakage concerns. Existing mitigation methods generate problem descriptions or examples for remote assistance. However, the inherent complexity of numerical reasoning hinders the local model from generating logically equivalent queries and accurately inferring answers with remote guidance. In this paper, we present a model collaboration framework with two key innovations: (1) a context-aware synthesis strategy that shifts the query domains while preserving logical consistency; and (2) a tool-based answer reconstruction approach that reuses the remote-generated problem-solving pattern with code snippets. Experimental results demonstrate that our method achieves better reasoning accuracy than solely using local models while providing stronger data protection than fully relying on remote models. Furthermore, our method improves accuracy by 16.2% - 43.6% while reducing data leakage by 2.3% - 44.6% compared to existing data protection approaches.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2403.04197">Large Language Models are In-Context Molecule Learners</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jiatong Li, Wei Liu, Zhihao Ding, Wenqi Fan, Yuqiang Li, Qing Li</div>
        <div class="preprint-abstract">arXiv:2403.04197v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated exceptional performance in biochemical tasks, especially the molecule caption translation task, which aims to bridge the gap between molecules and natural language texts. However, previous methods in adapting LLMs to the molecule-caption translation task required extra domain-specific pre-training stages, suffered weak alignment between molecular and textual spaces, or imposed stringent demands on the scale of LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation (ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment from context examples via In-Context Molecule Tuning. Specifically, ICMA incorporates the following three stages: Hybrid Context Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Hybrid Context Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval to retrieve similar informative context examples. Additionally, Post-retrieval Re-ranking is composed of Sequence Reversal and Random Walk selection to further improve the quality of retrieval results. Finally, In-Context Molecule Tuning unlocks the in-context learning and reasoning capability of LLMs with the retrieved examples and adapts the parameters of LLMs for better alignment between molecules and texts. Experimental results demonstrate that ICMA can empower LLMs to achieve state-of-the-art or comparable performance without extra training corpora and intricate structures, showing that LLMs are inherently in-context molecule learners.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00048">Distill-C: Enhanced NL2SQL via Distilled Customization with LLMs</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Cong Duy Vu Hoang, Gioacchino Tangari, Clemence Lanfranchi, Dalu Guo, Paul Cayet, Steve Siu, Don Dharmasiri, Yuan-Fang Li, Long Duong, Damien Hilloulin, Rhicheek Patra, Sungpack Hong, Hassan Chafi</div>
        <div class="preprint-abstract">arXiv:2504.00048v1 Announce Type: cross 
Abstract: The growing adoption of large language models (LLMs) in business applications has amplified interest in Natural Language to SQL (NL2SQL) solutions, in which there is competing demand for high performance and efficiency. Domain- and customer-specific requirements further complicate the problem. To address this conundrum, we introduce Distill-C, a distilled customization framework tailored for NL2SQL tasks. Distill-C utilizes large teacher LLMs to produce high-quality synthetic data through a robust and scalable pipeline. Finetuning smaller and open-source LLMs on this synthesized data enables them to rival or outperform teacher models an order of magnitude larger. Evaluated on multiple challenging benchmarks, Distill-C achieves an average improvement of 36% in execution accuracy compared to the base models from three distinct LLM families. Additionally, on three internal customer benchmarks, Distill-C demonstrates a 22.6% performance improvement over the base models. Our results demonstrate that Distill-C is an effective, high-performing and generalizable approach for deploying lightweight yet powerful NL2SQL models, delivering exceptional accuracies while maintaining low computational cost.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00882">CrackSQL: A Hybrid SQL Dialect Translation System Powered by Large Language Models</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Wei Zhou, Yuyang Gao, Xuanhe Zhou, Guoliang Li</div>
        <div class="preprint-abstract">arXiv:2504.00882v1 Announce Type: cross 
Abstract: Dialect translation plays a key role in enabling seamless interaction across heterogeneous database systems. However, translating SQL queries between different dialects (e.g., from PostgreSQL to MySQL) remains a challenging task due to syntactic discrepancies and subtle semantic variations. Existing approaches including manual rewriting, rule-based systems, and large language model (LLM)-based techniques often involve high maintenance effort (e.g., crafting custom translation rules) or produce unreliable results (e.g., LLM generates non-existent functions), especially when handling complex queries. In this demonstration, we present CrackSQL, the first hybrid SQL dialect translation system that combines rule and LLM-based methods to overcome these limitations. CrackSQL leverages the adaptability of LLMs to minimize manual intervention, while enhancing translation accuracy by segmenting lengthy complex SQL via functionality-based query processing. To further improve robustness, it incorporates a novel cross-dialect syntax embedding model for precise syntax alignment, as well as an adaptive local-to-global translation strategy that effectively resolves interdependent query operations. CrackSQL supports three translation modes and offers multiple deployment and access options including a web console interface, a PyPI package, and a command-line prompt, facilitating adoption across a variety of real-world use cases</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00178">Boundless Byte Pair Encoding: Breaking the Pre-tokenization Barrier</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Craig W. Schmidt, Varshini Reddy, Chris Tanner, Yuval Pinter</div>
        <div class="preprint-abstract">arXiv:2504.00178v1 Announce Type: cross 
Abstract: Pre-tokenization, the initial step in many modern tokenization pipelines, segments text into smaller units called pretokens, typically splitting on whitespace and punctuation. While this process encourages having full, individual words as tokens, it introduces a fundamental limitation in most tokenization algorithms such as Byte Pair Encoding (BPE). Specifically, pre-tokenization causes the distribution of tokens in a corpus to heavily skew towards common, full-length words. This skewed distribution limits the benefits of expanding to larger vocabularies, since the additional tokens appear with progressively lower counts. To overcome this barrier, we propose BoundlessBPE, a modified BPE algorithm that relaxes the pretoken boundary constraint. Our approach selectively merges two complete pretokens into a larger unit we term a superword. Superwords are not necessarily semantically cohesive. For example, the pretokens " of" and " the" might be combined to form the superword " of the". This merging strategy results in a substantially more uniform distribution of tokens across a corpus than standard BPE, and compresses text more effectively, with an approximate 20% increase in bytes per token.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2501.18565">BounTCHA: A CAPTCHA Utilizing Boundary Identification in Guided Generative AI-extended Videos</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Lehao Lin, Ke Wang, Maha Abdallah, Wei Cai</div>
        <div class="preprint-abstract">arXiv:2501.18565v3 Announce Type: replace-cross 
Abstract: In recent years, the rapid development of artificial intelligence (AI) especially multi-modal Large Language Models (MLLMs), has enabled it to understand text, images, videos, and other multimedia data, allowing AI systems to execute various tasks based on human-provided prompts. However, AI-powered bots have increasingly been able to bypass most existing CAPTCHA systems, posing significant security threats to web applications. This makes the design of new CAPTCHA mechanisms an urgent priority. We observe that humans are highly sensitive to shifts and abrupt changes in videos, while current AI systems still struggle to comprehend and respond to such situations effectively. Based on this observation, we design and implement BounTCHA, a CAPTCHA mechanism that leverages human perception of boundaries in video transitions and disruptions. By utilizing generative AI's capability to extend original videos with prompts, we introduce unexpected twists and changes to create a pipeline for generating guided short videos for CAPTCHA purposes. We develop a prototype and conduct experiments to collect data on humans' time biases in boundary identification. This data serves as a basis for distinguishing between human users and bots. Additionally, we perform a detailed security analysis of BounTCHA, demonstrating its resilience against various types of attacks. We hope that BounTCHA will act as a robust defense, safeguarding millions of web applications in the AI-driven era.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2312.14628">Holistic analysis on the sustainability of Federated Learning across AI product lifecycle</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Hongliu Cao</div>
        <div class="preprint-abstract">arXiv:2312.14628v2 Announce Type: replace 
Abstract: In light of emerging legal requirements and policies focused on privacy protection, there is a growing trend of companies across various industries adopting Federated Learning (FL). This decentralized approach involves multiple clients or silos, collaboratively training a global model under the coordination of a central server while utilizing their private local data. Unlike traditional methods that necessitate data sharing and transmission, Cross-Silo FL allows clients to share model updates rather than raw data, thereby enhancing privacy. Despite its growing adoption, the carbon impact associated with Cross-Silo FL remains poorly understood due to the limited research in this area. This study seeks to bridge this gap by evaluating the sustainability of Cross-Silo FL throughout the entire AI product lifecycle, extending the analysis beyond the model training phase alone. We systematically compare this decentralized method with traditional centralized approaches and present a robust quantitative framework for assessing the costs and CO2 emissions in real-world Cross-Silo FL environments. Our findings indicate that the energy consumption and costs of model training are comparable between Cross-Silo Federated Learning and Centralized Learning. However, the additional data transfer and storage requirements inherent in Centralized Learning can result in significant, often overlooked CO2 emissions. Moreover, we introduce an innovative data and application management system that integrates Cross-Silo FL and analytics, aiming at improving the sustainability and economic efficiency of IT enterprises.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00255">SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Yanzheng Xiang, Hanqi Yan, Shuyin Ouyang, Lin Gui, Yulan He</div>
        <div class="preprint-abstract">arXiv:2504.00255v1 Announce Type: cross 
Abstract: This study evaluates large language models (LLMs) in generating code from algorithm descriptions from recent NLP papers. The task requires two key competencies: (1) algorithm comprehension: synthesizing information from papers and academic literature to understand implementation logic, and (2) coding expertise: identifying dependencies and correctly implementing necessary APIs. To facilitate rigorous evaluation, we introduce SciReplicate-Bench, a benchmark of 100 tasks from 36 NLP papers published in 2024, featuring detailed annotations and comprehensive test cases. Building on SciReplicate-Bench, we propose Sci-Reproducer, a multi-agent framework consisting of a Paper Agent that interprets algorithmic concepts from literature and a Code Agent that retrieves dependencies from repositories and implement solutions. To assess algorithm understanding, we introduce reasoning graph accuracy, which quantifies similarity between generated and reference reasoning graphs derived from code comments and structure. For evaluating implementation quality, we employ execution accuracy, CodeBLEU, and repository dependency/API recall metrics. In our experiments, we evaluate various powerful Non-Reasoning LLMs and Reasoning LLMs as foundational models. The best-performing LLM using Sci-Reproducer achieves only 39% execution accuracy, highlighting the benchmark's difficulty.Our analysis identifies missing or inconsistent algorithm descriptions as key barriers to successful reproduction. We will open-source our benchmark, and code at https://github.com/xyzCS/SciReplicate-Bench.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2411.10397">Features that Make a Difference: Leveraging Gradients for Improved Dictionary Learning</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jeffrey Olmo, Jared Wilson, Max Forsey, Bryce Hepner, Thomas Vin Howe, David Wingate</div>
        <div class="preprint-abstract">arXiv:2411.10397v2 Announce Type: replace 
Abstract: Sparse Autoencoders (SAEs) are a promising approach for extracting neural network representations by learning a sparse and overcomplete decomposition of the network's internal activations. However, SAEs are traditionally trained considering only activation values and not the effect those activations have on downstream computations. This limits the information available to learn features, and biases the autoencoder towards neglecting features which are represented with small activation values but strongly influence model outputs. To address this, we introduce Gradient SAEs (g-SAEs), which modify the $k$-sparse autoencoder architecture by augmenting the TopK activation function to rely on the gradients of the input activation when selecting the $k$ elements. For a given sparsity level, g-SAEs produce reconstructions that are more faithful to original network performance when propagated through the network. Additionally, we find evidence that g-SAEs learn latents that are on average more effective at steering models in arbitrary contexts. By considering the downstream effects of activations, our approach leverages the dual nature of neural network features as both $\textit{representations}$, retrospectively, and $\textit{actions}$, prospectively. While previous methods have approached the problem of feature discovery primarily focused on the former aspect, g-SAEs represent a step towards accounting for the latter as well.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00869">m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning with Large Language Models</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Xiaoke Huang, Juncheng Wu, Hui Liu, Xianfeng Tang, Yuyin Zhou</div>
        <div class="preprint-abstract">arXiv:2504.00869v1 Announce Type: cross 
Abstract: Test-time scaling has emerged as a powerful technique for enhancing the reasoning capabilities of large language models. However, its effectiveness in medical reasoning remains uncertain, as the medical domain fundamentally differs from mathematical tasks in terms of knowledge representation and decision-making processes. In this paper, we provide the first comprehensive investigation of test-time scaling for medical reasoning and present m1, a simple yet effective approach that increases a model's medical reasoning capability at inference. Our evaluation across diverse medical tasks demonstrates that test-time scaling consistently enhances medical reasoning, enabling lightweight fine-tuned models under 10B parameters to establish new state-of-the-art performance, while our 32B model rivals previous 70B-scale medical LLMs. However, we identify an optimal reasoning token budget of approximately 4K, beyond which performance may degrade due to overthinking. Budget forcing, which extends test-time computation through iterative prompts, helps models double-check answers but does not necessarily improve the overall medical QA performance and, in some cases, even introduces errors into previously correct responses. Our case-by-case analysis identifies insufficient medical knowledge as a key bottleneck that prevents further performance gains through test-time scaling. We find that increasing data scale, improving data quality, and expanding model capacity consistently enhance medical knowledge grounding, enabling continued performance improvements, particularly on challenging medical benchmarks where smaller models reach saturation. These findings underscore fundamental differences between medical and mathematical reasoning in LLMs, highlighting that enriched medical knowledge, other than increased reasoning depth alone, is essential for realizing the benefits of test-time scaling.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00750">$C^2$AV-TSE: Context and Confidence-aware Audio Visual Target Speaker Extraction</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Wenxuan Wu, Xueyuan Chen, Shuai Wang, Jiadong Wang, Lingwei Meng, Xixin Wu, Helen Meng, Haizhou Li</div>
        <div class="preprint-abstract">arXiv:2504.00750v1 Announce Type: cross 
Abstract: Audio-Visual Target Speaker Extraction (AV-TSE) aims to mimic the human ability to enhance auditory perception using visual cues. Although numerous models have been proposed recently, most of them estimate target signals by primarily relying on local dependencies within acoustic features, underutilizing the human-like capacity to infer unclear parts of speech through contextual information. This limitation results in not only suboptimal performance but also inconsistent extraction quality across the utterance, with some segments exhibiting poor quality or inadequate suppression of interfering speakers. To close this gap, we propose a model-agnostic strategy called the Mask-And-Recover (MAR). It integrates both inter- and intra-modality contextual correlations to enable global inference within extraction modules. Additionally, to better target challenging parts within each sample, we introduce a Fine-grained Confidence Score (FCS) model to assess extraction quality and guide extraction modules to emphasize improvement on low-quality segments. To validate the effectiveness of our proposed model-agnostic training paradigm, six popular AV-TSE backbones were adopted for evaluation on the VoxCeleb2 dataset, demonstrating consistent performance improvements across various metrics.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2501.14577">ZETA: Leveraging Z-order Curves for Efficient Top-k Attention</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Qiuhao Zeng, Jerry Huang, Peng Lu, Gezheng Xu, Boxing Chen, Charles Ling, Boyu Wang</div>
        <div class="preprint-abstract">arXiv:2501.14577v3 Announce Type: replace 
Abstract: Over recent years, the Transformer has become a fundamental building block for sequence modeling architectures. Yet at its core is the use of self-attention, whose memory and computational cost grow quadratically with the sequence length $N$, rendering it prohibitively expensive for long sequences. A promising approach is top-$k$ attention, which selects only the $k$ most relevant tokens and achieves performance comparable to vanilla self-attention while significantly reducing space and computational demands. However, causal masks require the current query token to only attend to past tokens, preventing the existing top-$k$ attention method from efficiently searching for the most relevant tokens in parallel, thereby limiting training efficiency. In this work, we propose ZETA, leveraging \textbf{Z}-Order Curves for \textbf{E}fficient \textbf{T}op-$k$ \textbf{A}ttention, to enable parallel querying of past tokens for entire sequences. % in both space and time complexity of $\mathcal{O}(N \log N)$. We first theoretically show that the choice of key and query dimensions involves a trade-off between the curse of dimensionality and the preservation of relative distances after projection. In light of this insight, we propose reducing the dimensionality of keys and queries in contrast to values and further leverage $Z$-order curves to map low-dimensional keys and queries into \emph{one}-dimensional space, which permits parallel sorting, thereby largely improving the efficiency for top-$k$ token selection. Experimental results demonstrate that ZETA matches the performance of standard attention on the synthetic \textsc{Multi-Query Associative Recall} task and outperforms attention and its variants on \textsc{Long Range Arena} and \textsc{WikiText-103} language modeling.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00366">CopyQNN: Quantum Neural Network Extraction Attack under Varying Quantum Noise</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Zhenxiao Fu, Leyi Zhao, Xuhong Zhang, Yilun Xu, Gang Huang, Fan Chen</div>
        <div class="preprint-abstract">arXiv:2504.00366v1 Announce Type: cross 
Abstract: Quantum Neural Networks (QNNs) have shown significant value across domains, with well-trained QNNs representing critical intellectual property often deployed via cloud-based QNN-as-a-Service (QNNaaS) platforms. Recent work has examined QNN model extraction attacks using classical and emerging quantum strategies. These attacks involve adversaries querying QNNaaS platforms to obtain labeled data for training local substitute QNNs that replicate the functionality of cloud-based models. However, existing approaches have largely overlooked the impact of varying quantum noise inherent in noisy intermediate-scale quantum (NISQ) computers, limiting their effectiveness in real-world settings. To address this limitation, we propose the CopyQNN framework, which employs a three-step data cleaning method to eliminate noisy data based on its noise sensitivity. This is followed by the integration of contrastive and transfer learning within the quantum domain, enabling efficient training of substitute QNNs using a limited but cleaned set of queried data. Experimental results on NISQ computers demonstrate that a practical implementation of CopyQNN significantly outperforms state-of-the-art QNN extraction attacks, achieving an average performance improvement of 8.73% across all tasks while reducing the number of required queries by 90x, with only a modest increase in hardware overhead.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00037">ViT-Linearizer: Distilling Quadratic Knowledge into Linear-Time Vision Models</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Guoyizhe Wei, Rama Chellappa</div>
        <div class="preprint-abstract">arXiv:2504.00037v1 Announce Type: cross 
Abstract: Vision Transformers (ViTs) have delivered remarkable progress through global self-attention, yet their quadratic complexity can become prohibitive for high-resolution inputs. In this work, we present ViT-Linearizer, a cross-architecture distillation framework that transfers rich ViT representations into a linear-time, recurrent-style model. Our approach leverages 1) activation matching, an intermediate constraint that encourages student to align its token-wise dependencies with those produced by the teacher, and 2) masked prediction, a contextual reconstruction objective that requires the student to predict the teacher's representations for unseen (masked) tokens, to effectively distill the quadratic self-attention knowledge into the student while maintaining efficient complexity. Empirically, our method provides notable speedups particularly for high-resolution tasks, significantly addressing the hardware challenges in inference. Additionally, it also elevates Mamba-based architectures' performance on standard vision benchmarks, achieving a competitive 84.3% top-1 accuracy on ImageNet with a base-sized model. Our results underscore the good potential of RNN-based solutions for large-scale visual tasks, bridging the gap between theoretical efficiency and real-world practice.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.24270">Visual Acoustic Fields</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Yuelei Li, Hyunjin Kim, Fangneng Zhan, Ri-Zhao Qiu, Mazeyu Ji, Xiaojun Shan, Xueyan Zou, Paul Liang, Hanspeter Pfister, Xiaolong Wang</div>
        <div class="preprint-abstract">arXiv:2503.24270v2 Announce Type: replace-cross 
Abstract: Objects produce different sounds when hit, and humans can intuitively infer how an object might sound based on its appearance and material properties. Inspired by this intuition, we propose Visual Acoustic Fields, a framework that bridges hitting sounds and visual signals within a 3D space using 3D Gaussian Splatting (3DGS). Our approach features two key modules: sound generation and sound localization. The sound generation module leverages a conditional diffusion model, which takes multiscale features rendered from a feature-augmented 3DGS to generate realistic hitting sounds. Meanwhile, the sound localization module enables querying the 3D scene, represented by the feature-augmented 3DGS, to localize hitting positions based on the sound sources. To support this framework, we introduce a novel pipeline for collecting scene-level visual-sound sample pairs, achieving alignment between captured images, impact locations, and corresponding sounds. To the best of our knowledge, this is the first dataset to connect visual and acoustic signals in a 3D context. Extensive experiments on our dataset demonstrate the effectiveness of Visual Acoustic Fields in generating plausible impact sounds and accurately localizing impact sources. Our project page is at https://yuelei0428.github.io/projects/Visual-Acoustic-Fields/.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00408">From Intuition to Understanding: Using AI Peers to Overcome Physics Misconceptions</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Ruben Weijers, Denton Wu, Hannah Betts, Tamara Jacod, Yuxiang Guan, Vidya Sujaya, Kushal Dev, Toshali Goel, William Delooze, Reihaneh Rabbany, Ying Wu, Jean-Fran\c{c}ois Godbout, Kellin Pelrine</div>
        <div class="preprint-abstract">arXiv:2504.00408v1 Announce Type: cross 
Abstract: Generative AI has the potential to transform personalization and accessibility of education. However, it raises serious concerns about accuracy and helping students become independent critical thinkers. In this study, we designed a helpful AI "Peer" to help students correct fundamental physics misconceptions related to Newtonian mechanic concepts. In contrast to approaches that seek near-perfect accuracy to create an authoritative AI tutor or teacher, we directly inform students that this AI can answer up to 40% of questions incorrectly. In a randomized controlled trial with 165 students, those who engaged in targeted dialogue with the AI Peer achieved post-test scores that were, on average, 10.5 percentage points higher - with over 20 percentage points higher normalized gain - than a control group that discussed physics history. Qualitative feedback indicated that 91% of the treatment group's AI interactions were rated as helpful. Furthermore, by comparing student performance on pre- and post-test questions about the same concept, along with experts' annotations of the AI interactions, we find initial evidence suggesting the improvement in performance does not depend on the correctness of the AI. With further research, the AI Peer paradigm described here could open new possibilities for how we learn, adapt to, and grow with AI.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00233">Over-the-Air Edge Inference via End-to-End Metasurfaces-Integrated Artificial Neural Networks</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Kyriakos Stylianopoulos, Paolo Di Lorenzo, George C. Alexandropoulos</div>
        <div class="preprint-abstract">arXiv:2504.00233v1 Announce Type: new 
Abstract: In the Edge Inference (EI) paradigm, where a Deep Neural Network (DNN) is split across the transceivers to wirelessly communicate goal-defined features in solving a computational task, the wireless medium has been commonly treated as a source of noise. In this paper, motivated by the emerging technologies of Reconfigurable Intelligent Surfaces (RISs) and Stacked Intelligent Metasurfaces (SIM) that offer programmable propagation of wireless signals, either through controllable reflections or diffractions, we optimize the RIS/SIM-enabled smart wireless environment as a means of over-the-air computing, resembling the operations of DNN layers. We propose a framework of Metasurfaces-Integrated Neural Networks (MINNs) for EI, presenting its modeling, training through a backpropagation variation for fading channels, and deployment aspects. The overall end-to-end DNN architecture is general enough to admit RIS and SIM devices, through controllable reconfiguration before each transmission or fixed configurations after training, while both channel-aware and channel-agnostic transceivers are considered. Our numerical evaluation showcases metasurfaces to be instrumental in performing image classification under link budgets that impede conventional communications or metasurface-free systems. It is demonstrated that our MINN framework can significantly simplify EI requirements, achieving near-optimal performance with $50~$dB lower testing signal-to-noise ratio compared to training, even without transceiver channel knowledge.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00794">Conditional Temporal Neural Processes with Covariance Loss</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Boseon Yoo, Jiwoo Lee, Janghoon Ju, Seijun Chung, Soyeon Kim, Jaesik Choi</div>
        <div class="preprint-abstract">arXiv:2504.00794v1 Announce Type: new 
Abstract: We introduce a novel loss function, Covariance Loss, which is conceptually equivalent to conditional neural processes and has a form of regularization so that is applicable to many kinds of neural networks. With the proposed loss, mappings from input variables to target variables are highly affected by dependencies of target variables as well as mean activation and mean dependencies of input and target variables. This nature enables the resulting neural networks to become more robust to noisy observations and recapture missing dependencies from prior information. In order to show the validity of the proposed loss, we conduct extensive sets of experiments on real-world datasets with state-of-the-art models and discuss the benefits and drawbacks of the proposed Covariance Loss.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00762">Do We Truly Need So Many Samples? Multi-LLM Repeated Sampling Efficiently Scale Test-Time Compute</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jianhao Chen, Zishuo Xun, Bocheng Zhou, Han Qi, Qiaosheng Zhang, Yang Chen, Wei Hu, Yuzhong Qu, Wanli Ouyang, Shuyue Hu</div>
        <div class="preprint-abstract">arXiv:2504.00762v1 Announce Type: new 
Abstract: This paper presents a simple, effective, and cost-efficient strategy to improve LLM performance by scaling test-time compute. Our strategy builds upon the repeated-sampling-then-voting framework, with a novel twist: incorporating multiple models, even weaker ones, to leverage their complementary strengths that potentially arise from diverse training data and paradigms. By using consistency as a signal, our strategy dynamically switches between models. Theoretical analysis highlights the efficiency and performance advantages of our strategy. Extensive experiments on six datasets demonstrate that our strategy not only outperforms self-consistency and state-of-the-art multi-agent debate approaches, but also significantly reduces inference costs. Additionally, ModelSwitch requires only a few comparable LLMs to achieve optimal performance and can be extended with verification methods, demonstrating the potential of leveraging multiple LLMs in the generation-verification paradigm.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00727">Personality-Driven Decision-Making in LLM-Based Autonomous Agents</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Lewis Newsham, Daniel Prince</div>
        <div class="preprint-abstract">arXiv:2504.00727v1 Announce Type: new 
Abstract: The embedding of Large Language Models (LLMs) into autonomous agents is a rapidly developing field which enables dynamic, configurable behaviours without the need for extensive domain-specific training. In our previous work, we introduced SANDMAN, a Deceptive Agent architecture leveraging the Five-Factor OCEAN personality model, demonstrating that personality induction significantly influences agent task planning. Building on these findings, this study presents a novel method for measuring and evaluating how induced personality traits affect task selection processes - specifically planning, scheduling, and decision-making - in LLM-based agents. Our results reveal distinct task-selection patterns aligned with induced OCEAN attributes, underscoring the feasibility of designing highly plausible Deceptive Agents for proactive cyber defense strategies.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.01141">How Well do LLMs Compress Their Own Chain-of-Thought? A Token Complexity Approach</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Ayeong Lee, Ethan Che, Tianyi Peng</div>
        <div class="preprint-abstract">arXiv:2503.01141v2 Announce Type: replace-cross 
Abstract: Chain-of-thought prompting has emerged as a powerful technique for enabling large language models (LLMs) to solve complex reasoning tasks. However, these reasoning chains can be verbose, raising concerns about efficiency. In response, recent works have sought to decrease response lengths through simple prompting strategies (e.g. 'be concise'). In this work, we conduct the first systematic study of the relationship between reasoning length and model performance across a diverse range of compression instructions (e.g. 'use 10 words or less' or 'remove all punctuation'). In doing so, we discover a universal tradeoff between reasoning length and accuracy that persists across even very distinct reasoning chains. We demonstrate that this tradeoff emerges from a sharp threshold behavior at the question level: each task has an intrinsic 'token complexity' - a minimal number of tokens required for successful problem-solving. We show how token complexity enables us to compute information-theoretic limits on the accuracy-compression tradeoff, and find that prompt-based compression strategies operate far from these theoretical limits. This suggests there may be significant room for improvement and our framework provides a benchmark to help researchers evaluate progress in reasoning efficiency. Our work also highlights the importance of adaptive compression -- giving shorter responses for easier questions -- and we show that token complexity is a useful tool for measuring this capability.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2501.13329">Sparse identification of nonlinear dynamics and Koopman operators with Shallow Recurrent Decoder Networks</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Mars Liyao Gao, Jan P. Williams, J. Nathan Kutz</div>
        <div class="preprint-abstract">arXiv:2501.13329v2 Announce Type: replace 
Abstract: Modeling real-world spatio-temporal data is exceptionally difficult due to inherent high dimensionality, measurement noise, partial observations, and often expensive data collection procedures. In this paper, we present Sparse Identification of Nonlinear Dynamics with SHallow REcurrent Decoder networks (SINDy-SHRED), a method to jointly solve the sensing and model identification problems with simple implementation, efficient computation, and robust performance. SINDy-SHRED uses Gated Recurrent Units to model the temporal sequence of sparse sensor measurements along with a shallow decoder network to reconstruct the full spatio-temporal field from the latent state space. Our algorithm introduces a SINDy-based regularization for which the latent space progressively converges to a SINDy-class functional, provided the projection remains within the set. In restricting SINDy to a linear model, a Koopman-SHRED model is generated. SINDy-SHRED (i) learns a symbolic and interpretable generative model of a parsimonious and low-dimensional latent space for the complex spatio-temporal dynamics, (ii) discovers new physics models even for well-known physical systems, (iii) achieves provably robust convergence with an observed globally convex loss landscape, and (iv) achieves superior accuracy, data efficiency, and training time, all with fewer model parameters. We conduct systematic experimental studies on PDE data such as turbulent flows, real-world sensor measurements for sea surface temperature, and direct video data. The interpretable SINDy and Koopman models of latent state dynamics enable stable and accurate long-term video predictions, outperforming all current baseline deep learning models in accuracy, training time, and data requirements, including Convolutional LSTM, PredRNN, ResNet, and SimVP.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2410.02140">A Formal Framework for Understanding Length Generalization in Transformers</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Xinting Huang, Andy Yang, Satwik Bhattamishra, Yash Sarrof, Andreas Krebs, Hattie Zhou, Preetum Nakkiran, Michael Hahn</div>
        <div class="preprint-abstract">arXiv:2410.02140v2 Announce Type: replace 
Abstract: A major challenge for transformers is generalizing to sequences longer than those observed during training. While previous works have empirically shown that transformers can either succeed or fail at length generalization depending on the task, theoretical understanding of this phenomenon remains limited. In this work, we introduce a rigorous theoretical framework to analyze length generalization in causal transformers with learnable absolute positional encodings. In particular, we characterize those functions that are identifiable in the limit from sufficiently long inputs with absolute positional encodings under an idealized inference scheme using a norm-based regularizer. This enables us to prove the possibility of length generalization for a rich family of problems. We experimentally validate the theory as a predictor of success and failure of length generalization across a range of algorithmic and formal language tasks. Our theory not only explains a broad set of empirical observations but also opens the way to provably predicting length generalization capabilities in transformers.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2411.03389">Neurons for Neutrons: A Transformer Model for Computation Load Estimation on Domain-Decomposed Neutron Transport Problems</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Alexander Mote, Todd Palmer, Lizhong Chen</div>
        <div class="preprint-abstract">arXiv:2411.03389v3 Announce Type: replace-cross 
Abstract: Domain decomposition is a technique used to reduce memory overhead on large neutron transport problems. Currently, the optimal load-balanced processor allocation for these domains is typically determined through small-scale simulations of the problem, which can be time-consuming for researchers and must be repeated anytime a problem input is changed. We propose a Transformer model with a unique 3D input embedding, and input representations designed for domain-decomposed neutron transport problems, which can predict the subdomain computation loads generated by small-scale simulations. We demonstrate that such a model trained on domain-decomposed Small Modular Reactor (SMR) simulations achieves 98.2% accuracy while being able to skip the small-scale simulation step entirely. Tests of the model's robustness on variant fuel assemblies, other problem geometries, and changes in simulation parameters are also discussed.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.23820">When Counterfactual Reasoning Fails: Chaos and Real-World Complexity</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Yahya Aalaila, Gerrit Gro{\ss}mann, Sumantrak Mukherjee, Jonas Wahl, Sebastian Vollmer</div>
        <div class="preprint-abstract">arXiv:2503.23820v2 Announce Type: replace 
Abstract: Counterfactual reasoning, a cornerstone of human cognition and decision-making, is often seen as the 'holy grail' of causal learning, with applications ranging from interpreting machine learning models to promoting algorithmic fairness. While counterfactual reasoning has been extensively studied in contexts where the underlying causal model is well-defined, real-world causal modeling is often hindered by model and parameter uncertainty, observational noise, and chaotic behavior. The reliability of counterfactual analysis in such settings remains largely unexplored. In this work, we investigate the limitations of counterfactual reasoning within the framework of Structural Causal Models. Specifically, we empirically investigate \emph{counterfactual sequence estimation} and highlight cases where it becomes increasingly unreliable. We find that realistic assumptions, such as low degrees of model uncertainty or chaotic dynamics, can result in counterintuitive outcomes, including dramatic deviations between predicted and true counterfactual trajectories. This work urges caution when applying counterfactual reasoning in settings characterized by chaos and uncertainty. Furthermore, it raises the question of whether certain systems may pose fundamental limitations on the ability to answer counterfactual questions about their behavior.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2404.03275">DELTA: Decomposed Efficient Long-Term Robot Task Planning using Large Language Models</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Yuchen Liu, Luigi Palmieri, Sebastian Koch, Ilche Georgievski, Marco Aiello</div>
        <div class="preprint-abstract">arXiv:2404.03275v3 Announce Type: replace-cross 
Abstract: Recent advancements in Large Language Models (LLMs) have sparked a revolution across many research fields. In robotics, the integration of common-sense knowledge from LLMs into task and motion planning has drastically advanced the field by unlocking unprecedented levels of context awareness. Despite their vast collection of knowledge, large language models may generate infeasible plans due to hallucinations or missing domain information. To address these challenges and improve plan feasibility and computational efficiency, we introduce DELTA, a novel LLM-informed task planning approach. By using scene graphs as environment representations within LLMs, DELTA achieves rapid generation of precise planning problem descriptions. To enhance planning performance, DELTA decomposes long-term task goals with LLMs into an autoregressive sequence of sub-goals, enabling automated task planners to efficiently solve complex problems. In our extensive evaluation, we show that DELTA enables an efficient and fully automatic task planning pipeline, achieving higher planning success rates and significantly shorter planning times compared to the state of the art. Project webpage: https://delta-llm.github.io/</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00120">EMForecaster: A Deep Learning Framework for Time Series Forecasting in Wireless Networks with Distribution-Free Uncertainty Quantification</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Xavier Mootoo, Hina Tabassum, Luca Chiaraviglio</div>
        <div class="preprint-abstract">arXiv:2504.00120v1 Announce Type: new 
Abstract: With the recent advancements in wireless technologies, forecasting electromagnetic field (EMF) exposure has become critical to enable proactive network spectrum and power allocation, as well as network deployment planning. In this paper, we develop a deep learning (DL) time series forecasting framework referred to as \textit{EMForecaster}. The proposed DL architecture employs patching to process temporal patterns at multiple scales, complemented by reversible instance normalization and mixing operations along both temporal and patch dimensions for efficient feature extraction. We augment {EMForecaster} with a conformal prediction mechanism, which is independent of the data distribution, to enhance the trustworthiness of model predictions via uncertainty quantification of forecasts. This conformal prediction mechanism ensures that the ground truth lies within a prediction interval with target error rate $\alpha$, where $1-\alpha$ is referred to as coverage. However, a trade-off exists, as increasing coverage often results in wider prediction intervals. To address this challenge, we propose a new metric called the \textit{Trade-off Score}, that balances trustworthiness of the forecast (i.e., coverage) and the width of prediction interval. Our experiments demonstrate that EMForecaster achieves superior performance across diverse EMF datasets, spanning both short-term and long-term prediction horizons. In point forecasting tasks, EMForecaster substantially outperforms current state-of-the-art DL approaches, showing improvements of 53.97\% over the Transformer architecture and 38.44\% over the average of all baseline models. EMForecaster also exhibits an excellent balance between prediction interval width and coverage in conformal forecasting, measured by the tradeoff score, showing marked improvements of 24.73\% over the average baseline and 49.17\% over the Transformer architecture.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00223">A machine learning platform for development of low flammability polymers</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Duy Nhat Phan, Alexander B. Morgan, Lokendra Poudel, Rahul Bhowmik</div>
        <div class="preprint-abstract">arXiv:2504.00223v1 Announce Type: new 
Abstract: Flammability index (FI) and cone calorimetry outcomes, such as maximum heat release rate, time to ignition, total smoke release, and fire growth rate, are critical factors in evaluating the fire safety of polymers. However, predicting these properties is challenging due to the complexity of material behavior under heat exposure. In this work, we investigate the use of machine learning (ML) techniques to predict these flammability metrics. We generated synthetic polymers using Synthetic Data Vault to augment the experimental dataset. Our comprehensive ML investigation employed both our polymer descriptors and those generated by the RDkit library. Despite the challenges of limited experimental data, our models demonstrate the potential to accurately predict FI and cone calorimetry outcomes, which could be instrumental in designing safer polymers. Additionally, we developed POLYCOMPRED, a module integrated into the cloud-based MatVerse platform, providing an accessible, web-based interface for flammability prediction. This work provides not only the predictive modeling of polymer flammability but also an interactive analysis tool for the discovery and design of new materials with tailored fire-resistant properties.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.23157">Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards for Reasoning-Enhanced Text-to-SQL</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Mohammadreza Pourreza, Shayan Talaei, Ruoxi Sun, Xingchen Wan, Hailong Li, Azalia Mirhoseini, Amin Saberi, Sercan "O. Arik</div>
        <div class="preprint-abstract">arXiv:2503.23157v2 Announce Type: replace 
Abstract: Text-to-SQL is a challenging task involving multiple reasoning-intensive subtasks, including natural language understanding, database schema comprehension, and precise SQL query formulation. Existing approaches often rely on handcrafted reasoning paths with inductive biases that can limit their overall effectiveness. Motivated by the recent success of reasoning-enhanced models such as DeepSeek R1 and OpenAI o1, which effectively leverage reward-driven self-exploration to enhance reasoning capabilities and generalization, we propose a novel set of partial rewards tailored specifically for the Text-to-SQL task. Our reward set includes schema-linking, AI feedback, n-gram similarity, and syntax check, explicitly designed to address the reward sparsity issue prevalent in reinforcement learning (RL). Leveraging group relative policy optimization (GRPO), our approach explicitly encourages large language models (LLMs) to develop intrinsic reasoning skills necessary for accurate SQL query generation. With models of different sizes, we demonstrate that RL-only training with our proposed rewards consistently achieves higher accuracy and superior generalization compared to supervised fine-tuning (SFT). Remarkably, our RL-trained 14B-parameter model significantly outperforms larger proprietary models, e.g. o3-mini by 4% and Gemini-1.5-Pro-002 by 3% on the BIRD benchmark. These highlight the efficacy of our proposed RL-training framework with partial rewards for enhancing both accuracy and reasoning capabilities in Text-to-SQL tasks.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2412.17143">ASP-based Multi-shot Reasoning via DLV2 with Incremental Grounding</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Francesco Calimeri, Giovambattista Ianni, Francesco Pacenza, Simona Perri, Jessica Zangari</div>
        <div class="preprint-abstract">arXiv:2412.17143v4 Announce Type: replace 
Abstract: DLV2 is an AI tool for Knowledge Representation and Reasoning which supports Answer Set Programming (ASP) - a logic-based declarative formalism, successfully used in both academic and industrial applications. Given a logic program modelling a computational problem, an execution of DLV2 produces the so-called answer sets that correspond one-to-one to the solutions to the problem at hand. The computational process of DLV2 relies on the typical Ground & Solve approach where the grounding step transforms the input program into a new, equivalent ground program, and the subsequent solving step applies propositional algorithms to search for the answer sets. Recently, emerging applications in contexts such as stream reasoning and event processing created a demand for multi-shot reasoning: here, the system is expected to be reactive while repeatedly executed over rapidly changing data. In this work, we present a new incremental reasoner obtained from the evolution of DLV2 towards iterated reasoning. Rather than restarting the computation from scratch, the system remains alive across repeated shots, and it incrementally handles the internal grounding process. At each shot, the system reuses previous computations for building and maintaining a large, more general ground program, from which a smaller yet equivalent portion is determined and used for computing answer sets. Notably, the incremental process is performed in a completely transparent fashion for the user. We describe the system, its usage, its applicability and performance in some practically relevant domains. Under consideration in Theory and Practice of Logic Programming (TPLP).</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00851">Generalized Tensor-based Parameter-Efficient Fine-Tuning via Lie Group Transformations</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Chongjie Si, Zhiyi Shi, Xuehui Wang, Yichen Xiao, Xiaokang Yang, Wei Shen</div>
        <div class="preprint-abstract">arXiv:2504.00851v1 Announce Type: new 
Abstract: Adapting pre-trained foundation models for diverse downstream tasks is a core practice in artificial intelligence. However, the wide range of tasks and high computational costs make full fine-tuning impractical. To overcome this, parameter-efficient fine-tuning (PEFT) methods like LoRA have emerged and are becoming a growing research focus. Despite the success of these methods, they are primarily designed for linear layers, focusing on two-dimensional matrices while largely ignoring higher-dimensional parameter spaces like convolutional kernels. Moreover, directly applying these methods to higher-dimensional parameter spaces often disrupts their structural relationships. Given the rapid advancements in matrix-based PEFT methods, rather than designing a specialized strategy, we propose a generalization that extends matrix-based PEFT methods to higher-dimensional parameter spaces without compromising their structural properties. Specifically, we treat parameters as elements of a Lie group, with updates modeled as perturbations in the corresponding Lie algebra. These perturbations are mapped back to the Lie group through the exponential map, ensuring smooth, consistent updates that preserve the inherent structure of the parameter space. Extensive experiments on computer vision and natural language processing validate the effectiveness and versatility of our approach, demonstrating clear improvements over existing methods.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00186">Are Domain Generalization Benchmarks with Accuracy on the Line Misspecified?</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Olawale Salaudeen, Nicole Chiou, Shiny Weng, Sanmi Koyejo</div>
        <div class="preprint-abstract">arXiv:2504.00186v1 Announce Type: new 
Abstract: Spurious correlations are unstable statistical associations that hinder robust decision-making. Conventional wisdom suggests that models relying on such correlations will fail to generalize out-of-distribution (OOD), especially under strong distribution shifts. However, empirical evidence challenges this view as naive in-distribution empirical risk minimizers often achieve the best OOD accuracy across popular OOD generalization benchmarks. In light of these results, we propose a different perspective: many widely used benchmarks for evaluating robustness to spurious correlations are misspecified. Specifically, they fail to include shifts in spurious correlations that meaningfully impact OOD generalization, making them unsuitable for evaluating the benefit of removing such correlations. We establish conditions under which a distribution shift can reliably assess a model's reliance on spurious correlations. Crucially, under these conditions, we should not observe a strong positive correlation between in-distribution and OOD accuracy, often called "accuracy on the line." Yet, most state-of-the-art benchmarks exhibit this pattern, suggesting they do not effectively assess robustness. Our findings expose a key limitation in current benchmarks used to evaluate domain generalization algorithms, that is, models designed to avoid spurious correlations. We highlight the need to rethink how robustness to spurious correlations is assessed, identify well-specified benchmarks the field should prioritize, and enumerate strategies for designing future benchmarks that meaningfully reflect robustness under distribution shift.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00907">Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Ram Ramrakhya, Matthew Chang, Xavier Puig, Ruta Desai, Zsolt Kira, Roozbeh Mottaghi</div>
        <div class="preprint-abstract">arXiv:2504.00907v1 Announce Type: new 
Abstract: Embodied agents operating in real-world environments must interpret ambiguous and under-specified human instructions. A capable household robot should recognize ambiguity and ask relevant clarification questions to infer the user intent accurately, leading to more effective task execution. To study this problem, we introduce the Ask-to-Act task, where an embodied agent must fetch a specific object instance given an ambiguous instruction in a home environment. The agent must strategically ask minimal, yet relevant, clarification questions to resolve ambiguity while navigating under partial observability. To solve this problem, we propose a novel approach that fine-tunes multimodal large language models (MLLMs) as vision-language-action (VLA) policies using online reinforcement learning (RL) with LLM-generated rewards. Our method eliminates the need for large-scale human demonstrations or manually engineered rewards for training such agents. We benchmark against strong zero-shot baselines, including GPT-4o, and supervised fine-tuned MLLMs, on our task. Our results demonstrate that our RL-finetuned MLLM outperforms all baselines by a significant margin ($19.1$-$40.3\%$), generalizing well to novel scenes and tasks. To the best of our knowledge, this is the first demonstration of adapting MLLMs as VLA agents that can act and ask for help using LLM-generated rewards with online RL.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.18942">Video-T1: Test-Time Scaling for Video Generation</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Fangfu Liu, Hanyang Wang, Yimo Cai, Kaiyan Zhang, Xiaohang Zhan, Yueqi Duan</div>
        <div class="preprint-abstract">arXiv:2503.18942v2 Announce Type: replace-cross 
Abstract: With the scale capability of increasing training data, model size, and computational cost, video generation has achieved impressive results in digital creation, enabling users to express creativity across various domains. Recently, researchers in Large Language Models (LLMs) have expanded the scaling to test-time, which can significantly improve LLM performance by using more inference-time computation. Instead of scaling up video foundation models through expensive training costs, we explore the power of Test-Time Scaling (TTS) in video generation, aiming to answer the question: if a video generation model is allowed to use non-trivial amount of inference-time compute, how much can it improve generation quality given a challenging text prompt. In this work, we reinterpret the test-time scaling of video generation as a searching problem to sample better trajectories from Gaussian noise space to the target video distribution. Specifically, we build the search space with test-time verifiers to provide feedback and heuristic algorithms to guide searching process. Given a text prompt, we first explore an intuitive linear search strategy by increasing noise candidates at inference time. As full-step denoising all frames simultaneously requires heavy test-time computation costs, we further design a more efficient TTS method for video generation called Tree-of-Frames (ToF) that adaptively expands and prunes video branches in an autoregressive manner. Extensive experiments on text-conditioned video generation benchmarks demonstrate that increasing test-time compute consistently leads to significant improvements in the quality of videos. Project page: https://liuff19.github.io/Video-T1</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00485">Enhancing stroke disease classification through machine learning models via a novel voting system by feature selection techniques</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Mahade Hasan, Farhana Yasmin, Md. Mehedi Hassan, Xue Yu, Soniya Yeasmin, Herat Joshi, Sheikh Mohammed Shariful Islam</div>
        <div class="preprint-abstract">arXiv:2504.00485v1 Announce Type: new 
Abstract: Heart disease remains a leading cause of mortality and morbidity worldwide, necessitating the development of accurate and reliable predictive models to facilitate early detection and intervention. While state of the art work has focused on various machine learning approaches for predicting heart disease, but they could not able to achieve remarkable accuracy. In response to this need, we applied nine machine learning algorithms XGBoost, logistic regression, decision tree, random forest, k-nearest neighbors (KNN), support vector machine (SVM), gaussian na\"ive bayes (NB gaussian), adaptive boosting, and linear regression to predict heart disease based on a range of physiological indicators. Our approach involved feature selection techniques to identify the most relevant predictors, aimed at refining the models to enhance both performance and interpretability. The models were trained, incorporating processes such as grid search hyperparameter tuning, and cross-validation to minimize overfitting. Additionally, we have developed a novel voting system with feature selection techniques to advance heart disease classification. Furthermore, we have evaluated the models using key performance metrics including accuracy, precision, recall, F1-score, and the area under the receiver operating characteristic curve (ROC AUC). Among the models, XGBoost demonstrated exceptional performance, achieving 99% accuracy, precision, F1-Score, 98% recall, and 100% ROC AUC. This study offers a promising approach to early heart disease diagnosis and preventive healthcare.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2501.13727">Scalable Safe Multi-Agent Reinforcement Learning for Multi-Agent System</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Haikuo Du, Fandi Gou, Yunze Cai</div>
        <div class="preprint-abstract">arXiv:2501.13727v2 Announce Type: replace-cross 
Abstract: Safety and scalability are two critical challenges faced by practical Multi-Agent Systems (MAS). However, existing Multi-Agent Reinforcement Learning (MARL) algorithms that rely solely on reward shaping are ineffective in ensuring safety, and their scalability is rather limited due to the fixed-size network output. To address these issues, we propose a novel framework, Scalable Safe MARL (SS-MARL), to enhance the safety and scalability of MARL methods. Leveraging the inherent graph structure of MAS, we design a multi-layer message passing network to aggregate local observations and communications of varying sizes. Furthermore, we develop a constrained joint policy optimization method in the setting of local observation to improve safety. Simulation experiments demonstrate that SS-MARL achieves a better trade-off between optimality and safety compared to baselines, and its scalability significantly outperforms the latest methods in scenarios with a large number of agents.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00509">Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on Elementary School-Level Reasoning Problems?</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Kai Yan, Yufei Xu, Zhengyin Du, Xuesong Yao, Zheyu Wang, Xiaowen Guo, Jiecao Chen</div>
        <div class="preprint-abstract">arXiv:2504.00509v1 Announce Type: new 
Abstract: The rapid escalation from elementary school-level to frontier problems of the difficulty for LLM benchmarks in recent years have weaved a miracle for researchers that we are only inches away from surpassing human intelligence. However, is the LLMs' remarkable reasoning ability indeed comes from true intelligence by human standards, or are they simply reciting solutions witnessed during training at an Internet level? To study this problem, we propose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's recitation behavior when asked simple reasoning problems but with conditions subtly shifted, and conduct empirical analysis on our benchmark. Surprisingly, we found existing cutting-edge LLMs unanimously exhibits extremely severe recitation behavior; by changing one phrase in the condition, top models such as OpenAI-o1 and DeepSeek-R1 can suffer $60\%$ performance loss on elementary school-level arithmetic and reasoning problems. Such findings are a wake-up call to the LLM community that compels us to re-evaluate the true intelligence level of cutting-edge LLMs.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00036">Improving Diseases Predictions Utilizing External Bio-Banks</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Hido Pinto, Eran Segal</div>
        <div class="preprint-abstract">arXiv:2504.00036v1 Announce Type: cross 
Abstract: Machine learning has been successfully used in critical domains, such as medicine. However, extracting meaningful insights from biomedical data is often constrained by the lack of their available disease labels. In this research, we demonstrate how machine learning can be leveraged to enhance explainability and uncover biologically meaningful associations, even when predictive improvements in disease modeling are limited. We train LightGBM models from scratch on our dataset (10K) to impute metabolomics features and apply them to the UK Biobank (UKBB) for downstream analysis. The imputed metabolomics features are then used in survival analysis to assess their impact on disease-related risk factors. As a result, our approach successfully identified biologically relevant connections that were not previously known to the predictive models. Additionally, we applied a genome-wide association study (GWAS) on key metabolomics features, revealing a link between vascular dementia and smoking. Although being a well-established epidemiological relationship, this link was not embedded in the model's training data, which validated the method's ability to extract meaningful signals. Furthermore, by integrating survival models as inputs in the 10K data, we uncovered associations between metabolic substances and obesity, demonstrating the ability to infer disease risk for future patients without requiring direct outcome labels. These findings highlight the potential of leveraging external bio-banks to extract valuable biomedical insights, even in data-limited scenarios. Our results demonstrate that machine learning models trained on smaller datasets can still be used to uncover real biological associations when carefully integrated with survival analysis and genetic studies.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2412.16765">Optimization Insights into Deep Diagonal Linear Networks</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Hippolyte Labarri\`ere, Cesare Molinari, Lorenzo Rosasco, Silvia Villa, Cristian Vega</div>
        <div class="preprint-abstract">arXiv:2412.16765v2 Announce Type: replace 
Abstract: Overparameterized models trained with (stochastic) gradient descent are ubiquitous in modern machine learning. These large models achieve unprecedented performance on test data, but their theoretical understanding is still limited. In this paper, we take a step towards filling this gap by adopting an optimization perspective. More precisely, we study the implicit regularization properties of the gradient flow "algorithm" for estimating the parameters of a deep diagonal neural network. Our main contribution is showing that this gradient flow induces a mirror flow dynamic on the model, meaning that it is biased towards a specific solution of the problem depending on the initialization of the network. Along the way, we prove several properties of the trajectory.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00061">Evaluating the Feasibility and Accuracy of Large Language Models for Medical History-Taking in Obstetrics and Gynecology</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Dou Liu, Ying Long, Sophia Zuoqiu, Tian Tang, Rong Yin</div>
        <div class="preprint-abstract">arXiv:2504.00061v1 Announce Type: cross 
Abstract: Effective physician-patient communications in pre-diagnostic environments, and most specifically in complex and sensitive medical areas such as infertility, are critical but consume a lot of time and, therefore, cause clinic workflows to become inefficient. Recent advancements in Large Language Models (LLMs) offer a potential solution for automating conversational medical history-taking and improving diagnostic accuracy. This study evaluates the feasibility and performance of LLMs in those tasks for infertility cases. An AI-driven conversational system was developed to simulate physician-patient interactions with ChatGPT-4o and ChatGPT-4o-mini. A total of 70 real-world infertility cases were processed, generating 420 diagnostic histories. Model performance was assessed using F1 score, Differential Diagnosis (DDs) Accuracy, and Accuracy of Infertility Type Judgment (ITJ). ChatGPT-4o-mini outperformed ChatGPT-4o in information extraction accuracy (F1 score: 0.9258 vs. 0.9029, p = 0.045, d = 0.244) and demonstrated higher completeness in medical history-taking (97.58% vs. 77.11%), suggesting that ChatGPT-4o-mini is more effective in extracting detailed patient information, which is critical for improving diagnostic accuracy. In contrast, ChatGPT-4o performed slightly better in differential diagnosis accuracy (2.0524 vs. 2.0048, p > 0.05). ITJ accuracy was higher in ChatGPT-4o-mini (0.6476 vs. 0.5905) but with lower consistency (Cronbach's $\alpha$ = 0.562), suggesting variability in classification reliability. Both models demonstrated strong feasibility in automating infertility history-taking, with ChatGPT-4o-mini excelling in completeness and extraction accuracy. In future studies, expert validation for accuracy and dependability in a clinical setting, AI model fine-tuning, and larger datasets with a mix of cases of infertility have to be prioritized.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.23212">Convolutional Neural Networks Can (Meta-)Learn the Same-Different Relation</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Max Gupta, Sunayana Rane, R. Thomas McCoy, Thomas L. Griffiths</div>
        <div class="preprint-abstract">arXiv:2503.23212v2 Announce Type: replace-cross 
Abstract: While convolutional neural networks (CNNs) have come to match and exceed human performance in many settings, the tasks these models optimize for are largely constrained to the level of individual objects, such as classification and captioning. Humans remain vastly superior to CNNs in visual tasks involving relations, including the ability to identify two objects as `same' or `different'. A number of studies have shown that while CNNs can be coaxed into learning the same-different relation in some settings, they tend to generalize poorly to other instances of this relation. In this work we show that the same CNN architectures that fail to generalize the same-different relation with conventional training are able to succeed when trained via meta-learning, which explicitly encourages abstraction and generalization across tasks.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00809">Bio-heat regimes in fractal-based models of tumors</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">S\'ebastien Fumeron, Malte Henkel, Alexander L\'opez</div>
        <div class="preprint-abstract">arXiv:2504.00809v1 Announce Type: new 
Abstract: Anomalous heat diffusion is investigated for biological tissues displaying a fractal structure and long-term thermal memory, which is modeled via a fractional derivative. For increasing values of the fractional derivation order, the tissue temperature displays three kinds of bio-heat regimes: damped (or sub-diffusive), critical damping and under-damped oscillations. The temperature profiles depend on the fractal dimension of the tissue but notably also on a parameter related to its topology: the spectral dimension. The parametric analysis reveals that these two parameters have antagonistic effects on the pseudo period of the temperature oscillations and their amplitudes. We discuss how our results might impact some treatment protocols.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00480">Preconditioned Additive Gaussian Processes with Fourier Acceleration</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Theresa Wagner, Tianshi Xu, Franziska Nestler, Yuanzhe Xi, Martin Stoll</div>
        <div class="preprint-abstract">arXiv:2504.00480v1 Announce Type: new 
Abstract: Gaussian processes (GPs) are crucial in machine learning for quantifying uncertainty in predictions. However, their associated covariance matrices, defined by kernel functions, are typically dense and large-scale, posing significant computational challenges. This paper introduces a matrix-free method that utilizes the Non-equispaced Fast Fourier Transform (NFFT) to achieve nearly linear complexity in the multiplication of kernel matrices and their derivatives with vectors for a predetermined accuracy level. To address high-dimensional problems, we propose an additive kernel approach. Each sub-kernel in this approach captures lower-order feature interactions, allowing for the efficient application of the NFFT method and potentially increasing accuracy across various real-world datasets. Additionally, we implement a preconditioning strategy that accelerates hyperparameter tuning, further improving the efficiency and effectiveness of GPs.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.24050">A Deep Learning Framework for the Electronic Structure of Water: Towards a Universal Model</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Xinyuan Liang, Renxi Liu, Mohan Chen</div>
        <div class="preprint-abstract">arXiv:2503.24050v2 Announce Type: replace 
Abstract: Accurately modeling the electronic structure of water across scales, from individual molecules to bulk liquid, remains a grand challenge. Traditional computational methods face a critical trade-off between computational cost and efficiency. We present an enhanced machine-learning Deep Kohn-Sham (DeePKS) method for improved electronic structure, DeePKS-ES, that overcomes this dilemma. By incorporating the Hamiltonian matrix and their eigenvalues and eigenvectors into the loss function, we establish a universal model for water systems, which can reproduce high-level hybrid functional (HSE06) electronic properties from inexpensive generalized gradient approximation (PBE) calculations. Validated across molecular clusters and liquid-phase simulations, our approach reliably predicts key electronic structure properties such as band gaps and density of states, as well as total energy and atomic forces. This work bridges quantum-mechanical precision with scalable computation, offering transformative opportunities for modeling aqueous systems in catalysis, climate science, and energy storage.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2409.04613">Convergence of Decentralized Actor-Critic Algorithm in General-sum Markov Games</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Chinmay Maheshwari, Manxi Wu, Shankar Sastry</div>
        <div class="preprint-abstract">arXiv:2409.04613v5 Announce Type: replace-cross 
Abstract: Markov games provide a powerful framework for modeling strategic multi-agent interactions in dynamic environments. Traditionally, convergence properties of decentralized learning algorithms in these settings have been established only for special cases, such as Markov zero-sum and potential games, which do not fully capture real-world interactions. In this paper, we address this gap by studying the asymptotic properties of learning algorithms in general-sum Markov games. In particular, we focus on a decentralized algorithm where each agent adopts an actor-critic learning dynamic with asynchronous step sizes. This decentralized approach enables agents to operate independently, without requiring knowledge of others' strategies or payoffs. We introduce the concept of a Markov Near-Potential Function (MNPF) and demonstrate that it serves as an approximate Lyapunov function for the policy updates in the decentralized learning dynamics, which allows us to characterize the convergent set of strategies. We further strengthen our result under specific regularity conditions and with finite Nash equilibria.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.14481">Don't lie to your friends: Learning what you know from collaborative self-play</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jacob Eisenstein, Reza Aghajani, Adam Fisch, Dheeru Dua, Fantine Huot, Mirella Lapata, Vicky Zayats, Jonathan Berant</div>
        <div class="preprint-abstract">arXiv:2503.14481v2 Announce Type: replace 
Abstract: To be helpful assistants, AI agents must be aware of their own capabilities and limitations. This includes knowing when to answer from parametric knowledge versus using tools, when to trust tool outputs, and when to abstain or hedge. Such capabilities are hard to teach through supervised fine-tuning because they require constructing examples that reflect the agent's specific capabilities. We therefore propose a radically new approach to teaching agents what they know: \emph{collaborative self-play}. We construct multi-agent collaborations in which the group is rewarded for collectively arriving at correct answers. The desired meta-knowledge emerges from the incentives built into the structure of the interaction. We focus on small societies of agents that have access to heterogeneous tools (corpus-specific retrieval), and therefore must collaborate to maximize their success while minimizing their effort. Experiments show that group-level rewards for multi-agent communities can induce policies that \emph{transfer} to improve tool use and selective prediction in settings where individual agents are deployed in isolation.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00047">EAP4EMSIG -- Enhancing Event-Driven Microscopy for Microfluidic Single-Cell Analysis</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Nils Friederich, Angelo Jovin Yamachui Sitcheu, Annika Nassal, Erenus Yildiz, Matthias Pesch, Maximilian Beichter, Lukas Scholtes, Bahar Akbaba, Thomas Lautenschlager, Oliver Neumann, Dietrich Kohlheyer, Hanno Scharr, Johannes Seiffarth, Katharina N\"oh, Ralf Mikut</div>
        <div class="preprint-abstract">arXiv:2504.00047v1 Announce Type: cross 
Abstract: Microfluidic Live-Cell Imaging yields data on microbial cell factories. However, continuous acquisition is challenging as high-throughput experiments often lack realtime insights, delaying responses to stochastic events. We introduce three components in the Experiment Automation Pipeline for Event-Driven Microscopy to Smart Microfluidic Single-Cell Analysis: a fast, accurate Deep Learning autofocusing method predicting the focus offset, an evaluation of real-time segmentation methods and a realtime data analysis dashboard. Our autofocusing achieves a Mean Absolute Error of 0.0226\textmu m with inference times below 50~ms. Among eleven Deep Learning segmentation methods, Cellpose~3 reached a Panoptic Quality of 93.58\%, while a distance-based method is fastest (121~ms, Panoptic Quality 93.02\%). All six Deep Learning Foundation Models were unsuitable for real-time segmentation.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00885">Spectral Architecture Search for Neural Networks</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Gianluca Peri, Lorenzo Giambagli, Lorenzo Chicchi, Duccio Fanelli</div>
        <div class="preprint-abstract">arXiv:2504.00885v1 Announce Type: new 
Abstract: Architecture design and optimization are challenging problems in the field of artificial neural networks. Working in this context, we here present SPARCS (SPectral ARchiteCture Search), a novel architecture search protocol which exploits the spectral attributes of the inter-layer transfer matrices. SPARCS allows one to explore the space of possible architectures by spanning continuous and differentiable manifolds, thus enabling for gradient-based optimization algorithms to be eventually employed. With reference to simple benchmark models, we show that the newly proposed method yields a self-emerging architecture with a minimal degree of expressivity to handle the task under investigation and with a reduced parameter count as compared to other viable alternatives.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2109.11679">Safe Policy Learning through Extrapolation: Application to Pre-trial Risk Assessment</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Eli Ben-Michael, D. James Greiner, Kosuke Imai, Zhichao Jiang</div>
        <div class="preprint-abstract">arXiv:2109.11679v4 Announce Type: replace-cross 
Abstract: Algorithmic recommendations and decisions have become ubiquitous in today's society. Many of these data-driven policies, especially in the realm of public policy, are based on known, deterministic rules to ensure their transparency and interpretability. We examine a particular case of algorithmic pre-trial risk assessments in the US criminal justice system, which provide deterministic classification scores and recommendations to help judges make release decisions. Our goal is to analyze data from a unique field experiment on an algorithmic pre-trial risk assessment to investigate whether the scores and recommendations can be improved. Unfortunately, prior methods for policy learning are not applicable because they require existing policies to be stochastic. We develop a maximin robust optimization approach that partially identifies the expected utility of a policy, and then finds a policy that maximizes the worst-case expected utility. The resulting policy has a statistical safety property, limiting the probability of producing a worse policy than the existing one, under structural assumptions about the outcomes. Our analysis of data from the field experiment shows that we can safely improve certain components of the risk assessment instrument by classifying arrestees as lower risk under a wide range of utility specifications, though the analysis is not informative about several components of the instrument.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.20136">Innovative LSGTime Model for Crime Spatiotemporal Prediction Based on MindSpore Framework</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Zhenkai Qin, BaoZhong Wei, Caifeng Gao</div>
        <div class="preprint-abstract">arXiv:2503.20136v3 Announce Type: replace 
Abstract: With the acceleration of urbanization, the spatiotemporal characteristics of criminal activities have become increasingly complex. Accurate prediction of crime distribution is crucial for optimizing the allocation of police resources and preventing crime. This paper proposes LGSTime, a crime spatiotemporal prediction model that integrates Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and the Multi-head Sparse Self-attention mechanism. LSTM and GRU capture long-term dependencies in crime time series, such as seasonality and periodicity, through their unique gating mechanisms. The Multi-head Sparse Self-attention mechanism, on the other hand, focuses on both temporal and spatial features of criminal events simultaneously through parallel processing and sparsification techniques, significantly improving computational efficiency and prediction accuracy. The integrated model leverages the strengths of each technique to better handle complex spatiotemporal data. Experimental findings demonstrate that the model attains optimal performance across four real - world crime datasets. In comparison to the CNN model, it exhibits performance enhancements of 2.8\%, 1.9\%, and 1.4\% in the Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE) metrics respectively. These results offer a valuable reference for tackling the challenges in crime prediction.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.06405">Heterogeneous bimodal attention fusion for speech emotion recognition</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jiachen Luo, Huy Phan, Lin Wang, Joshua Reiss</div>
        <div class="preprint-abstract">arXiv:2503.06405v3 Announce Type: replace-cross 
Abstract: Multi-modal emotion recognition in conversations is a challenging problem due to the complex and complementary interactions between different modalities. Audio and textual cues are particularly important for understanding emotions from a human perspective. Most existing studies focus on exploring interactions between audio and text modalities at the same representation level. However, a critical issue is often overlooked: the heterogeneous modality gap between low-level audio representations and high-level text representations. To address this problem, we propose a novel framework called Heterogeneous Bimodal Attention Fusion (HBAF) for multi-level multi-modal interaction in conversational emotion recognition. The proposed method comprises three key modules: the uni-modal representation module, the multi-modal fusion module, and the inter-modal contrastive learning module. The uni-modal representation module incorporates contextual content into low-level audio representations to bridge the heterogeneous multi-modal gap, enabling more effective fusion. The multi-modal fusion module uses dynamic bimodal attention and a dynamic gating mechanism to filter incorrect cross-modal relationships and fully exploit both intra-modal and inter-modal interactions. Finally, the inter-modal contrastive learning module captures complex absolute and relative interactions between audio and text modalities. Experiments on the MELD and IEMOCAP datasets demonstrate that the proposed HBAF method outperforms existing state-of-the-art baselines.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2403.13846">A Clustering Method with Graph Maximum Decoding Information</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Xinrun Xu, Manying Lv, Zhanbiao Lian, Yurong Wu, Jin Yan, Shan Jiang, Zhiming Ding</div>
        <div class="preprint-abstract">arXiv:2403.13846v3 Announce Type: replace 
Abstract: The clustering method based on graph models has garnered increased attention for its widespread applicability across various knowledge domains. Its adaptability to integrate seamlessly with other relevant applications endows the graph model-based clustering analysis with the ability to robustly extract "natural associations" or "graph structures" within datasets, facilitating the modelling of relationships between data points. Despite its efficacy, the current clustering method utilizing the graph-based model overlooks the uncertainty associated with random walk access between nodes and the embedded structural information in the data. To address this gap, we present a novel Clustering method for Maximizing Decoding Information within graph-based models, named CMDI. CMDI innovatively incorporates two-dimensional structural information theory into the clustering process, consisting of two phases: graph structure extraction and graph vertex partitioning. Within CMDI, graph partitioning is reformulated as an abstract clustering problem, leveraging maximum decoding information to minimize uncertainty associated with random visits to vertices. Empirical evaluations on three real-world datasets demonstrate that CMDI outperforms classical baseline methods, exhibiting a superior decoding information ratio (DI-R). Furthermore, CMDI showcases heightened efficiency, particularly when considering prior knowledge (PK). These findings underscore the effectiveness of CMDI in enhancing decoding information quality and computational efficiency, positioning it as a valuable tool in graph-based clustering analyses.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.20794">Can Zero-Shot Commercial APIs Deliver Regulatory-Grade Clinical Text DeIdentification?</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Veysel Kocaman, Muhammed Santas, Yigit Gul, Mehmet Butgul, David Talby</div>
        <div class="preprint-abstract">arXiv:2503.20794v2 Announce Type: replace-cross 
Abstract: We evaluate the performance of four leading solutions for de-identification of unstructured medical text - Azure Health Data Services, AWS Comprehend Medical, OpenAI GPT-4o, and John Snow Labs - on a ground truth dataset of 48 clinical documents annotated by medical experts. The analysis, conducted at both entity-level and token-level, suggests that John Snow Labs' Medical Language Models solution achieves the highest accuracy, with a 96% F1-score in protected health information (PHI) detection, outperforming Azure (91%), AWS (83%), and GPT-4o (79%). John Snow Labs is not only the only solution which achieves regulatory-grade accuracy (surpassing that of human experts) but is also the most cost-effective solution: It is over 80% cheaper compared to Azure and GPT-4o, and is the only solution not priced by token. Its fixed-cost local deployment model avoids the escalating per-request fees of cloud-based services, making it a scalable and economical choice.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2411.03855">MambaPEFT: Exploring Parameter-Efficient Fine-Tuning for Mamba</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Masakazu Yoshimura, Teruaki Hayashi, Yota Maeda</div>
        <div class="preprint-abstract">arXiv:2411.03855v3 Announce Type: replace-cross 
Abstract: An ecosystem of Transformer-based models has been established by building large models with extensive data. Parameter-efficient fine-tuning (PEFT) is a crucial technology for deploying these models to downstream tasks with minimal cost while achieving effective performance. Recently, Mamba, a State Space Model (SSM)-based model, has attracted attention as a potential alternative to Transformers. While many large-scale Mamba-based models have been proposed, efficiently adapting pre-trained Mamba-based models to downstream tasks remains unexplored. In this paper, we conduct an exploratory analysis of PEFT methods for Mamba. We investigate the effectiveness of existing PEFT methods for Transformers when applied to Mamba. We also modify these methods to better align with the Mamba architecture. Additionally, we propose new Mamba-specific PEFT methods that leverage the distinctive structure of Mamba. Our experiments indicate that PEFT performs more effectively for Mamba than Transformers. Lastly, we demonstrate how to effectively combine multiple PEFT methods and provide a framework that outperforms previous works. To ensure reproducibility, we will release the code after publication.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2412.16802">Balls-and-Bins Sampling for DP-SGD</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Lynn Chua, Badih Ghazi, Charlie Harrison, Ethan Leeman, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Amer Sinha, Chiyuan Zhang</div>
        <div class="preprint-abstract">arXiv:2412.16802v2 Announce Type: replace 
Abstract: We introduce the Balls-and-Bins sampling for differentially private (DP) optimization methods such as DP-SGD. While it has been common practice to use some form of shuffling in DP-SGD implementations, privacy accounting algorithms have typically assumed that Poisson subsampling is used instead. Recent work by Chua et al. (ICML 2024), however, pointed out that shuffling based DP-SGD can have a much larger privacy cost in practical regimes of parameters. In this work we show that the Balls-and-Bins sampling achieves the "best-of-both" samplers, namely, the implementation of Balls-and-Bins sampling is similar to that of Shuffling and models trained using DP-SGD with Balls-and-Bins sampling achieve utility comparable to those trained using DP-SGD with Shuffling at the same noise multiplier, and yet, Balls-and-Bins sampling enjoys similar-or-better privacy amplification as compared to Poisson subsampling in practical regimes.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00786">FeatInsight: An Online ML Feature Management System on 4Paradigm Sage-Studio Platform</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Xin Tong, Xuanhe Zhou, Bingsheng He, Guoliang Li, Zirui Tang, Wei Zhou, Fan Wu, Mian Lu, Yuqiang Chen</div>
        <div class="preprint-abstract">arXiv:2504.00786v1 Announce Type: cross 
Abstract: Feature management is essential for many online machine learning applications and can often become the performance bottleneck (e.g., taking up to 70% of the overall latency in sales prediction service). Improper feature configurations (e.g., introducing too many irrelevant features) can severely undermine the model's generalization capabilities. However, managing online ML features is challenging due to (1) large-scale, complex raw data (e.g., the 2018 PHM dataset contains 17 tables and dozens to hundreds of columns), (2) the need for high-performance, consistent computation of interdependent features with complex patterns, and (3) the requirement for rapid updates and deployments to accommodate real-time data changes. In this demo, we present FeatInsight, a system that supports the entire feature lifecycle, including feature design, storage, visualization, computation, verification, and lineage management. FeatInsight (with OpenMLDB as the execution engine) has been deployed in over 100 real-world scenarios on 4Paradigm's Sage Studio platform, handling up to a trillion-dimensional feature space and enabling millisecond-level feature updates. We demonstrate how FeatInsight enhances feature design efficiency (e.g., for online product recommendation) and improve feature computation performance (e.g., for online fraud detection). The code is available at https://github.com/4paradigm/FeatInsight.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2408.16245">Large-Scale Multi-omic Biosequence Transformers for Modeling Protein-Nucleic Acid Interactions</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Sully F. Chen, Robert J. Steele, Glen M. Hocky, Beakal Lemeneh, Shivanand P. Lad, Eric K. Oermann</div>
        <div class="preprint-abstract">arXiv:2408.16245v3 Announce Type: replace 
Abstract: The transformer architecture has revolutionized bioinformatics and driven progress in the understanding and prediction of the properties of biomolecules. Almost all research on large-scale biosequence transformers has focused on one domain at a time (single-omic), usually DNA/RNA or proteins. These models have seen incredible success in downstream tasks in each domain, and have achieved particularly noteworthy breakthroughs in sequence modeling and structural modeling. However, these single-omic models are naturally incapable of efficiently modeling multi-omic tasks, one of the most biologically critical being protein-nucleic acid interactions. We present our work training the largest open-source multi-omic foundation model to date. We show that these multi-omic models (MOMs) can learn joint representations between various single-omic distributions that are emergently consistent with the Central Dogma of molecular biology despite only being trained on unlabeled biosequences. We further demonstrate that MOMs can be fine-tuned to achieve state-of-the-art results on protein-nucleic acid interaction tasks, namely predicting the change in Gibbs free energy ($\Delta G$) of the binding interaction between a given nucleic acid and protein. Remarkably, we show that multi-omic biosequence transformers emergently learn useful structural information without any \textit{a priori} structural training, allowing us to predict which protein residues are most involved in the protein-nucleic acid binding interaction. Lastly, we provide evidence that multi-omic biosequence models are in many cases superior to foundation models trained on single-omics distributions, both in performance-per-FLOP and absolute performance, suggesting a more generalized or foundational approach to building these models for biology.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.22968">HRET: A Self-Evolving LLM Evaluation Toolkit for Korean</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Hanwool Lee, Soo Yong Kim, Dasol Choi, SangWon Baek, Seunghyeok Hong, Ilgyun Jeong, Inseon Hwang, Naeun Lee, Guijin Son</div>
        <div class="preprint-abstract">arXiv:2503.22968v2 Announce Type: replace-cross 
Abstract: Recent advancements in Korean large language models (LLMs) have spurred numerous benchmarks and evaluation methodologies, yet the lack of a standardized evaluation framework has led to inconsistent results and limited comparability. To address this, we introduce HRET Haerae Evaluation Toolkit, an open-source, self-evolving evaluation framework tailored specifically for Korean LLMs. HRET unifies diverse evaluation methods, including logit-based scoring, exact-match, language-inconsistency penalization, and LLM-as-a-Judge assessments. Its modular, registry-based architecture integrates major benchmarks (HAE-RAE Bench, KMMLU, KUDGE, HRM8K) and multiple inference backends (vLLM, HuggingFace, OpenAI-compatible endpoints). With automated pipelines for continuous evolution, HRET provides a robust foundation for reproducible, fair, and transparent Korean NLP research.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00218">$\textit{Agents Under Siege}$: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Rana Muhammad Shahroz Khan, Zhen Tan, Sukwon Yun, Charles Flemming, Tianlong Chen</div>
        <div class="preprint-abstract">arXiv:2504.00218v1 Announce Type: cross 
Abstract: Most discussions about Large Language Model (LLM) safety have focused on single-agent settings but multi-agent LLM systems now create novel adversarial risks because their behavior depends on communication between agents and decentralized reasoning. In this work, we innovatively focus on attacking pragmatic systems that have constrains such as limited token bandwidth, latency between message delivery, and defense mechanisms. We design a $\textit{permutation-invariant adversarial attack}$ that optimizes prompt distribution across latency and bandwidth-constraint network topologies to bypass distributed safety mechanisms within the system. Formulating the attack path as a problem of $\textit{maximum-flow minimum-cost}$, coupled with the novel $\textit{Permutation-Invariant Evasion Loss (PIEL)}$, we leverage graph-based optimization to maximize attack success rate while minimizing detection risk. Evaluating across models including $\texttt{Llama}$, $\texttt{Mistral}$, $\texttt{Gemma}$, $\texttt{DeepSeek}$ and other variants on various datasets like $\texttt{JailBreakBench}$ and $\texttt{AdversarialBench}$, our method outperforms conventional attacks by up to $7\times$, exposing critical vulnerabilities in multi-agent systems. Moreover, we demonstrate that existing defenses, including variants of $\texttt{Llama-Guard}$ and $\texttt{PromptGuard}$, fail to prohibit our attack, emphasizing the urgent need for multi-agent specific safety mechanisms.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00515">Training Frozen Feature Pyramid DINOv2 for Eyelid Measurements with Infinite Encoding and Orthogonal Regularization</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Chun-Hung Chen</div>
        <div class="preprint-abstract">arXiv:2504.00515v1 Announce Type: new 
Abstract: Accurate measurement of eyelid parameters such as Margin Reflex Distances (MRD1, MRD2) and Levator Function (LF) is critical in oculoplastic diagnostics but remains limited by manual, inconsistent methods. This study evaluates deep learning models: SE-ResNet, EfficientNet, and the vision transformer-based DINOv2 for automating these measurements using smartphone-acquired images. We assess performance across frozen and fine-tuned settings, using MSE, MAE, and R2 metrics. DINOv2, pretrained through self-supervised learning, demonstrates superior scalability and robustness, especially under frozen conditions ideal for mobile deployment. Lightweight regressors such as MLP and Deep Ensemble offer high precision with minimal computational overhead. To address class imbalance and improve generalization, we integrate focal loss, orthogonal regularization, and binary encoding strategies. Our results show that DINOv2 combined with these enhancements delivers consistent, accurate predictions across all tasks, making it a strong candidate for real-world, mobile-friendly clinical applications. This work highlights the potential of foundation models in advancing AI-powered ophthalmic care.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00831">Example-Based Concept Analysis Framework for Deep Weather Forecast Models</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Soyeon Kim, Junho Choi, Subeen Lee, Jaesik Choi</div>
        <div class="preprint-abstract">arXiv:2504.00831v1 Announce Type: new 
Abstract: To improve the trustworthiness of an AI model, finding consistent, understandable representations of its inference process is essential. This understanding is particularly important in high-stakes operations such as weather forecasting, where the identification of underlying meteorological mechanisms is as critical as the accuracy of the predictions. Despite the growing literature that addresses this issue through explainable AI, the applicability of their solutions is often limited due to their AI-centric development. To fill this gap, we follow a user-centric process to develop an example-based concept analysis framework, which identifies cases that follow a similar inference process as the target instance in a target model and presents them in a user-comprehensible format. Our framework provides the users with visually and conceptually analogous examples, including the probability of concept assignment to resolve ambiguities in weather mechanisms. To bridge the gap between vector representations identified from models and human-understandable explanations, we compile a human-annotated concept dataset and implement a user interface to assist domain experts involved in the the framework development.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00852">ReaLitE: Enrichment of Relation Embeddings in Knowledge Graphs using Numeric Literals</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Antonis Klironomos, Baifan Zhou, Zhuoxun Zheng, Gad-Elrab Mohamed, Heiko Paulheim, Evgeny Kharlamov</div>
        <div class="preprint-abstract">arXiv:2504.00852v1 Announce Type: new 
Abstract: Most knowledge graph embedding (KGE) methods tailored for link prediction focus on the entities and relations in the graph, giving little attention to other literal values, which might encode important information. Therefore, some literal-aware KGE models attempt to either integrate numerical values into the embeddings of the entities or convert these numerics into entities during preprocessing, leading to information loss. Other methods concerned with creating relation-specific numerical features assume completeness of numerical data, which does not apply to real-world graphs. In this work, we propose ReaLitE, a novel relation-centric KGE model that dynamically aggregates and merges entities' numerical attributes with the embeddings of the connecting relations. ReaLitE is designed to complement existing conventional KGE methods while supporting multiple variations for numerical aggregations, including a learnable method.
  We comprehensively evaluated the proposed relation-centric embedding using several benchmarks for link prediction and node classification tasks. The results showed the superiority of ReaLitE over the state of the art in both tasks.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00767">Automated Explanation of Machine Learning Models of Footballing Actions in Words</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Pegah Rahimian, Jernej Flisar, David Sumpter</div>
        <div class="preprint-abstract">arXiv:2504.00767v1 Announce Type: new 
Abstract: While football analytics has changed the way teams and analysts assess performance, there remains a communication gap between machine learning practice and how coaching staff talk about football. Coaches and practitioners require actionable insights, which are not always provided by models. To bridge this gap, we show how to build wordalizations (a novel approach that leverages large language models) for shots in football. Specifically, we first build an expected goals model using logistic regression. We then use the co-efficients of this regression model to write sentences describing how factors (such as distance, angle and defensive pressure) contribute to the model's prediction. Finally, we use large language models to give an entertaining description of the shot. We describe our approach in a model card and provide an interactive open-source application describing shots in recent tournaments. We discuss how shot wordalisations might aid communication in coaching and football commentary, and give a further example of how the same approach can be applied to other actions in football.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00128">Improving Predictions of Convective Storm Wind Gusts through Statistical Post-Processing of Neural Weather Models</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Antoine Leclerc, Erwan Koch, Monika Feldmann, Daniele Nerini, Tom Beucler</div>
        <div class="preprint-abstract">arXiv:2504.00128v1 Announce Type: cross 
Abstract: Issuing timely severe weather warnings helps mitigate potentially disastrous consequences. Recent advancements in Neural Weather Models (NWMs) offer a computationally inexpensive and fast approach for forecasting atmospheric environments on a 0.25{\deg} global grid. For thunderstorms, these environments can be empirically post-processed to predict wind gust distributions at specific locations. With the Pangu-Weather NWM, we apply a hierarchy of statistical and deep learning post-processing methods to forecast hourly wind gusts up to three days ahead. To ensure statistical robustness, we constrain our probabilistic forecasts using generalised extreme-value distributions across five regions in Switzerland. Using a convolutional neural network to post-process the predicted atmospheric environment's spatial patterns yields the best results, outperforming direct forecasting approaches across lead times and wind gust speeds. Our results confirm the added value of NWMs for extreme wind forecasting, especially for designing more responsive early-warning systems.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00341">Integrated LLM-Based Intrusion Detection with Secure Slicing xApp for Securing O-RAN-Enabled Wireless Network Deployments</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Joshua Moore, Aly Sabri Abdalla, Prabesh Khanal, Vuk Marojevic</div>
        <div class="preprint-abstract">arXiv:2504.00341v1 Announce Type: cross 
Abstract: The Open Radio Access Network (O-RAN) architecture is reshaping telecommunications by promoting openness, flexibility, and intelligent closed-loop optimization. By decoupling hardware and software and enabling multi-vendor deployments, O-RAN reduces costs, enhances performance, and allows rapid adaptation to new technologies. A key innovation is intelligent network slicing, which partitions networks into isolated slices tailored for specific use cases or quality of service requirements. The RAN Intelligent Controller further optimizes resource allocation, ensuring efficient utilization and improved service quality for user equipment (UEs). However, the modular and dynamic nature of O-RAN expands the threat surface, necessitating advanced security measures to maintain network integrity, confidentiality, and availability. Intrusion detection systems have become essential for identifying and mitigating attacks. This research explores using large language models (LLMs) to generate security recommendations based on the temporal traffic patterns of connected UEs. The paper introduces an LLM-driven intrusion detection framework and demonstrates its efficacy through experimental deployments, comparing non fine-tuned and fine-tuned models for task-specific accuracy.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00389">CyberBOT: Towards Reliable Cybersecurity Education via Ontology-Grounded Retrieval Augmented Generation</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Chengshuai Zhao, Riccardo De Maria, Tharindu Kumarage, Kumar Satvik Chaudhary, Garima Agrawal, Yiwen Li, Jongchan Park, Yuli Deng, Ying-Chih Chen, Huan Liu</div>
        <div class="preprint-abstract">arXiv:2504.00389v1 Announce Type: new 
Abstract: Advancements in large language models (LLMs) have enabled the development of intelligent educational tools that support inquiry-based learning across technical domains. In cybersecurity education, where accuracy and safety are paramount, systems must go beyond surface-level relevance to provide information that is both trustworthy and domain-appropriate. To address this challenge, we introduce CyberBOT, a question-answering chatbot that leverages a retrieval-augmented generation (RAG) pipeline to incorporate contextual information from course-specific materials and validate responses using a domain-specific cybersecurity ontology. The ontology serves as a structured reasoning layer that constrains and verifies LLM-generated answers, reducing the risk of misleading or unsafe guidance. CyberBOT has been deployed in a large graduate-level course at Arizona State University (ASU), where more than one hundred students actively engage with the system through a dedicated web-based platform. Computational evaluations in lab environments highlight the potential capacity of CyberBOT, and a forthcoming field study will evaluate its pedagogical impact. By integrating structured domain reasoning with modern generative capabilities, CyberBOT illustrates a promising direction for developing reliable and curriculum-aligned AI applications in specialized educational contexts.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2406.17216">Machine Unlearning Fails to Remove Data Poisoning Attacks</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Martin Pawelczyk, Jimmy Z. Di, Yiwei Lu, Ayush Sekhari, Gautam Kamath, Seth Neel</div>
        <div class="preprint-abstract">arXiv:2406.17216v2 Announce Type: replace 
Abstract: We revisit the efficacy of several practical methods for approximate machine unlearning developed for large-scale deep learning. In addition to complying with data deletion requests, one often-cited potential application for unlearning methods is to remove the effects of poisoned data. We experimentally demonstrate that, while existing unlearning methods have been demonstrated to be effective in a number of settings, they fail to remove the effects of data poisoning across a variety of types of poisoning attacks (indiscriminate, targeted, and a newly-introduced Gaussian poisoning attack) and models (image classifiers and LLMs); even when granted a relatively large compute budget. In order to precisely characterize unlearning efficacy, we introduce new evaluation metrics for unlearning based on data poisoning. Our results suggest that a broader perspective, including a wider variety of evaluations, are required to avoid a false sense of confidence in machine unlearning procedures for deep learning without provable guarantees. Moreover, while unlearning methods show some signs of being useful to efficiently remove poisoned data without having to retrain, our work suggests that these methods are not yet ``ready for prime time,'' and currently provide limited benefit over retraining.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00342">Aligning Diffusion Model with Problem Constraints for Trajectory Optimization</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Anjian Li, Ryne Beeson</div>
        <div class="preprint-abstract">arXiv:2504.00342v1 Announce Type: cross 
Abstract: Diffusion models have recently emerged as effective generative frameworks for trajectory optimization, capable of producing high-quality and diverse solutions. However, training these models in a purely data-driven manner without explicit incorporation of constraint information often leads to violations of critical constraints, such as goal-reaching, collision avoidance, and adherence to system dynamics. To address this limitation, we propose a novel approach that aligns diffusion models explicitly with problem-specific constraints, drawing insights from the Dynamic Data-driven Application Systems (DDDAS) framework. Our approach introduces a hybrid loss function that explicitly measures and penalizes constraint violations during training. Furthermore, by statistically analyzing how constraint violations evolve throughout the diffusion steps, we develop a re-weighting strategy that aligns predicted violations to ground truth statistics at each diffusion step. Evaluated on a tabletop manipulation and a two-car reach-avoid problem, our constraint-aligned diffusion model significantly reduces constraint violations compared to traditional diffusion models, while maintaining the quality of trajectory solutions. This approach is well-suited for integration into the DDDAS framework for efficient online trajectory adaptation as new environmental data becomes available.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00470">Less is More: Efficient Black-box Attribution via Minimal Interpretable Subset Selection</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Ruoyu Chen, Siyuan Liang, Jingzhi Li, Shiming Liu, Li Liu, Hua Zhang, Xiaochun Cao</div>
        <div class="preprint-abstract">arXiv:2504.00470v1 Announce Type: new 
Abstract: To develop a trustworthy AI system, which aim to identify the input regions that most influence the models decisions. The primary task of existing attribution methods lies in efficiently and accurately identifying the relationships among input-prediction interactions. Particularly when the input data is discrete, such as images, analyzing the relationship between inputs and outputs poses a significant challenge due to the combinatorial explosion. In this paper, we propose a novel and efficient black-box attribution mechanism, LiMA (Less input is More faithful for Attribution), which reformulates the attribution of important regions as an optimization problem for submodular subset selection. First, to accurately assess interactions, we design a submodular function that quantifies subset importance and effectively captures their impact on decision outcomes. Then, efficiently ranking input sub-regions by their importance for attribution, we improve optimization efficiency through a novel bidirectional greedy search algorithm. LiMA identifies both the most and least important samples while ensuring an optimal attribution boundary that minimizes errors. Extensive experiments on eight foundation models demonstrate that our method provides faithful interpretations with fewer regions and exhibits strong generalization, shows an average improvement of 36.3% in Insertion and 39.6% in Deletion. Our method also outperforms the naive greedy search in attribution efficiency, being 1.6 times faster. Furthermore, when explaining the reasons behind model prediction errors, the average highest confidence achieved by our method is, on average, 86.1% higher than that of state-of-the-art attribution algorithms. The code is available at https://github.com/RuoyuChen10/LIMA.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00711">GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited Environments</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Enjun Du, Xunkai Li, Tian Jin, Zhihan Zhang, Rong-Hua Li, Guoren Wang</div>
        <div class="preprint-abstract">arXiv:2504.00711v1 Announce Type: new 
Abstract: The era of foundation models has revolutionized AI research, yet Graph Foundation Models (GFMs) remain constrained by the scarcity of large-scale graph corpora. Traditional graph data synthesis techniques primarily focus on simplistic structural operations, lacking the capacity to generate semantically rich nodes with meaningful textual attributes: a critical limitation for real-world applications. While large language models (LLMs) demonstrate exceptional text generation capabilities, their direct application to graph synthesis is impeded by context window limitations, hallucination phenomena, and structural consistency challenges. To address these issues, we introduce GraphMaster, the first multi-agent framework specifically designed for graph data synthesis in data-limited environments. GraphMaster orchestrates four specialized LLM agents (Manager, Perception, Enhancement, and Evaluation) that collaboratively optimize the synthesis process through iterative refinement, ensuring both semantic coherence and structural integrity. To rigorously evaluate our approach, we create new data-limited "Sub" variants of six standard graph benchmarks, specifically designed to test synthesis capabilities under realistic constraints. Additionally, we develop a novel interpretability assessment framework that combines human evaluation with a principled Grassmannian manifold-based analysis, providing both qualitative and quantitative measures of semantic coherence. Experimental results demonstrate that GraphMaster significantly outperforms traditional synthesis methods across multiple datasets, establishing a strong foundation for advancing GFMs in data-scarce environments.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00221">GazeLLM: Multimodal LLMs incorporating Human Visual Attention</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jun Rekimoto</div>
        <div class="preprint-abstract">arXiv:2504.00221v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are advancing into Multimodal LLMs (MLLMs), capable of processing image, audio, and video as well as text. Combining first-person video, MLLMs show promising potential for understanding human activities through video and audio, enabling many human-computer interaction and human-augmentation applications such as human activity support, real-world agents, and skill transfer to robots or other individuals. However, handling high-resolution, long-duration videos generates large latent representations, leading to substantial memory and processing demands, limiting the length and resolution MLLMs can manage. Reducing video resolution can lower memory usage but often compromises comprehension. This paper introduces a method that optimizes first-person video analysis by integrating eye-tracking data, and proposes a method that decomposes first-person vision video into sub areas for regions of gaze focus. By processing these selectively gazed-focused inputs, our approach achieves task comprehension equivalent to or even better than processing the entire image at full resolution, but with significantly reduced video data input (reduce the number of pixels to one-tenth), offering an efficient solution for using MLLMs to interpret and utilize human skills.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00030">Token-Driven GammaTune: Adaptive Calibration for Enchanced Speculative Decoding</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Aayush Gautam, Susav Shrestha, Narasimha Annapareddy</div>
        <div class="preprint-abstract">arXiv:2504.00030v1 Announce Type: cross 
Abstract: Speculative decoding accelerates large language model (LLM) inference by using a smaller draft model to propose tokens, which are then verified by a larger target model. However, selecting an optimal speculation length is critical for maximizing speedup while minimizing wasted computation. We introduce \textit{GammaTune} and \textit{GammaTune+}, training-free adaptive algorithms that dynamically adjust speculation length based on token acceptance rates using a heuristic-based switching mechanism. Evaluated on SpecBench across multiple tasks and model pairs, our method outperforms other heuristic-based approaches and fixed-length speculative decoding, achieving an average speedup of 15\% ($\pm$5\%) with \textit{GammaTune} and 16\% ($\pm$3\%) with \textit{GammaTune+}, while reducing performance variance. This makes \textit{GammaTune} a robust and efficient solution for real-world deployment.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00262">High-pressure Ion Trap</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Evgeny V. Krylov</div>
        <div class="preprint-abstract">arXiv:2504.00262v1 Announce Type: new 
Abstract: A High-Pressure Ion Trap operating at pressure ~1 Torr is a core component of the portable hand-held mass-spectrometric gas analyzer. A comprehensive mathematical model of the HPIT is described in this paper. The influence of the instrumental parameters (gas composition, pressure, and temperature; applied voltages; ion trap size and geometry) and ion properties (mass, diffusion, and mobility) on the ion trap analytical parameters (mass-spectral peak position, height, and width) is examined. The model explained the difference between a high-pressure and regular low-pressure ion trap.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00938">AI Judges in Design: Statistical Perspectives on Achieving Human Expert Equivalence With Vision-Language Models</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Kristen M. Edwards, Farnaz Tehranchi, Scarlett R. Miller, Faez Ahmed</div>
        <div class="preprint-abstract">arXiv:2504.00938v1 Announce Type: new 
Abstract: The subjective evaluation of early stage engineering designs, such as conceptual sketches, traditionally relies on human experts. However, expert evaluations are time-consuming, expensive, and sometimes inconsistent. Recent advances in vision-language models (VLMs) offer the potential to automate design assessments, but it is crucial to ensure that these AI ``judges'' perform on par with human experts. However, no existing framework assesses expert equivalence. This paper introduces a rigorous statistical framework to determine whether an AI judge's ratings match those of human experts. We apply this framework in a case study evaluating four VLM-based judges on key design metrics (uniqueness, creativity, usefulness, and drawing quality). These AI judges employ various in-context learning (ICL) techniques, including uni- vs. multimodal prompts and inference-time reasoning. The same statistical framework is used to assess three trained novices for expert-equivalence. Results show that the top-performing AI judge, using text- and image-based ICL with reasoning, achieves expert-level agreement for uniqueness and drawing quality and outperforms or matches trained novices across all metrics. In 6/6 runs for both uniqueness and creativity, and 5/6 runs for both drawing quality and usefulness, its agreement with experts meets or exceeds that of the majority of trained novices. These findings suggest that reasoning-supported VLM models can achieve human-expert equivalence in design evaluation. This has implications for scaling design evaluation in education and practice, and provides a general statistical framework for validating AI judges in other domains requiring subjective content evaluation.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00521">Automated detection of atomicity violations in large-scale systems</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Hang He, Yixing Luo, Chengcheng Wan, Ting Su, Haiying Sun, Geguang Pu</div>
        <div class="preprint-abstract">arXiv:2504.00521v1 Announce Type: cross 
Abstract: Atomicity violations in interrupt-driven programs pose a significant threat to software safety in critical systems. These violations occur when the execution sequence of operations on shared resources is disrupted by asynchronous interrupts. Detecting atomicity violations is challenging due to the vast program state space, application-level code dependencies, and complex domain-specific knowledge. We propose Clover, a hybrid framework that integrates static analysis with large language model (LLM) agents to detect atomicity violations in real-world programs. Clover first performs static analysis to extract critical code snippets and operation information. It then initiates a multi-agent process, where the expert agent leverages domain-specific knowledge to detect atomicity violations, which are subsequently validated by the judge agent. Evaluations on RaceBench 2.1, SV-COMP, and RWIP demonstrate that Clover achieves a precision/recall of 92.3%/86.6%, outperforming existing approaches by 27.4-118.2% on F1-score.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2405.19531">Hierarchical Procedural Framework for Low-latency Robot-Assisted Hand-Object Interaction</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Mingqi Yuan, Huijiang Wang, Kai-Fung Chu, Fumiya Iida, Bo Li, Wenjun Zeng</div>
        <div class="preprint-abstract">arXiv:2405.19531v2 Announce Type: replace-cross 
Abstract: Advances in robotics have been driving the development of human-robot interaction (HRI) technologies. However, accurately perceiving human actions and achieving adaptive control remains a challenge in facilitating seamless coordination between human and robotic movements. In this paper, we propose a hierarchical procedural framework to enable dynamic robot-assisted hand-object interaction. An open-loop hierarchy leverages the computer vision (CV)-based 3D reconstruction of the human hand, based on which motion primitives have been designed to translate hand motions into robotic actions. The low-level coordination hierarchy fine-tunes the robot's action by using the continuously updated 3D hand models. Experimental validation demonstrates the effectiveness of the hierarchical control architecture. The adaptive coordination between human and robot behavior has achieved a delay of $\leq 0.3$ seconds in the tele-interaction scenario. A case study of ring-wearing tasks indicates the potential application of this work in assistive technologies such as healthcare and manufacturing.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00308">FedPaI: Achieving Extreme Sparsity in Federated Learning via Pruning at Initialization</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Haonan Wang, Zeli Liu, Kajimusugura Hoshino, Tuo Zhang, John Paul Walters, Stephen Crago</div>
        <div class="preprint-abstract">arXiv:2504.00308v1 Announce Type: new 
Abstract: Federated Learning (FL) enables distributed training on edge devices but faces significant challenges due to resource constraints in edge environments, impacting both communication and computational efficiency. Existing iterative pruning techniques improve communication efficiency but are limited by their centralized design, which struggles with FL's decentralized and data-imbalanced nature, resulting in suboptimal sparsity levels. To address these issues, we propose FedPaI, a novel efficient FL framework that leverages Pruning at Initialization (PaI) to achieve extreme sparsity. FedPaI identifies optimal sparse connections at an early stage, maximizing model capacity and significantly reducing communication and computation overhead by fixing sparsity patterns at the start of training. To adapt to diverse hardware and software environments, FedPaI supports both structured and unstructured pruning. Additionally, we introduce personalized client-side pruning mechanisms for improved learning capacity and sparsity-aware server-side aggregation for enhanced efficiency. Experimental results demonstrate that FedPaI consistently outperforms existing efficient FL that applies conventional iterative pruning with significant leading in efficiency and model accuracy. For the first time, our proposed FedPaI achieves an extreme sparsity level of up to 98% without compromising the model accuracy compared to unpruned baselines, even under challenging non-IID settings. By employing our FedPaI with joint optimization of model learning capacity and sparsity, FL applications can benefit from faster convergence and accelerate the training by 6.4 to 7.9 times.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2504.00510">Operator Learning with Domain Decomposition for Geometry Generalization in PDE Solving</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Jianing Huang, Kaixuan Zhang, Youjia Wu, Ze Cheng</div>
        <div class="preprint-abstract">arXiv:2504.00510v1 Announce Type: new 
Abstract: Neural operators have become increasingly popular in solving \textit{partial differential equations} (PDEs) due to their superior capability to capture intricate mappings between function spaces over complex domains. However, the data-hungry nature of operator learning inevitably poses a bottleneck for their widespread applications. At the core of the challenge lies the absence of transferability of neural operators to new geometries. To tackle this issue, we propose operator learning with domain decomposition, a local-to-global framework to solve PDEs on arbitrary geometries. Under this framework, we devise an iterative scheme \textit{Schwarz Neural Inference} (SNI). This scheme allows for partitioning of the problem domain into smaller subdomains, on which local problems can be solved with neural operators, and stitching local solutions to construct a global solution. Additionally, we provide a theoretical analysis of the convergence rate and error bound. We conduct extensive experiments on several representative PDEs with diverse boundary conditions and achieve remarkable geometry generalization compared to alternative methods. These analysis and experiments demonstrate the proposed framework's potential in addressing challenges related to geometry generalization and data efficiency.</div>
    </div>
        
    <div class="preprint">
        <div class="preprint-title"><a href="https://arxiv.org/abs/2503.18497">Statistically Testing Training Data for Unwanted Error Patterns using Rule-Oriented Regression</a></div>
        <div class="preprint-dates">2025-04-02</div>
        <div class="preprint-authors">Stefan Rass, Martin Dallinger</div>
        <div class="preprint-abstract">arXiv:2503.18497v2 Announce Type: replace 
Abstract: Artificial intelligence models trained from data can only be as good as the underlying data is. Biases in training data propagating through to the output of a machine learning model are a well-documented and well-understood phenomenon, but the machinery to prevent these undesired effects is much less developed. Efforts to ensure data is clean during collection, such as using bias-aware sampling, are most effective when the entity controlling data collection also trains the AI. In cases where the data is already available, how do we find out if the data was already manipulated, i.e., ``poisoned'', so that an undesired behavior would be trained into a machine learning model? This is a challenge fundamentally different to (just) improving approximation accuracy or efficiency, and we provide a method to test training data for flaws, to establish a trustworthy ground-truth for a subsequent training of machine learning models (of any kind). Unlike the well-studied problem of approximating data using fuzzy rules that are generated from the data, our method hinges on a prior definition of rules to happen before seeing the data to be tested. Therefore, the proposed method can also discover hidden error patterns, which may also have substantial influence. Our approach extends the abilities of conventional statistical testing by letting the ``test-condition'' be any Boolean condition to describe a pattern in the data, whose presence we wish to determine. The method puts fuzzy inference into a regression model, to get the best of the two: explainability from fuzzy logic with statistical properties and diagnostics from the regression, and finally also being applicable to ``small data'', hence not requiring large datasets as deep learning methods do. We provide an open source implementation for demonstration and experiments.</div>
    </div>
        
</body>
</html>
    